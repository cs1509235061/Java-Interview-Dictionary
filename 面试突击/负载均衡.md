# 负载均衡

## 1.负载均衡

负载均衡建立在现有网络结构之上，提供了一种廉价、有效、透明的方法来扩展网络设备和服务器的带宽，增加了吞吐量，加强了网络数据处理能力，并提高了网络的灵活性和可用性。项目中常用的负载均衡有四层负载均衡和七层负载均衡。

## 2.四层负载均衡与七层负载均衡的对比

四层负载均衡基于IP和端口的方式实现网络的负载均衡，具体实现为对外提供一个虚拟IP和端口接收所有用户的请求，然后根据负载均衡配置和负载均衡策略将请求发送给真实的服务器。

七层负载均衡基于URL等资源来实现应用层基于内容的负载均衡，具体实现为通过虚拟的URL或主机名接收所有用户的请求，然后将请求发送给真实的服务器。

四层负载均衡和七层负载均衡的最大差别是：四层负载均衡只能针对IP地址和端口上的数据做统一的分发，而七层负载均衡能根据消息的内容做更加详细的有针对性的负载均衡。我们通常使用LVS等技术实现基于Socket的四层负载均衡，使用Nginx等技术实现基于内容分发的七层负载均衡，比如将以“/user/* ”开头的URL请求负载到单点登录服务器，而将以“/business/* ”开头的URL请求负载到具体的业务服务器。

## 3.四层负载均衡

四层负载均衡主要通过修改报文中的目标地址和端口来实现报文的分发和负载均衡。以TCP为例，负载均衡设备在接收到第1个来自客户端的SYN请求后，会根据负载均衡配置和负载均衡策略选择一个最佳的服务器，并将报文中的目标IP地址修改为该服务器的IP直接转发给该服务器。TCP连接的建立（即三次握手过程）是在客户端和服务器端之间完成的，负载均衡设备只起到路由器的转发功能。

四层负载均衡常用的软硬件如下。

- F5：硬件负载均衡器，功能完备，价格昂贵。
- LVS：基于IP+端口实现的四层负载软件，常和Keepalive配合使用。
- Nginx：同时实现四层负载和七层负载均衡，带缓存功能，可基于正则表达式灵活转发。

## 4.七层负载均衡

七层负载均衡又叫作“内容负载均衡”，主要通过解析报文中真正有意义的应用层内容，并根据负载均衡配置和负载均衡策略选择一个最佳的服务器响应用户的请求。

七层应用负载可以使整个网络更智能化，七层负载均衡根据不同的数据类型将数据存储在不同的服务器上来提高网络整体的负载能力。比如将客户端的基本信息存储在内存较大的缓存服务器上，将文件信息存储在磁盘空间较大的文件服务器上，将图片视频存储在网络I/O能力较强的流媒体服务器上。在接收到不同的客户端的请求时从不同的服务器上获取数据并将其返回给客户端，提高客户端的访问效率。

七层负载均衡常用的软件如下。

- HAProxy：支持七层代理、会话保持、标记、路径转移等。
- Nginx：同时实现四层负载和七层负载均衡，在HTTP和Mail协议上功能比较好，性能与HAProxy差不多。
- Apache：使用简单，性能较差。

## 5.负载均衡算法

常用的负载均衡算法有：轮询均衡（Round Robin）、权重轮询均衡（Weighted Round Robin）、随机均衡（Random）、权重随机均衡（Weighted Random）、响应速度均衡（Response Time）、最少连接数均衡（Least Connection）、处理能力均衡、DNS响应均衡（Flash DNS）、散列算法均衡、IP地址散列、URL散列。不同的负载均衡算法适用于不同的应用场景。

### 轮询均衡（RoundRobin）

轮询均衡指将客户端请求轮流分配到1至N台服务器上，每台服务器均被均等地分配一定数量的客户端请求。轮询均衡算法适用于集群中所有服务器都有相同的软硬件配置和服务能力的情况下。

### 权重轮询均衡（WeightedRoundRobin）

权重轮询均衡指根据每台服务器的不同配置及服务能力，为每台服务器都设置不同的权重值，然后按照设置的权重值轮询地将请求分配到不同的服务器上。例如，服务器A的权重值被设计成3，服务器B的权重值被设计成3，服务器C的权重值被设计成4，则服务器A、B、C将分别承担30%、30%、40%的客户端请求。权重轮询均衡算法主要用于服务器配置不均等的集群中。

### 随机均衡（Random）

随机均衡指将来自网络的请求随机分配给内部的多台服务器，不考虑服务器的配置和负载情况。

### 权重随机均衡（WeightedRandom）

权重随机均衡算法类似于权重轮询算法，只是在分配请求时不再轮询发送，而是随机选择某个权重的服务器发送。

### 响应速度均衡（ResponseTime）

响应速度均衡指根据服务器设备响应速度的不同将客户端请求发送到响应速度最快的服务器上。对响应速度的获取是通过负载均衡设备定时为每台服务都发出一个探测请求（例如Ping）实现的。响应速度均衡能够为当前的每台服务器根据其不同的负载情况分配不同的客户端请求，这有效避免了某台服务器单点负载过高的情况。但需要注意的是，这里探测到的响应速度是负载均衡设备到各个服务器之间的响应速度，并不完全代表客户端到服务器的响应速度，因此存在一定偏差。

### 最少连接数均衡（LeastConnection）

最少连接数均衡指在负载均衡器内部记录当前每台服务器正在处理的连接数量，在有新的请求时，将该请求分配给连接数最少的服务器。这种均衡算法适用于网络连接和带宽有限、CPU处理任务简单的请求服务，例如FTP。

### 处理能力均衡

处理能力均衡算法将服务请求分配给内部负荷最轻的服务器，负荷是根据服务器的CPU型号、CPU数量、内存大小及当前连接数等换算而成的。处理能力均衡算法由于考虑到了内部服务器的处理能力及当前网络的运行状况，所以相对来说更加精确，尤其适用于七层负载均衡的场景。

### DNS响应均衡（FlashDNS）

DNS响应均衡算法指在分布在不同中心机房的负载均衡设备都收到同一个客户端的域名解析请求时，所有负载均衡设备均解析此域名并将解析后的服务器IP地址返回给客户端，客户端向收到第一个域名解析后的IP地址发起请求服务，而忽略其他负载均衡设备的响应。这种均衡算法适用于全局负载均衡的场景。

### 散列算法均衡

散列算法均衡指通过一致性散列算法和虚拟节点技术将相同参数的请求总是发送到同一台服务器，该服务器将长期、稳定地为某些客户端提供服务。在某个服务器被移除或异常宕机后，该服务器的请求基于虚拟节点技术平摊到其他服务器，而不会影响集群整体的稳定性。

### IP地址散列

IP地址散列指在负载均衡器内部维护了不同链接上客户端和服务器的IP对应关系表，将来自同一客户端的请求统一转发给相同的服务器。该算法能够以会话为单位，保证同一客户端的请求能够一直在同一台服务器上处理，主要适用于客户端和服务器需要保持长连接的场景，比如基于TCP长连接的应用。

### URL散列

URL散列指通过管理客户端请求URL信息的散列表，将相同URL的请求转发给同一台服务器。该算法主要适用于在七层负载中根据用户请求类型的不同将其转发给不同类型的应用服务器。

## 6.LVS的原理及应用

LVS（Linux Virtual Server）是一个虚拟的服务器集群系统，采用IP负载均衡技术将请求均衡地转移到不同的服务器上执行，且通过调度器自动屏蔽故障服务器，从而将一组服务器构成一个高性能、高可用的虚拟服务器。整个服务器集群的结构对用户是透明的，无须修改客户端和服务器端的程序，便可实现客户端到服务器的负载均衡。

### LVS的原理

LVS由前端的负载均衡器（Load Balancer，LB）和后端的真实服务器（Real Server，RS）群组成，在真实服务器间可通过局域网或广域网连接。LVS的这种结构对用户是透明的，用户只需要关注作为LB的虚拟服务器（Virtual Server），而不需要关注提供服务的真实服务器群。在用户的请求被发送给虚拟服务器后，LB根据设定的包转发策略和负载均衡调度算法将用户的请求转发给真实服务器，真实服务器再将用户请求的结果返回给用户。

实现LVS的核心组件有负载均衡调度器、服务器池和共享存储。

- 负载均衡调度器（Load Balancer/Director）：是整个集群对外提供服务的入口，通过对外提供一个虚拟IP 来接收客户端请求。在客户端将请求发送到该虚拟IP 后，负载均衡调度器会负责将请求按照负载均衡策略发送到一组具体的服务器上。
- 服务器池（Server Pool）：服务器池是一组真正处理客户端请求的真实服务器，具体执行的服务有WEB、MAIL、FTP和DNS等。
- 共享存储（Shared Storage）：为服务器池提供一个共享的存储区，使得服务器池拥有相同的内容，提供相同的服务。

### LVS的应用

- CIP（客户端IP）：用户记录发送给集群的源IP地址
- VIP（虚拟IP）：用于Director 对外提供服务的IP地址
- DIP（Director IP）：Director 用于连接内外网络的IP地址，即负载均衡器上的IP地址
- RIP（真实IP）：集群中真实服务器的物理IP地址
- LIP（LVS内部IP）：LVS集群的内部通信IP

LVS的IP负载均衡技术是通过IPVS模块实现的。IPVS是LVS集群系统的核心软件，被安装在Director Server上，同时在Director Server上虚拟出一个IP地址。用户通过这个虚拟的IP地址访问服务器。这个虚拟的IP地址一般被称为LVS的VIP，即Virtual IP。访问的请求首先经过VIP到达负载调度器，然后由负载调度器从真实服务器列表中选取一个服务节点响应用户的请求。

### LVS数据转发

LVS的数据转发流程是LVS设计的核心部分，如下所述：

（1）PREROUTING链接收用户请求：客户端向PREROUTING链发送请求。

（2）INPUT链转发：在PREROUTING链通过RouteTable列表发现请求数据包的目的地址是本机时，将数据包发送给INPUT链。

（3）IPVS检查：IPVS检查INPUT链上的数据包，如果数据包中的目的地址和端口不在规则列表中，则将该数据包发送到用户空间的ipvsadm。ipvsadm主要用于用户定义和管理集群。

（4）POSTROUTING链转发：如果数据包里面的目的地址和端口都在规则里面，那么将该数据包中的目的地址修改为事先定义好的真实服务器地址，通过FORWARD将数据发送到POSTROUTING链。

（5）真实服务器转发：POSTROUTING链根据数据包中的目的地址将数据包转发到真实服务器。

### LVS NAT模式

LVS NAT（Network Address Translation）即网络地址转换模式。

NAT模式通过对请求报文和响应报文的地址进行改写完成对数据的转发，具体流程如下。

（1）客户端将请求报文发送到LVS，请求报文的源地址是CIP（ClientIP Address，客户端IP），目标地址是VIP（Virtual IP Address，虚拟IP）。

（2）LVS在收到报文后，发现请求的IP地址在LVS的规则列表中存在，则将客户端请求报文的目标地址VIP修改为RIP（Real-server IPAddress，后端服务器的真实IP），并将报文发送到具体的真实服务器上。

（3）真实服务器在收到报文后，由于报文的目标地址是自己的IP，所以会响应该请求，并将响应报文返回给LVS。

（4）LVS在收到数据后将此报文的源地址修改为本机IP地址，即VIP，并将报文发送给客户端。

NAT模式的特点

- 请求的报文和响应的报文都需要通过LVS进行地址改写，因此在并发访问量较大的时候LVS存在瓶颈问题，一般适用于节点不是很多的情况下。
- 只需要在LVS上配置一个公网IP即可。
- 每台内部的真实服务器的网关地址都必须是LVS的内网地址。
- NAT 模式支持对IP 地址和端口进行转换，即用户请求的端口和真实服务器的端口可以不同。

### LVS DR模式

LVS DR（Direct Routing）模式用直接路由技术实现，通过改写请求报文的MAC地址将请求发送给真实服务器。

LVD DR模式是局域网中经常被用到的一种模式，其报文转发流程如下。

（1）客户端将请求发送给LVS，请求报文的源地址是CIP，目标地址是VIP。

（2）LVS在收到报文后，发现请求在规则中存在，则将客户端请求报文的源MAC地址改为自己的DIP（Direct IP Address，内部转发IP）的MAC地址，将目标MAC改为RIP的MAC地址，并将此包发送给真实服务器。

（3）真实服务器在收到请求后发现请求报文中的目标MAC是自己，就会将此报文接收下来，在处理完请求报文后，将响应报文通过lo（回环路由）接口发送给eth0网卡，并最终发送给客户端。

DR模式的特点

- 通过LVS修改数据包的目的MAC地址实现转发。注意，源IP地址仍然是CIP，目标IP地址仍然是VIP地址。
- 请求的报文均经过LVS，而真实服务器响应报文时无须经过LVS，因此在并发访问量大时比NAT模式的效率高很多。
- 因为DR 模式是通过MAC 地址改写机制实现转发的，因此所有真实服务器节点和LVS只能被部署在同一个局域网内。
- 真实服务器主机需要绑定VIP 地址在lo接口（掩码 32 位）上，并且需要配置ARP抑制。
- 真实服务器节点的默认网关无须被配置为LVS网关，只需要被配置为上级路由的网关，能让真实服务器直接出网即可。
- DR 模式仅做MAC 地址的改写，不能改写目标端口，即真实服务器端口和VIP端口必须相同。

### LVS TUN模式

TUN（IP Tunneling）通过IP隧道技术实现。

LVS TUN模式常用于跨网段或跨机房的负载均衡，具体的报文转发流程如下。

（1）客户端将请求发送给前端的LVS，请求报文的源地址是CIP，目标地址是VIP。

（2）LVS在收到报文后，发现请求在规则里中存在，则将在客户端请求报文的首部再封装一层IP报文，将源地址改为DIP，将目标地址改为RIP，并将此包发送给真实服务器。

（3）真实服务器在收到请求报文后会先拆开第1层封装，因为发现里面还有一层IP首部的目标地址是自己lo接口上的VIP，所以会处理该请求报文，并将响应报文通过lo接口发送给eth0网卡，并最终发送给客户端。

TUN模式的特点

- UNNEL模式需要设置lo接口的VIP不能在公网上出现。
- TUNNEL模式必须在所有的真实服务器上绑定VIP的IP地址。
- TUNNEL 模式中VIP→真实服务器的包通信通过TUNNEL 隧道技术实现，不管是内网还是外网都能通信，所以不需要LVS和真实服务器在同一个网段内。
- 在TUNNEL 模式中，真实服务器会把响应报文直接发送给客户端而不经过LVS，负载能力较强。
- TUNNEL 模式采用的是隧道模式，使用方法相对复杂，一般用于跨机房LVS 实现，并且需要所有服务器都支持IP Tunneling或IPEncapsulation协议。

### LVS FULLNAT模式

无论是DR模式还是NAT模式，都要求LVS和真实服务器在同一个VLAN下，否则LVS无法作为真实服务器的网关，因此跨VLAN的真实服务器无法接入。同时，在流量增大、真实服务器水平扩容时，单点LVS会成为瓶颈。

FULLNAT能够很好地解决LVS和真实服务器跨VLAN的问题，在跨VLAN问题解决后，LVS和真实服务器不再存在VLAN上的从属关系，可以做到多个LVS对应多个真实服务器，解决水平扩容的问题。FULLNAT的原理是在NAT的基础上引入Local Address IP（内网IP地址），将CIP→VIP转换为LIP→RIP，而LIP和RIP均为IDC内网IP，可以通过交换机实现跨VLAN通信。

LVS FULLNAT具体的报文转发流程如下。

（1）客户端将请求发送给LVS的DNAT，请求报文的源地址是CIP，目标地址是VIP。

（2）LVS在收到数据后将源地址CIP修改成LIP（Local IP Address，LVS的内网IP），将目标地址VIP修改为RIP，并将数据发送到真实服务器。多个LIP在同一个IDC数据中心，可以通过交换机跨VLAN通信。

（3）真实服务器在收到数据包并处理完成后，将目标地址修改为LIP，将源地址修改为RIP，最终将这个数据包返回给LVS。

（4）LVS在收到数据包后，将数据包中的目标地址修改为CIP，将源地址修改为VIP，并将数据发送给客户端。

## 7.Nginx反向代理与负载均衡

一般的负载均衡软件如LVS实现的功能只是对请求数据包的转发和传递，从负载均衡下的节点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户；而反向代理服务器在接收到用户的访问请求后，会代理用户重新向节点服务器（We b服务器、文件服务器、视频服务器）发起请求，反向代理服务器和节点服务器做具体的数据交互，最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端就是反向代理服务器，而非真实的网站访问用户。

### upstream_module

ngx_http_upstream_module是Nginx的负载均衡模块，可以实现网站的负载均衡功能即节点的健康检查。upstream模块允许Nginx定义一组或多组节点服务器，在使用时可通过proxy_pass代理方式把网站的请求发送到事先定义好的对应Upstream组的名字上。

```html
upstream restLVSServer{
    server 191.168.1.10:9000 weight=5;
    server 191.168.1.11:9000;
    server example.com:9000 max_fails=2 fail_timeout=10s bakup;
}
```

如上代码定义了名为restLVSServer的upstream，并在其中定义了 3个服务地址，在用户请求restLVSServer服务时，Nginx会根据权重将请求转发到具体的服务器。常用的upstream配置如下。

- weight：服务器权重。
- max_fails：Nginx尝试连接后端服务器的最大失败次数，如果失败时大于max_fails，则认为该服务器不可用。
- fail_timeout：max_fails和fail_timeout一般会关联使用，如果某台服务器在fail_timeout时间内出现了max_fails次连接失败，那么Nginx会认为其已经挂掉，从而在fail_timeout时间内不再去请求它，fail_timeout默认是10s，max_fails默认是1，即在默认情况下只要发生错误就认为服务器挂了，如果将max_fails设置为0，则表示取消这项检查。
- backup：表示当前服务器是备用服务器，只有其他非backup后端服务器都挂掉或很忙时，才会分配请求给它。
- down：标志服务器永远不可用。

### proxy_pass

proxy_pass指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过location功能匹配指定的URI，然后把接收到的服务匹配URI的请求通过proxy_pass抛给定义好的upstream节点池。

```html
location /download/{
    proxy_pass http://192.168.1.13:9000/download/vedio/;
}
```

如上代码定义了一个download的反向代理，在客户端请求/download时，Nginx会将具体的请求转发给proxy_pass配置的地址处理请求，这里配置的地址是http://192.168.1.13:9000/download/vedio/。

常用proxy_pass配置:

- proxy_next_upstream：在什么情况下将请求传递到下一个upstream
- proxy_limite_rate：限制从后端服务器读取响应的速率
- proxy_set_header：设置HTTP请求header，后续请求会将header传给后端服务器节点
- client_body_buffer_size：客户端请求主体缓冲区的大小
- proxy_connect_timeout：代理与后端节点服务器连接的超时时间
- proxy_send_timeout：后端节点数据回传的超时时间
- proxy_read_timeout：设置Nginx从代理的后端服务器获取信息的时间，表示在连接成功建立后，Nginx等待后端服务器的响应时间
- proxy_buffer_size：设置缓冲区的大小
- proxy_buffers：设置缓冲区的数量和大小
- proxy_busy_buffers_size：由于设置系统很忙时可以使用的proxy_buffers大小
- proxy_temp_file_write_size：指定缓存临时文件的大小

## 8.CDN的原理

CDN（Content Delivery Network，内容分发网络）指基于部署在各地的机房服务器，通过中心平台的负载均衡、内容分发、调度的能力，使用户就近获取所需内容，降低网络延迟，提升用户访问的响应速度和体验度。

### CDN的关键技术

CDN的关键技术包括内容发布、内容路由、内容交换和性能管理，具体如下。

- 内容发布：借助建立索引、缓存、流分裂、组播等技术，将内容发布到网络上距离用户最近的中心机房。
- 内容路由：通过内容路由器中的重定向（DNS）机制，在多个中心机房的服务器上负载均衡用户的请求，使用户从最近的中心机房获取数据。
- 内容交换：根据内容的可用性、服务器的可用性及用户的背景，在缓存服务器上利用应用层交换、流分裂、重定向等技术，智能地平衡负载流量。
- 性能管理：通过内部和外部监控系统，获取网络部件的信息，测量内容发布的端到端性能（包丢失、延时、平均带宽、启动时间、帧速率等），保证网络处于最佳运行状态。

### CDN的主要特点

- 本地缓存（Cache）加速：将用户经常访问的数据（尤其静态数据）缓存在本地，以提升系统的响应速度和稳定性。
- 镜像服务：消除不同运营商之间的网络差异，实现跨运营商的网络加速，保证不同运营商网络中的用户都能得到良好的网络体验。
- 远程加速：利用DNS 负载均衡技术为用户选择服务质量最优的服务器，加快用户远程访问的速度。
- 带宽优化：自动生成服务器的远程镜像缓存服务器，远程用户在访问时从就近的缓存服务器上读取数据，减少远程访问的带宽，分担网络流量，并降低原站点的We b服务器负载等。
- 集群抗攻击：通过网络安全技术和CDN 之间的智能冗余机制，可以有效减少网络攻击对网站的影响。

### 内容分发系统

将用户请求的数据分发到就近的各个中心机房，以保障为用户提供快速、高效的内容服务。缓存的内容包括静态图片、视频、文本、用户最近访问的JSON数据等。缓存的技术包括内存环境、分布式缓存、本地文件缓存等。缓存的策略主要考虑缓存更新、缓存淘汰机制。

其基本的工作单元就是各个Cache服务器。负责直接响应用户请求，将内容快速分发到用户；同时还负责内容更新，保证和源站内容的同步。

根据内容类型和服务种类的不同，分发服务系统分为多个子服务系统，如：网页加速服务、流媒体加速服务、应用加速服务等。每个子服务系统都是一个分布式的服务集群，由功能类似、地域接近的分布部署的Cache集群组成。

在承担内容同步、更新和响应用户请求之外，分发服务系统还需要向上层的管理调度系统反馈各个Cache设备的健康状况、响应情况、内容缓存状况等，以便管理调度系统能够根据设定的策略决定由哪个Cache设备来响应用户的请求。

### 负载均衡系统

负载均衡系统是整个CDN系统的核心，负载均衡根据当前网络的流量分布、各中心机房服务器的负载和用户请求的特点将用户的请求负载到不同的中心机房或不同的服务器上，以保障用户内容访问的流畅性。负载均衡系统包括全局负载均衡（GSLB）和本地负载均衡（SLB）。

- 全局负载均衡主要指跨机房的负载均衡，通过DNS 解析或者应用层重定向技术将用户的请求负载到就近的中心机房上。
- 本地负载均衡主要指机房内部的负载均衡，一般通过缓存服务器，基于LVS、Nginx、服务网关等技术实现用户访问的负载。

### 管理系统

分为运营管理和网络管理子系统。网络管理系统实现对CDN系统的设备管理、拓扑管理、链路监控和故障管理，为管理员提供对全网资源的可视化的集中管理，通常用web方式实现。运营管理是对CDN系统的业务管理，负责处理业务层面的与外界系统交互所必须的一些收集、整理、交付工作。包括用户管理、产品管理、计费管理、统计分析等。

## 9.Nginx 有哪些作用？

http 协议代理 搭建虚拟主机 服务的反向代理 在反向代理中配置集群的负载均衡

## 10.什么是正向代理？

正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。

## 11.什么是反向代理？

反向代理（Reverse Proxy）方式是指以代理服务器来接受internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。

## 12.负载均衡的部署方式

负载均衡有三种部署方式：路由模式、桥接模式、服务直接返回模式。路由模式部署灵活，约60%的用户采用这种方式部署；桥接模式不改变现有的网络架构；服务直接返回（DSR）比较适合吞吐量大特别是内容分发的网络应用。约30%的用户采用这种模式。 1、路由模式（推荐） 路由模式的部署方式，服务器的网关必须设置成负载均衡机的LAN口地址，且与WAN口分署不同的逻辑网络。因此所有返回的流量也都经过负载均衡。这种方式对网络的改动小，能均衡任何下行流量。 2、桥接模式 桥接模式配置简单，不改变现有网络。负载均衡的WAN口和LAN口分别连接上行设备和下行服务器。LAN口不需要配置IP（WAN口与LAN口是桥连接），所有的服务器与负载均衡均在同一逻辑网络中。由于这种安装方式容错性差，网络架构缺乏弹性，对广播风暴及其他生成树协议循环相关联的错误敏感，因此一般不推荐这种安装架构。 3、服务直接返回模式 这种安装方式负载均衡的LAN口不使用，WAN口与服务器在同一个网络中，互联网的客户端访问负载均衡的虚IP（VIP），虚IP对应负载均衡机的WAN口，负载均衡根据策略将流量分发到服务器上，服务器直接响应客户端的请求。因此对于客户端而言，响应他的IP不是负载均衡机的虚IP（VIP），而是服务器自身的IP地址。也就是说返回的流量是不经过负载均衡的。因此这种方式适用大流量高带宽要求的服务。

## 13.常见的软件负载均衡技术

1、基于DNS的负载均衡 由于在DNS服务器中，可以为多个不同的地址配置相同的名字，最终查询这个名字的客户机将在解析这个名字时得到其中一个地址，所以这种代理方式是通过DNS服务中的随机名字解析域名和IP来实现负载均衡。 2、反向代理负载均衡（如Apache+JK2+Tomcat这种组合） 该种代理方式与普通的代理方式不同，标准代理方式是客户使用代理访问多个外部Web服务器，之所以被称为反向代理模式是因为这种代理方式是多个客户使用它访问内部Web服务器，而非访问外部服务器。 3、基于NAT（Network Address Translation）的负载均衡技术（如Linux VirtualServer，简称LVS） 该技术通过一个地址转换网关将每个外部连接均匀转换为不同的内部服务器地址，因此外部网络中的计算机就各自与自己转换得到的地址上的服务器进行通信，从而达到负载均衡的目的。其中网络地址转换网关位于外部地址和内部地址之间，不仅可以实现当外部客户机访问转换网关的某一外部地址时可以转发到某一映射的内部的地址上，还可使内部地址的计算机能访问外部网络。

## 14.负载均衡的算法

源地址哈希算法

源地址哈希法的思想是根据服务消费者请求客户端的IP地址，通过哈希函数计算得到一个哈希值，将此哈希值和服务器列表的大小进行取模运算，得到的结果便是要访问的服务器地址的序号。采用源地址哈希法进行负载均衡，相同的IP客户端，如果服务器列表不变，将映射到同一个后台服务器进行访问。

加权轮询算法

假如 Nginx 每收到6个客户端的请求，会把其中的1个转发给后端a，把其中的2个转发给后端b，把其中的3个转发给后端c。 加权轮询算法的结果，就是要生成一个服务器序列。每当有请求到来时，就依次从该序列中取出下一个服务器用于处理该请求。

加权随机算法

加权随机法跟加权轮询法类似，根据后台服务器不同的配置和负载情况，配置不同的权重。不同的是，它是按照权重来随机选取服务器的，而非顺序。

## 15.Nginx是如何处理一个HTTP请求的呢？

Nginx 是一个高性能的 Web 服务器，能够同时处理大量的并发请求。它结合多进程机制和异步机制 ， 异步机制使用的是异步非阻塞方式 ，接下来就给大家介绍一下 Nginx 的多线程机制和异步非阻塞机制。 1、多进程机制 服务器每当收到一个客户端时，就有 服务器主进程 （ master process ）生成一个 子进程（ worker process ）出来和客户端建立连接进行交互，直到连接断开，该子进程就结束了。 使用进程的好处是各个进程之间相互独立，不需要加锁，减少了使用锁对性能造成影响，同时降低编程 的复杂度，降低开发成本。其次，采用独立的进程，可以让进程互相之间不会影响 ，如果一个进程发生 异常退出时，其它进程正常工作， master 进程则很快启动新的 worker 进程，确保服务不会中断，从 而将风险降到最低。 缺点是操作系统生成一个子进程需要进行 内存复制等操作，在资源和时间上会产生一定的开销。当有大 量请求时，会导致系统性能下降 。 2、异步非阻塞机制 每个工作进程 使用 异步非阻塞方式 ，可以处理 多个客户端请求 。 当某个 工作进程 接收到客户端的请求以后，调用 IO 进行处理，如果不能立即得到结果，就去 处理其 他请求 （即为 非阻塞 ）；而 客户端 在此期间也 无需等待响应 ，可以去处理其他事情（即为 异步 ）。 当 IO 返回时，就会通知此 工作进程 ；该进程得到通知，暂时 挂起 当前处理的事务去 响应客户端请求

## 16.列举一些Nginx的特性

1. Nginx服务器的特性包括：
2. 反向代理/L7负载均衡器
3. 嵌入式Perl解释器
4. 动态二进制升级
5. 可用于重新编写URL，具有非常好的PCRE支持

## 17.在Nginx中，如何使用未定义的服务器名称来阻止处理请求？

只需将请求删除的服务器就可以定义为：

```html
Server{
listen 80;
server_name "";
return 444;
}
```

这里，服务器名被保留为一个空字符串，它将在没有“主机”头字段的情况下匹配请求，而一个特殊的 Nginx的非标准代码444被返回，从而终止连接。 一般推荐 worker 进程数与CPU内核数一致，这样一来不存在大量的子进程生成和管理任务，避免了进 程之间竞争CPU 资源和进程切换的开销。而且 Nginx 为了更好的利用 多核特性 ，提供了 CPU 亲缘性 的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 Cache 的失 效。 对于每个请求，有且只有一个工作进程 对其处理。首先，每个 worker 进程都是从 master进程 fork 过 来。在 master 进程里面，先建立好需要 listen 的 socket（listenfd） 之后，然后再 fork 出多个 worker 进程。 所有 worker 进程的 listenfd 会在新连接到来时变得可读 ，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢占 accept_mutex ，抢到互斥锁的那个进程注册 listenfd 读事 件 ，在读事件里调用 accept 接受该连接。 当一个 worker 进程在 accept 这个连接之后，就开始读取请求、解析请求、处理请求，产生数据后， 再返回给客户端 ，最后才断开连接。这样一个完整的请求就是这样的了。我们可以看到，一个请求，完 全由 worker 进程来处理，而且只在一个 worker 进程中处理。

在 Nginx 服务器的运行过程中， 主进程和工作进程 需要进程交互。交互依赖于 Socket 实现的管道来实 现。

## 18.请解释Nginx服务器上的Master和Worker进程分别是什么?

主程序 Master process 启动后，通过一个 for 循环来 接收 和 处理外部信号 ； 主进程通过 fork() 函数产生 worker 子进程 ，每个子进程执行一个 for循环来实现Nginx服务器对 事件的接收和处理 。

## 19.请解释代理中的正向代理和反向代理

首先，代理服务器一般指局域网内部的机器通过代理服务器发送请求到互联网上的服务器，代理服务器 一般作用在客户端。例如：GoAgent翻墙软件。我们的客户端在进行翻墙操作的时候，我们使用的正是 正向代理，通过正向代理的方式，在我们的客户端运行一个软件，将我们的HTTP请求转发到其他不同 的服务器端，实现请求的分发。

反向代理服务器作用在服务器端，它在服务器端接收客户端的请求，然后将请求分发给具体的服务器进 行处理，然后再将服务器的相应结果反馈给客户端。Nginx就是一个反向代理服务器组件

反向代理正好与正向代理相反，对于客户端而言代理服务器就像是原始服务器，并且客户端不需要进行 任何特别的设置。客户端向反向代理的命名空间（name-space）中的内容发送普通请求，接着反向代 理将判断向何处（原始服务器）转交请求，并将获得的内容返回给客户端。

