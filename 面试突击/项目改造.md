# 项目改造

### 1.初步改造方案

1.技术方案

eureka，ribbon + feign，zuul（默认就整合ribbon和hystrix）

2.微服务网关层

我们使用zuul来构建微服务架构的网关，起码部署两台机器，双机部署是最最起码的，这样子可以保证如果一台机器挂了，还有另外一台机器可以用，我们在生产系统里面，每个服务，最最起码是双机部署

3.业务服务层

一共将整个电商系统拆分为15个服务，每个服务部署两台机器，双机部署，避免单点故障，如果只部署一台机器，挂了整个就挂了，单点故障。

4.数据存储层

所有的服务都共用一个数据存储，也就是一个数据库，这个数据库就部署在一台机器上，刚开始是不做分库分表的，然后的话呢，就是在数据库服务器中，创建15个逻辑库，在一个MySQL中，创建15个database。

在微服务架构中，每个服务都是自己的代码仓库，每个服务都是自己的数据库，逻辑库，一个MySQL中的一个database。后面如果压力上来了，你就可以将不同的服务的数据库迁移到不同的机器上去。

5.服务注册中心

采用Eureka来作为服务注册中心，每个服务都是一个Eureka Client，会自动往Eureka Server去进行服务注册，eureka server是双机部署的，这样的话呢可以避免单点故障

6.服务调用

采用ribbon + feign进行服务调用，feign进行生命式的服务调用，ribbon进行服务调用的负载均衡

7.开发框架

每个服务的业务逻辑的开发，统一采用SSM框架（spring mvc + spring + mybatis + spring boot）

### 2.技术思考

1、服务注册中心

eureka server来做的，我们要来思考服务注册和发现的核心运行过程中，有哪些地方是需要我们来配置和调整的

1.1 eureka server的请求压力

他是双机部署，然后eureka server两台机器互相之间都会进行注册，完成eureka server集群的一个识别和构造

eureka server在线上的生产环境是否要部署很多台机器，没什么太大的必要，eureka server的设计原则，是纯粹基于自己的内存来设计的，也就是不会比如说依赖数据库，或者是网络请求，所以eureka server纯内存操作，都是很快的

每一台eureka server的机器都可以承载很多的并发请求，一台普通的4核8G的机器，部署eureka server，每秒钟接收个几百的请求都是问题不大的

你觉得eureka server的压力大吗？不大，其实各个服务都是eureka client，eureka client其实每隔30秒来一次心跳的请求，所以这个压力其实一点儿都不大，比如说你有100个服务，每隔服务部署2台机器，就有200个服务实例，30秒有200个心跳请求，每秒呢？每秒大概也就8~10个心跳请求

1.2 服务注册的时效性

spring cloud对eureka做了额外的封装，只要服务一旦启动，立马就会发出注册的请求，时效性基本在毫秒级

1.3 服务发现的时效性

服务刚启动的时候，发现其他所有服务的时效性

**服务刚启动的时候，立马发现其他所有服务的时效性是毫秒级，立马就会去发送一个请求，拿到所有的注册表**

如果说其他服务加了一台机器，此时这个服务要发现别的服务新增了一台机器，要过多久可以发现

eureka server的多级缓存的机制

比如说，现在服务A新增了一台机器，会更新到注册表中去，而且会立马过期掉原有的缓存，会立马过期掉这个ReadWriteCacheMap

很多人会天真的认为，服务A新增了一台机器，其他服务最多30秒之内一定会感知到服务A新增了一台机器，你如果考虑到eureka server内部的多级缓存的机制，你就会发现说，其实在极端情况下，服务A新增了一台机器，可能会要1分钟的时间，60s的时间，才能让其他服务感知到

每个服务拉取增量注册表的间隔，就是在初始化调度任务的代码那里看一下，默认的时间是多少，看看我们可以通过设置哪个参数来改变这个时间

int registryFetchIntervalSeconds = clientConfig.getRegistryFetchIntervalSeconds();

默认情况下，就是30秒，spring-cloud-netflix-eureka-client工程里的EurekaClientConfigBean里面，定义了所有参数的默认值，如果你要设置，就是eureka.client作为前缀，然后加上变量的名字，就可以设置自己的数值

eureka-core工程，ResponseCacheImpl，在spring-cloud-netflix-eureka-server工程里的EurekaServerConfigBean，里面对所有的参数都进行了定义，默认情况下也是30秒

如果要修改，eureka.server.responseCacheUpdateIntervalMs = 10000

**eureka服务发现是分钟级的时效性**

1.4 服务心跳的间隔

任何一个服务启动之后都会定时发送心跳的通知，通知eureka server自己还是存活的状态

心跳的间隔是多长时间

在spring-cloud-netflix-eureka-client工程里，有一个EurekaInstanceConfigBean，这个里面就定义了默认的发送心跳的时间，就是30秒

eureka.client.leaseRenewalIntervalInSeconds = 30

1.5 服务故障的自动感知的时效性

服务故障了，eureka server过了多久可以感知到服务故障了，然后将这个服务给他下线呢？

首先在eureka server中，是每隔60s去执行一次evict task，去判断一下当前所有的服务实例，是否有的服务实例出现了故障，一直没有发送心跳过来，是否要将故障的服务实例给他下线呢

eureka.server.evictionIntervalTimerInMs = 60 * 1000

按照这套evict task执行的机制，还有eureka本身的一个bug（90s * 2），很可能在极端情况下，从一个服务宕机之后，到evict task发现和判定这个服务已经故障，可能最多要差不多4分钟，最少也得2分钟~3分钟

发现故障了以后，从服务注册表中摘除，然后过期掉readWriteCacheMap缓存

加上两个缓存map的同步，以及其他服务30s一次增量拉取的成本，很可能在极端情况下，服务B是要过了将近5分钟才能感知到服务A的某台机器故障，宕机了。即使不在极端情况下，其他服务要感知到某个服务实例的故障，起码也要三分钟~四分钟

**我们一般都是按照极端情况来计算的，服务实例故障，自动感知的时效性，服务A某台机器宕机了，服务B感知服务A那台机器宕机了，时效性在5分钟以内，起码三四分钟**

引申出来的一个问题，如果说服务A某台机器宕机了，要3分钟以后服务B才能感知到，此时3分钟以内，服务A请求服务B的那台机器就会是失败的，此时怎么办呢？所以才要做超时设置（多少秒之内请求不成功就timeout），失败重试（服务A的一台机器挂了，你可以重试服务A的其他机器）

eureka.instance.leaseExpirationDurationInSeconds = 90

1.6 服务下线的感知的时效性

服务正常下线的话，会怎么样呢？这块的时效性，跟之前说的那个服务新增了一台机器，新增了一个服务实例，那么一般感知时效性在1分钟以内。**如果服务正常下线，执行DiscoveryClient的shutown()方法，此时其他服务感知到这个服务实例下线，也是在1分钟以内。**

1.7 eureka server的自我保护的稳定性

在eureka的自我保护机制的代码里，大量的运用了hard code硬编码，惨痛一点，他默认你的心跳的时间间隔是30s，一分钟就2次心跳，也就是说你压根儿就不能去修改心跳的间隔，否则就会跟eureka自我保护机制的hard code硬编码的代码出现冲突

在eureka的自我保护代码里，充斥了大量的问题和bug，在尤其是测试环境下，你会发现经常就是动不动就进入自我保护的模式，说实话自我保护模式非常不稳定，完全不适合生产环境来使用

否则如果在生产环境中，动不动进入自我保护的模式，你平时服务故障了，他都不会去进行服务的故障感知和实例摘除，那这个事情就坑了

我之前做测试的时候，包括我们的一些同学做测试的时候，经常会发现每分钟期望的心跳次数是算错了，跟我们期望的是不符合的，计算每分钟的心跳次数，如果心跳次数<期望的心跳次数，就进入自我保护机制，不让evict task摘除任何服务实例

不要用，这么垃圾的代码，写的这么不靠谱的机制，不要用，在生产环境，测试环境都直接关了

eureka.server.enableSelfPreservation = false

1.8 eureka server集群的负载均衡

如果eureka server部署集群的时候，各个服务在注册或者是发送心跳，是如何请求eureka server集群的呢？

**你的每个服务里，会配置一个eureka server列表，谁配置在第一个，所有的服务优先就是访问那个eureka server。然后如果那台eureka server宕机了，那么此时所有的服务在重试过后都会访问其他的eureka server，而且此后大家都会去访问那台eureka server。**

1.9 eureka server集群同步的时效性

你配置了多少台server机器，其中接收到请求的那条机器，就会将请求转发给其他所有的机器

将这个集群数据同步的任务，会放入一个acceptorQueue里面去，AcceptorRunner后台线程来处理，每隔10ms会执行一次业务逻辑

中间有一个打包的过程，他默认会将500ms内的请求，注册、心跳、下线，打成一个batch，再一次性将这个batch发送给其他的机器，减少网络通信的次数，减少网络通信的开销，集群同步的批量处理的机制

**eureka server集群同步的时效性，基本上是在1s内，几百毫秒都是正常的**

2、服务调用

ribbon + feign，我们主要是面向feign来做，ribbon作为feign底层依赖的这么一套机制来搞的

2.1 ribbon + eureka服务发现的时效性

eureka client感知到其他服务上线了一个新的服务实例，1分钟以内，几十秒；eureka client感知到其他服务有个服务实例宕机了，大概是5分钟以内，三四分钟

比如说购物车服务要访问库存服务，刚开始库存服务就一台机器

后来某天，库存服务进行服务扩容，新增了1台机器，此时购物车服务本地的eureka client大概是1分钟才感知到人家新增了1台机器，ribbon的PollingServerListUpdater，刚好是30秒过后去刷新本地的eureka client的注册表到ribbon内部去

ribbon感知到库存服务新增1台机器，可能又过了30秒了，1.5分钟，1分30秒，如果比较快呢，1分钟组左右

目前库存服务有2台机器，其中1台机器宕机了，此时购物车服务本地的eureka client大概也需要三四分钟，最长是5分钟时间感知到，ribbon每隔30秒刷新eureka client的注册表到ribbon内部，极限情况下，ribbon感知到库存服务某台机器宕机了，可能需要5.5分钟，正常也需要个4分钟左右

2.2 ribbon的负载均衡算法

spring cloud环境下，ZoneAwareLoadBalancer，机房感知负载均衡器，比如说如果是多机房部署的话，比如在上海和北京，各部署了一部分机器在一个机房里，你一个系统，购物车服务有10台机器，库存服务有10台机器

在北京机房里，购物车服务放了5台机器，库存服务放了5台机器；在上海机房里，购物车服务也是5台机器，库存服务也是5台机器

然后ZoneAwareLoadBalancer，比如说北京机房里的购物车服务，现在要访问库存服务，是针对库存服务两个机房的10台机器去做负载均衡吗？优先是同机房访问，北京机房的机器，尽可能就是优先访问自己北京机房里的库存服务的5台机器

那么就是在北京机房的库存服务的5台机器中进行负载均衡

对于绝对多数的中小型公司来说，没有机房的概念，抛弃掉机房的事儿不考虑的话，那么他的负载均衡的算法，就是最最基础的round robin轮询算法，每次请求下一台机器，对一个机器列表，循环往复的请求

2.3 ribbon的服务故障感知的时效性

比如库存服务有2台机器，其中一台机器故障宕机了，如果要让依赖库存服务的购物车服务感知到，可能需要几分钟的时间，三四分钟，五六分钟，都有可能

比如说在这几分钟的时间里面，ribbon内部的保存的库存服务的server list，还是2台机器，此时不断的请求过来，ribbon负载均衡算法还是会不断的将请求流量分发给库存服务已经宕机的那台机器

如果请求已经宕机的一台机器，一定是会有问题的，连接一定会连接不上去的，connect timeout异常和报错，如果说你不做任何处理的话，可能会在短时间内导致大量的请求都会失败

超时和重试的参数

```html
ribbon:
    ConnectTimeout: 1000
    ReadTimeout: 1000
    OkToRetryOnAllOperations: true
    MaxAutoRetries: 1
    MaxAutoRetriesNextServer: 3
```

ConnectTimeout：连接一台机器的超时时间

ReadTimeout：向一台机器发起请求的时间

重点是通过实验来设置好MaxAutoRetries和MaxAutoRetriesNextServer这两个参数

2.4 feign的服务调用的超时

2.5 feign的服务调用的失败重试

3、服务网关

3.1 ribbon预加载

第一次请求zuul的时候是很慢的，很容易超时

```html
zuul:
    ribbon:
        eager-load:
            enabled: true
```

3.2 zuul + ribbon + eureka感知服务上线和故障的时效性

zuul 和 feign在这块是类似的，ribbon + eureka，eureka client感知到服务新实例上线，大概可能要个1分钟以内，ribbon那里，1.5分钟，1分钟左右，zuul也是一样的

如果说购物车服务现在有2台机器，但是不幸的是有1台机器宕机了，此时怎么办呢？eureka client大概可能需要5分钟以内的时间才能感知到，ribbon，5.5分钟，三四分钟，四五分钟。zuul的ribbon，感知到服务实例故障了，四五分钟

在某个服务实例宕机了，zuul还是不停的将请求转发过去会怎么样呢？肯定会失败，请求不了，此时zuul默认就是整合hystrix，hystrix会感知请求失败，异常了，直接会走hystrix的降级的逻辑，是什么都没有的，所以异常直接就会抛出来

zuul会有一个统一的error filter，会将这个异常给打印出来，反馈到调用方那里去

3.3 **请求超时和重试**

如果说某个服务实例突然挂了，zuul默认情况下是无法处理这种情况的，所以在线上生产环境下是很要命的，绝对不能让zuul直接这样子裸奔，不设置任何重试的机制直接就上了。如果某个服务实例挂了，打印error日志，给前端返回一段异常的json串：

```html
{
  "timestamp": 1532067539032,
  "status": 500,
  "error": "Internal Server Error",
  "exception": "com.netflix.zuul.exception.ZuulException",
  "message": "GENERAL"
} 
```

默认情况下，hystrix实际上虽然是用了，但是没有这个降级的策略，默认情况下，无论是被hystrix线程池拒绝（被限流）、请求超时（出现过几次请求超时的现象）、发生了异常（某个服务实例宕机了）

抛出异常，hystrix降级没有办法做任何处理，异常只能被打印出来，封装成一个json串法功给前端浏览器来显示，或者是将json串发送给调用zuul网关的android、ios、PC前端，抛出来的这个异常，统一都是ZuulException，其实具体是什么异常，他是不告诉你的，他只告诉你内部出现了异常和错误

不是特别好，起码针对服务实例可能会挂、或者是可能会偶尔出现网络调用超时（失败），设置个最基本的重试的策略

zuul也是用的hystrix + ribbon那套东西，所以说，超时这里要考虑hystrix和ribbon的，而且hystrix的超时要考虑ribbon的重试次数和单次超时时间

ribbon的超时，hystrix的超时，zuul默认启用了hystrix，你要考虑到hystrix的超时

hystrix是包裹了ribbno的使用的，一般来说，hystrix的超时时长必须大于ribbon的超时时长，否则如果hystrix设置了超时是1s，ribbon设置的超时时长是2s，那么ribbon其实还没超时，hystrix直接就超时了

必须是hystrix超时时长最好是远大于ribbon超时时长，超时和重试尽量都是以ribbon为主

hystrix的超时时间计算公式如下：

(ribbon.ConnectTimeout + ribbon.ReadTimeout) * (ribbon.MaxAutoRetries + 1) * (ribbon.MaxAutoRetriesNextServer + 1)

```html
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 10000
```

如果不配置ribbon的超时时间，默认的hystrix超时时间是4000ms（4s）

4、开发框架

SSM + Spring Boot

5、数据库

MySQL，一台机器，部署一个mysql服务，在里面创建15个逻辑库，15个中心，每个中心对应一个逻辑库

在微服务的架构中，有一点是非常的重要的，服务与服务之间一般是不会共享数据库的，每个服务一般是使用自己独立的一个数据库，然后如果某个服务要访问另外一个服务的数据的话，不能直接查其他服务的数据库的，而是通过其他服务暴露出来的接口来访问

6、工程拆分

我们一共会将电商系统拆分为15个中心，15个工程，每个服务一个工程，每个服务的工程对应一个独立的git仓库，每个服务都是完全独立的，每个服务都是在eclipse里是单独的一个工程，有自己独立的代码仓库，有自己独立的集成测试机器、QA测试机器、staging测试机器、prod生产机器，独立的部署，独立的数据库（某台数据库服务器上的一个逻辑库，比如mysql中的一个database）

### 3.服务拆分

spring boot，是作为一个parent父工程引入进来，约束了很多依赖的版本

spring cloud，是作为dependency management引入进来的，约束了很多依赖的版本

所以说大家只要保证spring boot和spring cloud版本基本一致就ok了

大量的服务与服务之间的调用，都是用的那种很复杂的domain类，传递大量的无效的字段。实际上在开发的时候，如果是分布式的系统，考虑网络间通信的效率问题，你可以尽可能的减少互相之间传递的参数的数量，可以提高网络通信的效率。

### 4.引入MQ消息中间件

库存服务，这块，其实是将库存更新的消息，异步发送到一个内存队列中的，我们之前是用的内存队列，但是现在的话呢，我们没法用这个内存队列了，因为我们的库存服务和调度服务给隔离开来了，独立部署，可能都不在一台机器上

库存服务发送库存更新的消息，到MQ（消息中间件），独立部署的中间件，调度服务就去消费这个MQ中的消息，拿到了额消息之后，调度服务再来更新自己本地的库存

常见情况

1、MQ的技术选型：你要对常见的几种MQ技术都有一点点的了解，而且知道在什么情况下选用哪种MQ

2、消息队列，部署为一个集群，这个东西呢，我们会在后面给大家来讲解，这块东西在项目阶段二中不会深入去讲解，但是在面试突击课程中，剖析了一下常见的MQ技术的高可用的原理

3、消息队列，消费到重复数据怎么办

4、消息队列，发送过去的数据丢失了怎么办

5、消息队列中的数据如何按照顺序来发送与消费

6、大量的消息积压在消息队列中该怎么办呢

7、消息队列的核心原理（你如何来设计一个消息中间件）

### 5.RabbitMQ发送与接收消息

你发送的每条消息，只会被一个worker（consumer）消费到和处理，consumer有3个，rabbitmq会被一个队列中的消息按照round robin轮询的算法，依次将消息推送给每个consumer，每个consumer都会接受到1/3的消息

ack机制

如果某个consumer消费一个消息，消费到一半儿就宕机了，这个消息没处理完，就会导致消息的丢失，消息队列中的数据丢失

consumer会自动进行ack，通知rabbitmq自己消费完了一个消息，自动ack的机制，可能什么呢？某个消息还在处理中，consumer就自动通知rabbitmq这个消息处理完了。后面宕机了，消息丢了。

consumer将自动ack关闭掉，然后自己在处理完一个任务之后，确定处理完毕了，再手动执行ack机制，通知rabbitmq处理完毕。如果处理到一半的时候，宕机了，rabbitmq没有收到这个消息的ack通知，就会将消息分发给其他的consumer再次处理

durability持久化机制

如果某个消息发送到了rabbitmq中，还没有来得及推送给消费者，此时rabbitmq自己宕机了，也会导致消息会丢失

消息的持久化机制

生产者在发送消息的时候，发送到rabbitmq之后，rabbitmq会将消息持久化到磁盘上去，然后才会通知生产者说ok，此次消息发送成功了。如果rabbitmq宕机了，没有将消息持久化到磁盘上去，此时生产者写入会报错，然后你就需要不断的重试写入，直到rabbitmq恢复正常，保证消息不会丢失

消息分发的均匀性

有的消息处理很耗时，有的消息处理不耗时，此时就会导致有的consumer一直在处理很耗时的消息，接收到的消息比较少；另外的consumer处理的是不耗时的消息，消息是均匀分发的，分发给一个worker，再是下一个worker。如果有的worker一直在处理耗时的任务，就会导致其他的任务处理的可能就是不耗时的任务，一直比较空闲

除非是一个worker已经处理完了一个message，而且通知了ack以后，才会给这个worker再次分发下一个消息，如果某个worker在处理耗时的任务，还没处理完，rabbitmq就不会将其他的消息分发给这个worker了

他会直接将其他的耗时的任务分发给其他的worker

### 6.spring cloud streaming整合rabbitmq

**1、加入rabbitmq配置**

在application.yml中分别加入rabbitmq的配置：

```html
rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest
```

**2、编写生产者**

在ServiceB中加入以下的一些依赖：

```html
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
```

编写发送消息的接口

```html
public interface MessageService {
    @Output(“test-queue”)
    SubscribableChannel testQueue();
}  
```

在启动类上加入注解

@EnableBinding(MessageService.class)

然后在一个Controller中发送消息

```html
public class ServiceBController {
    @Autowired
    private MessageService messageService;

    public String sendMessage() {
        Message message = MessageBuilder.withPayload(“hello world”.getBytes()).build();
        messageService.testQueue().send(message);
        return “SUCCESS”;
    } 
}
```

**3、编写消费者**

在application.yml中加入消费组：

如果你不设置这个消费组，会导致ServiceA启动了两台机器，每条消息都会推送给每台机器。但是如果你将ServiceA所有机器的消费组设置为一个，那么就是在各台机器之间round robin轮询发送消息

```html
spring:
	application:
		name: ServiceA
	cloud:
		stream:
			bindings:
				my-test-queue:
					group: groupA
```

在ServiceA中加入以下的一些依赖：

```html
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
```

编写消息接口

```html
public interface MessageService {
	@Input(“test-queue”)
	SubscribableChannel testQueue();
}
```

在启动类上加入注解

```html
@EnableBinding(MessageService.class)
@StreamListener(“test-queue”)
public void receive(byte[] message) {
	System.out.println(“接收到的消息是：” + new String(message));
}
```

只要消息可以发送，可以接受，多个消费者可以均匀的接受一部分消息，就成功了，ack机制、持久化机制、均匀分发，涉及到一些参数的设置，刚开始用mq，你不搞也无所谓，在你的系统负载很低的时候，故障很少出现的

服务B发送的消息，服务A只有一台机器会接受到这个消息

### 7.问题

1、接口调用超时重试的时候，导致了大量脏数据的产生【解决，只要给业务表增加唯一索引就可以了，保证其业务上没有脏数据的产生】

2、在spring cloud环境下，多个服务叠加在一起，会出现超乎寻常的大量的重试，我们要判断一下，这个重试是否正常

3、wms服务中，出现了很严重的mysql的deadlock死锁的问题，导致wms服务的接口响应的性能很差

4、对本地数据库的操作是纯事务，不应该将本地数据库操作跟远程接口的调用，混在一块儿，可能会出现莫名其妙的事务之间锁争用的问题，通过分布式系统，触发了一个分布式系统场景下的问题

5、对于这种插入数据的操作，我们是否要进行超时重试呢，如果要进行超时重试，就有可能会产生脏数据，那么我们就必须保证这个接口的幂等性

什么叫做分布式系统接口的幂等性，是什么意思呢？就是说，可能会有人重试你几次，发送一样的请求，你必须保证一样的请求不能重复的更新数据，否则就会导致数据出现脏数据。。。。

# 自研分布式微服务注册中心

### 1.Eureka、ZooKeeper、Consul等服务注册中心有什么问题

第一个问题，默认的参数配置下，服务发现的速度是很慢的，一般来说要几十秒或者几分钟才能发现一个新的服务，或者感知到一个服务的下线。可以通过配置他的各种参数，让他的服务发现和故障感知的时效性提高到1s以内

比如eureka有3台机器，你90个服务实例，30个服务实例对eureka01发起请求，30个服务实例对eureka02发起请求，30个服务实例对eureka03发起请求，eureka01要同步给eureka02和eureka03，90个服务他们每秒比如发起大量的请求，eureka每台机器都要承受所有服务的请求，服务实例越来越多，每秒会有心跳，还会有拉取注册表，eureka单机就会出现性能瓶颈，超过他单机能承载的一个最大的量了

服务发现和故障感知 -> 其他所有的服务都要监听zk里一个znode的子节点的变化，如果有子节点变化，那么zk就会反向通知所有的其他服务说，有服务变化了，可能是上线、下线、崩溃

zk最多就是给几百个或者上千个服务实例反向推送变化的服务注册表的数据，量也不是很大，100KB，瞬间反向推送给几百个上千个服务实例，瞬间会通过网卡输出最多上百MB的数据出去

频繁的服务上线和下线，会导致zk频繁的打爆自己的网卡，不可能在1秒内就全部推送出去，慢慢推送，网卡被打爆掉，甚至会影响zk集群间的通信，zk可能会不稳定，甚至挂掉；或者是服务注册的时候就会遇到zk的报错，zk会极为的不稳定

### 2.Raft协议：什么是分布式一致性协议

（1）集群里多台机器，只有一个人可以接收写入请求，他就是leader，集群需要选举出来一个人当做leader

（2）leader收到写入请求之后，需要采取一定的方案同步给集群里其他的机器

（3）如果leader崩溃掉了，其他的机器就会重新选举出来一个leader

划分出来一个主从的角色，leader、follower

### 3.Raft协议：如何实现Leader选举？

启动了之后，一般来说，3台机器，每个人都会投票给自己，选举自己当选为leader，他对自己的投票会发给其他的机器，选不出来leader，3台机器，最起码需要2条机器，大多数人都认可你当leader，你才可以当选为leader

Raft协议，如果一轮投票，发现大家没有选举出来一个leader，此时如何呢？大家都走一个随机时间的等待，timeout时间过后，再次发起第二轮选举。机器01选择休眠等待3秒钟，机器02选择休眠等待1.5秒钟，机器03选择休眠等待4秒钟

第二轮选举的时候，机器02先苏醒过来，发现进入了第二轮投票，他发现此时没有人发送选票给他，他就还是选举自己当做leader，发送给了机器01和机器03

机器02：投票给自己，机器02，机器02

机器01：机器02，投票给机器02，机器02

机器03：机器02，机器02，投票给机器02

大家发现选票都投完了，发现超过半数的人（全票）都投给了机器02，此时机器02当选为leader，Raft协议本质就是通过随机休眠时间保证说一定会在某一轮中投票出来一个人当选为leader

### 4.Raft协议：2PC与过半写机制

心跳：每个服务定期发送心跳给leader，如果发现某个服务超过一定时间没有发送心跳，自动认为他宕机了

刚开始leader受到一个注册请求，uncommitted，他把这个uncommitted请求发送给各个follower，作为第一个阶段，各个follower收到uncommitted请求之后，返回ack给leader，如果leader收到了超过半数的follower的ack

此时leader就把这个请求标记为committed，同时发送committed请求给所有的follower，让他们也对消息进行committed

跟zk的ZAB协议是一致的

### 5.Raft协议：Leader的崩溃恢复机制

有服务，注册的请求还没发送过去，leader就崩溃了，此时进入了在剩下的两台follower中重新选举leader过程中，还没有诞生一个新的leader，服务注册的请求是失败的，无法发送，你需要不停的重试

如果说你的注册请求发送到了一个leader上去，此时uncommitted状态，没有发送uncommitted请求到其他follower上去，leader崩溃了，此时这个注册请求是失败的，需要服务不停的重试，选举出来新的Leader

如果说你的注册请求发送到了leader上去，也发送了uncommitted请求到其他follower上去，部分follower收到了请求，但是还没达到半数follower返回ack给Leader，leader就挂了，会导致服务也认为服务注册强求失败了

如果说已经超过了半数的uncommitted请求的ack给leader了，服务注册请求已经成功了，此时leader崩溃了，选举一个新的leader，会去接收其他follower的ack，如果超过半数follower有ack，直接commit操作

跟zookeeper几乎是类似的

### 6.客户端是如何进行服务发现的

参考eureka的服务发现的多级缓存的架构，主要是用于避免掉对同一个内存数据结构频繁的读写出现大量的锁互斥，多线程频繁读写，避免涉及到大量的加锁和等待的过程

刚开始的时候，服务发现，会先找ReadOnlyMap，如果没有就找ReadWriteMap，如果没有就从核心注册表加载出来放入ReadWriteMap中以及ReadOnlyMap中，后续服务会每隔一段时间去加载注册表

默认情况，就是从ReadOnlyMap里去加载

如果注册表出现了变更，此时会直接invalid删除ReadWriteMap中所有的数据，一个后台线程会每隔一段时间检查ReadOnlyMap和ReadWriteMap两者数据是否一致，如果不一致，就是发现ReadWriteMap里空了，此时会清空ReadOnlyMap

下次别人服务过来加载注册表，看到ReadOnlyMap是空的，直接就会从核心注册表里加载数据，填充两个map就可以了

### 7.基于Raft协议的注册中心的本质：容量受限于单机

所有的写请求都得基于leader去完成，如果你有大量的服务实例，上万服务实例，心跳间隔调的很低，100ms，1s一次，拉取服务注册表的间隔调节的很低的话，此时你的leader一定压力是会很大的

单机都要上万，甚至是几万并发的请求都是有可能的

服务注册表的数据量特别大，因为包含了大量的服务实例，几万，几十万

单台机器的内存容量是有限的，我升级leader的机器配置，24核48G，让他去一台机器抗超高并发，抗超大数据量

leader承载所有的写，leader和follower每台机器保留的都是完整的数据，所有注册表数据都在一台机器上，一台leader机器抗所有的写，从架构的角度而言，单机瓶颈，这样也是有问题的

### 8.Peers以及Raft协议下的服务注册中心的缺陷

eureka：纯peers，任何一个节点都可以接收读写请求，每台机器都要承载所有的写入的请求，把心跳上报的频率调节的很小，把拉取注册表的频率调节的很小，保证你的服务发现的时效性，注册表可能会很大，每个人都频繁的来拉取

zookeeper以及类似的基于raft协议的注册中心：类似的问题

单机抗所有并发，无法伸缩

单机抗所有数据，内存可能不够，没有办法进行伸缩

单机网卡流量有限，服务太多会打满网卡

机器，都是用千兆网卡，理论上每秒就是传输100+MB，一般还到不了，每秒也就传输几十MB这样子

### 9.心跳机制

基于数据分片机制正常上报

shard概念是非常，机器崩溃的时候，集群扩容，用一个shard去聚合多个服务的数据，服务就是跟一个shard严密的绑定在一起的，让我们的每个服务跟master之间建立一个长连接其实就可以了

心跳，应该频率高一些，服务就通过一个跟master机器的一个长连接，每秒钟都上报一个心跳过去

采用什么样的频率依据心跳进行故障感知？

定时去检测各个shard里面的服务，最近一段时间是否上报了心跳，如果心跳正常，那么就不用做什么了，但是如果超过了一定的时间，没有心跳过来，此时你就应该认为这个服务就故障了

立马就反向通知订阅这个服务的其他服务就可以了

如果master机器发现某个服务默认超过5秒钟都没有心跳，立即就认为他是宕机了

服务发现的长连接能否基于Slave机器进行？

服务A的shard对应的应该是一个机器组的概念，模仿RocketMQ里面的broker group形成的一个概念，进行主从同步的几台机器，应该是可以形成一个组的，此时我们可以从这个组里的master+slaves里面，随机挑选出来一台机器

对于slave，他可以仅仅是复制心跳，对于slave角色而言，是不会启动检查心跳的线程

如果Slave机器挂了如何进行长连接转移？

一旦slave挂了，之前跟这个slave建立上连接的服务，立马随机从组里重新挑选一台机器出来，可能是master，也可能是其他的slave

如果Master机器挂了，如何让Slave热切换为Master？

对于一个组内的master而言，他应该是说要定时的给slave发送心跳的，告诉slave自己当前其实还活着，每秒钟发送一次心跳，各个slave一旦发现比如说超过5秒没收到这个master发送过来的心跳，此时大家就判断说，这个master必死

必须得有一个新的master出现，多个slave是否需要进行一下选举呢？

也可以基于raft协议去进行一个选举，如果只有一个slave必然是这个slave去进行接管；在这里，不要基于raft协议随便去选一个master出来，各个slave之间还是进行投票，不是随机休眠挑选一个slave

每个人去比较，谁之前从master接收到的数据是最多的，就让谁当新的master，完全是仿照zookeeper来做的；如果大家接收到的数据是一样多的，此时可以投票让slaveId最大的那个机器成为新的master

对于其他的服务而言，他们应该都转变为每隔5秒钟，去Controller节点尝试拉取这个分组里最新的master是谁，Controller主要就是管集群里有哪些机器，shard在各个分组的分配是如何定的

新的master选举出来之后，就去找controller进行一个注册

对宕机的Master机器，如何采用Slave角色进行恢复？

必须得以slave的方式去进行一个启动，让他去连接到当前最新的master上去，跟新的master进行数据恢复，另外一个新的master感知到有slave挂载上来了，此时必然就会让一部分的机器转移长连接到新的slave上去，进行rebalance