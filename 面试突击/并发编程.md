# 并发编程

## 线程

### 1.**并发与并行**

什么是并发

并发（Concurrent），在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行。

那么，操作系统视如何实现这种并发的呢？

现在我们用到操作系统，无论是Windows、Linux还是MacOS等其实都是**多用户多任务分时操作系统**。使用这些操作系统的用户是可以“同时”干多件事的。

但是实际上，对于单CPU的计算机来说，在CPU中，同一时间是只能干一件事儿的。为了看起来像是“同时干多件事”，分时操作系统是把CPU的时间划分成长短基本相同的时间区间,即”时间片”，通过操作系统的管理，把这些时间片依次轮流地分配给各个用户使用。

如果某个作业在时间片结束之前,整个任务还没有完成，那么该作业就被暂停下来,放弃CPU，等待下一轮循环再继续做.此时CPU又分配给另一个作业去使用。

由于计算机的处理速度很快，只要时间片的间隔取得适当,那么一个用户作业从用完分配给它的一个时间片到获得下一个CPU时间片，中间有所”停顿”，但用户察觉不出来,好像整个系统全由它”独占”似的。

所以，在单CPU的计算机中，我们看起来“同时干多件事”，其实是通过CPU时间片技术，并发完成的。

什么是并行

并行（Parallel），当系统有一个以上CPU时，当一个CPU执行一个进程时，另一个CPU可以执行另一个进程，两个进程互不抢占CPU资源，可以同时进行，这种方式我们称之为并行(Parallel)。

### 2.线程与进程的区别

为了看起来像是“同时干多件事”，分时操作系统是把CPU的时间划分成长短基本相同的”时间片”，通过操作系统的管理，把这些时间片依次轮流地分配给各个用户的各个任务使用。

在多任务处理系统中，CPU需要处理所有程序的操作，当用户来回切换它们时，需要记录这些程序执行到哪里。在操作系统中，CPU切换到另一个进程需要保存当前进程的状态并恢复另一个进程的状态：当前运行任务转为就绪（或者挂起、删除）状态，另一个被选定的就绪任务成为当前任务。**上下文切换**就是这样一个过程，他允许CPU记录并恢复各种正在运行程序的状态，使它能够完成切换操作。

在程序中，上下文切换过程中的“页码”信息是保存在进程控制块（PCB）中的。PCB还经常被称作“切换帧”（switchframe）。“页码”信息会一直保存到CPU的内存中，直到他们被再次使用。

对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。

在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。

**进程是资源分配的基本单元，线程是执行的基本单元，同一个进程的多个线程之间共享资源**。

### 3.线程的特点

在多线程操作系统中，通常是在一个进程中包括多个线程，每个线程都是作为利用CPU的基本单位，是花费最小开销的实体。线程具有以下属性。

轻型实体

线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。 线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。TCB包括以下信息： （1）线程状态。 （2）当线程不运行时，被保存的现场资源。 （3）一组执行堆栈。 （4）存放每个线程的局部变量主存区。 （5）访问同一个进程中的主存和其它资源。 用于指示被执行指令序列的程序计数器、保留局部变量、少数状态参数和返回地址等的一组寄存器和堆栈。

独立调度和分派的基本单位

在多线程操作系统中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。

可并发执行

在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。

共享进程资源

在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。

### 4.线程的实现

主流的操作系统都提供了线程实现，实现线程主要有3种方式：使用内核线程实现、使用用户线程实现和使用用户线程加轻量级进程混合实现。

使用内核线程实现

内核线程（Kernel-Level Thread,KLT）就是直接由操作系统内核（Kernel，下称内核）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就叫做多线程内核（Multi-Threads Kernel）。

程序一般不会直接去使用内核线程，而是去使用内核线程的一种高级接口——轻量级进程（Light Weight Process,LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。

由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使有一个轻量级进程在系统调用中阻塞了，也不会影响整个进程继续工作，但是轻量级进程具有它的局限性：首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换。其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程的数量是有限的。

使用用户线程实现

从广义上来讲，一个线程只要不是内核线程，就可以认为是用户线程（User Thread,UT），因此，从这个定义上来讲，轻量级进程也属于用户线程，但轻量级进程的实现始终是建立在内核之上的，许多操作都要进行系统调用，效率会受到限制。

而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知线程存在的实现。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也可以支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。

使用用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要用户程序自己处理。线程的创建、切换和调度都是需要考虑的问题，而且由于操作系统只把处理器资源分配到进程，那诸如“阻塞如何处理”、“多处理器系统中如何将线程映射到其他处理器上”这类问题解决起来将会异常困难，甚至不可能完成。因而使用用户线程实现的程序一般都比较复杂。

使用用户线程加轻量级进程混合实现

线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式。在这种混合实现下，既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发。而操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级线程来完成，大大降低了整个进程被完全阻塞的风险。在这种混合模式中，用户线程与轻量级进程的数量比是不定的，即为N：M的关系。

### 5.线程的状态

线程是有状态的，并且这些状态之间也是可以互相流转的。Java中线程的状态分为6种：

- 1.初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
- 1.运行(RUNNABLE)：Java线程中将就绪（READY）和运行中（RUNNING）两种状态笼统的称为“运行”。
  - 就绪（READY）:线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中并分配cpu使用权 。
  - 运行中（RUNNING）：就绪(READY)的线程获得了cpu 时间片，开始执行程序代码。
- 3.阻塞(BLOCKED)：表示线程阻塞于锁（关于锁，在后面章节会介绍）。
- 4.等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
- 5.超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
- 6.终止(TERMINATED)：表示该线程已经执行完毕。

### 6.线程调度

**给多个线程按照特定的机制分配CPU的使用权的过程就叫做线程调度。**

Linux线程调度

在Linux中，线程是由进程来实现，线程就是轻量级进程（ lightweight process ），因此在Linux中，线程的调度是按照进程的调度方式来进行调度的，也就是说线程是调度单元。

Linux这样实现的线程的好处的之一是：线程调度直接使用进程调度就可以了，没必要再搞一个进程内的线程调度器。在Linux中，调度器是基于线程的调度策略（scheduling policy）和静态调度优先级（static scheduling priority）来决定那个线程来运行。

在Linux中，主要有三种调度策略。分别是：

- SCHED_OTHER 分时调度策略，（默认的）
- SCHED_FIFO 实时调度策略，先到先服务
- SCHED_RR 实时调度策略，时间片轮转

Windows线程调度

Windows 采用基于优先级的、抢占调度算法来调度线程。

用于处理调度的 Windows 内核部分称为调度程序，Windows 调度程序确保具有最高优先级的线程总是在运行的。由于调度程序选择运行的线程会一直运行，直到被更高优先级的线程所抢占，或终止，或时间片已到，或调用阻塞系统调用（如 I/O）。如果在低优先级线程运行时，更高优先级的实时线程变成就绪，那么低优先级线程就被抢占。这种抢占使得实时线程在需要使用 CPU 时优先得到使用。

### 1.线程的创建方式

- 继承Thread类
- 实现Runnable接口
- 通过ExecutorService和Callable实现有返回值的线程
- 基于线程池

继承Thread类

Thread类实现了Runnable接口并定义了操作线程的一些方法，我们可以通过继承Thread类的方式创建一个线程。具体实现为创建一个类并继承Thread接口，然后实例化线程对象并调用start方法启动线程。start方法是一个native方法，通过在操作系统上启动一个新线程，并最终执行run方法来启动一个线程。run方法内的代码是线程类的具体实现逻辑。

```html
//通过继承Thread类创建NewThread线程
public class NewThread extends Thread{
    public void run(){
    
    }
} 
//实例化一个线程对象
NewThread newThread=new NewThread();
//调用start方法启动线程
newThread.start();
```

以上代码定义了一个名为NewThread的线程类，该类继承了Thread，run方法内的代码为线程的具体执行逻辑，在使用该线程时只需新建一个该线程的对象并调用其start方法即可。

实现Runnable接口

基于Java编程语言的规范，如果子类已经继承（extends）了一个类，就无法再直接继承Thread类，此时可以通过实现Runnable接口创建线程。具体的实现过程为：通过实现Runnable接口创建ChildrenClassThread线程，实例化名称为childrenThread的线程实例，创建Thread类的实例并传入childrenThread线程实例，调用线程的start方法启动线程。具体的实现代码如下：

```html
//通过继承Thread类创建NewThread线程
public class ChildrenClassThread extends SuperClass implements Runnable{
    public void run(){
    
    }
} 
//实例化一个ChildrenClassThread对象
ChildrenClassThread childrenThread=new ChildrenClassThread();
//创建一个线程对象并将其传入已经实例化好的childrenThread实例
Thread thread=new Thread(childrenThread);
//调用start方法启动线程
thread.start();
```

在传入一个实现了Runnable的线程实例target给Thread后，Thread的run方法在执行时就会调用target.run方法并执行该线程具体的实现逻辑。在JDK源码中，run方法的实现代码如下：

```html
@Override
public void run(){
    if(target!=null){
        target.run();
    }
}
```

通过ExecutorService和Callable、Future实现有返回值的线程

有时，我们需要在主线程中开启多个线程并发执行一个任务，然后收集各个线程执行返回的结果并将最终结果汇总起来，这时就要用到Callable接口。

有返回值的任务必须实现Callable接口，类似的，无返回值的任务必须Runnable接口。

具体的实现方法为：创建一个类并实现Callable接口，在call方法中实现具体的运算逻辑并返回计算结果。具体的调用过程为：创建一个线程池、一个用于接收返回结果的Future List及Callable线程实例，使用线程池提交任务并将线程执行之后的结果保存在Future中，在线程执行结束后遍历Future List中的Future对象，在该对象上调用get方法就可以获取Callable线程任务返回的数据并汇总结果，实现代码如下：

```html
//实现Callable接口创建MyCallable线程
public class MyCallable implements Callable<String>{
    private String name;
    public MyCallable(String name){//通过构造函数为线程传递参数，以定义线程名称
        this.name=name;
    }
    @Override
    public String call() throws Exception{//call方法内为线程实现逻辑
        return name;
    }
}
    //创建一个固定大小为5的线程池
    ExecutorService pool=Executors.newFixedThreadPool(5);
    //创建有多个返回值的任务列表List
    List<Future> list=new ArrayList<Future>();
    for(int i=0;i<5;i++){
        //创建一个有返回值的线程实例
        Callable c=new MyCallable(i+"");
        //提交线程，获取Future对象并将其保存到Future List
        Future future=pool.submit(c);
        list.add(future);
    }
    //关闭线程池，等待线程执行结束
    pool.shutdown();
}
```

基于线程池

线程是非常宝贵的计算资源，在每次需要时创建并在运行结束后销毁是非常浪费资源的。我们可以使用缓存策略并使用线程池来创建线程，具体过程为创建一个线程池并用该线程池提交线程任务。

```html
ExecutorService threadPool=Executors.newFixedThreadPool(10);
for(int i=0;i<10;i++){
    threadPool.execute(new Runnable(){
        @Override
        public void run(){
        }
    });
}
```

### 2.线程的生命周期

线程的生命周期分为新建（New）、就绪（Runnable）、运行（Running）、阻塞（Blocked）和死亡（Dead）这5种状态。在系统运行过程中不断有新的线程被创建，旧的线程在执行完毕后被清理，线程在排队获取共享资源或者锁时将被阻塞，因此运行中的线程会在就绪、阻塞、运行状态之间来回切换。

其流程如下所述。

（1）调用new方法新建一个线程，这时线程处于新建状态。

（2）调用start方法启动一个线程，这时线程处于就绪状态。

（3）处于就绪状态的线程等待线程获取CPU资源，在等待其获取CPU资源后线程会执行run方法进入运行状态。

（4）正在运行的线程在调用了yield方法或失去处理器资源时，会再次进入就绪状态。

（5）正在执行的线程在执行了sleep方法、I/O阻塞、等待同步锁、等待通知、调用suspend方法等操作后，会挂起并进入阻塞状态，进入Blocked池。

（6）阻塞状态的线程由于出现sleep时间已到、I/O方法返回、获得同步锁、收到通知、调用resume方法等情况，会再次进入就绪状态，等待CPU时间片的轮询。该线程在获取CPU资源后，会再次进入运行状态。

（7）处于运行状态的线程，在调用run方法或call方法正常执行完成、调用stop方法停止线程或者程序执行错误导致异常退出时，会进入死亡状态。

新建状态：New

在Java中使用new关键字创建一个线程，新创建的线程将处于新建状态。在创建线程时主要是为线程分配内存并初始化其成员变量的值。

就绪状态：Runnable

新建的线程对象在调用start方法之后将转为就绪状态。此时JVM完成了方法调用栈和程序计数器的创建，等待该线程的调度和运行。

运行状态：Running

就绪状态的线程在竞争到CPU的使用权并开始执行run方法的线程执行体时，会转为运行状态，处于运行状态的线程的主要任务就是执行run方法中的逻辑代码。

阻塞状态：Blocked

运行中的线程会主动或被动地放弃CPU的使用权并暂停运行，此时该线程将转为阻塞状态，直到再次进入可运行状态，才有机会再次竞争到CPU使用权并转为运行状态。阻塞的状态分为以下三种。

（1）等待阻塞：在运行状态的线程调用o.wait方法时，JVM会把该线程放入等待队列（Waitting Queue）中，线程转为阻塞状态。

（2）同步阻塞：在运行状态的线程尝试获取正在被其他线程占用的对象同步锁时，JVM会把该线程放入锁池（Lock Pool）中，此时线程转为阻塞状态。

（3）其他阻塞：运行状态的线程在执行Thread.sleep(long ms)、Thread.join()或者发出I/O请求时，JVM会把该线程转为阻塞状态。直到sleep()状态超时、Thread.join()等待线程终止或超时，或者I/O处理完毕，线程才重新转为可运行状态。

线程死亡：Dead

线程在以下面三种方式结束后转为死亡状态。

- 线程正常结束：run方法或call方法执行完成。
- 线程异常退出：运行中的线程抛出一个Error或未捕获的Exception，线程异常退出。
- 手动结束：调用线程对象的stop方法手动结束运行中的线程（该方式会瞬间释放线程占用的同步对象锁，导致锁混乱和死锁，不推荐使用）。

### 3.线程的基本方法

线程相关的基本方法有wait、notify、notifyAll、sleep、join、yield等，这些方法控制线程的运行，并影响线程的状态变化。

线程等待：wait方法

调用wait方法的线程会进入WAITING状态，只有等到其他线程的通知或被中断后才会返回。需要注意的是，在调用wait方法后会释放对象的锁，因此wait方法一般被用于同步方法或同步代码块中。

线程睡眠：sleep方法

调用sleep方法会导致当前线程休眠。与wait方法不同的是，sleep方法不会释放当前占有的锁，会导致线程进入TIMED-WATING状态，而wait方法会导致当前线程进入WATING状态。

线程让步：yield方法

调用yield方法会使当前线程让出（释放）CPU执行时间片，与其他线程一起重新竞争CPU时间片。在一般情况下，优先级高的线程更有可能竞争到CPU时间片，但这不是绝对的，有的操作系统对线程的优先级并不敏感。

线程中断：interrupt方法

interrupt方法用于向线程发行一个终止通知信号，会影响该线程内部的一个中断标识位，这个线程本身并不会因为调用了interrupt方法而改变状态（阻塞、终止等）。状态的具体变化需要等待接收到中断标识的程序的最终处理结果来判定。对interrupt方法的理解需要注意以下4个核心点。

- 调用interrupt方法并不会中断一个正在运行的线程，也就是说处于Running状态的线程并不会因为被中断而终止，仅仅改变了内部维护的中断标识位而已。
- 若因为调用sleep方法而使线程处于TIMED-WATING状态，则这时调用interrupt方法会抛出InterruptedException，使线程提前结束TIMED-WATING状态。
- 许多声明抛出InterruptedException的方法如Thread.sleep(longmills)，在抛出异常前都会清除中断标识位，所以在抛出异常后调用isInterrupted方法将会返回false。
- 中断状态是线程固有的一个标识位，可以通过此标识位安全终止线程。比如，在想终止一个线程时，可以先调用该线程的interrupt方法，然后在线程的run方法中根据该线程isInterrupted方法的返回状态值安全终止线程。

线程加入：join方法

join方法用于等待其他线程终止，如果在当前线程中调用一个线程的join方法，则当前线程转为阻塞状态，等到另一个线程结束，当前线程再由阻塞状态转为就绪状态，等待获取CPU的使用权。

为什么要用join()方法？

在很多情况下，主线程生成并启动了子线程，需要等到子线程返回结果并收集和处理再退出，这时就要用到join方法。

线程唤醒：notify方法

Object类有个notify方法，用于唤醒在此对象监视器上等待的一个线程，如果所有线程都在此对象上等待，则会选择唤醒其中一个线程，选择是任意的。

我们通常调用其中一个对象的wait方法在对象的监视器上等待，直到当前线程放弃此对象上的锁定，才能继续执行被唤醒的线程，被唤醒的线程将以常规方式与在该对象上主动同步的其他线程竞争。类似的方法还有notifyAll，用于唤醒在监视器上等待的所有线程。

后台守护线程：setDaemon方法

setDaemon方法用于定义一个守护线程，也叫作“服务线程”，该线程是后台线程，有一个特性，即为用户线程提供公共服务，在没有用户线程可服务时会自动离开。

守护线程的优先级较低，用于为系统中的其他对象和线程提供服务。将一个用户线程设置为守护线程的方法是在线程对象创建之前用线程对象的setDaemon(true)来设置。

在后台守护线程中定义的线程也是后台守护线程。

后台守护线程是JVM级别的，比如垃圾回收线程就是一个经典的守护线程，在我们的程序中不再有任何线程运行时，程序就不会再产生垃圾，垃圾回收器也就无事可做，所以在回收JVM上仅剩的线程时，垃圾回收线程会自动离开。它始终在低级别的状态下运行，用于实时监控和管理系统中的可回收资源。

守护线程是运行在后台的一种特殊线程，独立于控制终端并且周期性地执行某种任务或等待处理某些已发生的事件。也就是说，守护线程不依赖于终端，但是依赖于JVM，与JVM“同生共死”。在JVM中的所有线程都是守护线程时，JVM就可以退出了，如果还有一个或一个以上的非守护线程，则JVM不会退出。

其他方法

- sleep()：强迫一个线程睡眠Ｎ毫秒。
- isAlive()：判断一个线程是否存活。
- join()：等待线程终止。
- activeCount()：程序中活跃的线程数。
- enumerate()：枚举程序中的线程。
- currentThread()：得到当前线程。
- isDaemon()：一个线程是否为守护线程。
- setDaemon()：设置一个线程为守护线程。(用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束)
- setName()：为线程设置一个名称。
- wait()：强迫一个线程等待。
- notify()：通知一个线程继续运行。
- setPriority()：设置一个线程的优先级。
- getPriority():：获得一个线程的优先级。

### 4.sleep方法与wait方法的区别

sleep方法与wait方法的区别如下。

- sleep方法属于Thread类，wait方法则属于Object类。
- sleep方法暂停执行指定的时间，让出CPU 给其他线程，但其监控状态依然保持，在指定的时间过后又会自动恢复运行状态。
- 在调用sleep方法的过程中，线程不会释放对象锁。
- 在调用wait方法时，线程会放弃对象锁，进入等待此对象的等待锁池，只有针对此对象调用notify方法后，该线程才能进入对象锁池准备获取对象锁，并进入运行状态。

### 5.start方法与run方法的区别

start方法与run方法的区别如下。

- start方法用于启动线程，真正实现了多线程运行。在调用了线程的start方法后，线程会在后台执行，无须等待run方法体的代码执行完毕，就可以继续执行下面的代码。
- 在通过调用Thread类的start方法启动一个线程时，此线程处于就绪状态，并没有运行。
- run方法也叫作线程体，包含了要执行的线程的逻辑代码，在调用run方法后，线程就进入运行状态，开始运行run方法中的代码。在run方法运行结束后，该线程终止，CPU再调度其他线程。

### 6.终止线程的4种方式

正常运行结束

指线程体执行完成，线程自动结束。

使用退出标志退出线程

在一般情况下，在run方法执行完毕时，线程会正常结束。然而，有些线程是后台线程，需要长时间运行，只有在系统满足某些特殊条件后，才能触发关闭这些线程。这时可以使用一个变量来控制循环，比如设置一个boolean类型的标志，并通过设置这个标志为true或false来控制while循环是否退出，具体的实现代码如下：

```html
public class ThreadSafe extends Thread{
    public volatile boolean exit=false;
    public void run(){
        while(!exit){
            //执行业务逻辑代码
        }
    }
}
```

以上代码在线程中定义了一个退出标志exit，exit的默认值为false。在定义exit时使用了一个Java关键字volatile，这个关键字用于使exit线程同步安全，也就是说在同一时刻只能有一个线程修改exit的值，在exit为true时，while循环退出。

使用Interrupt方法终止线程

使用interrupt方法终止线程有以下两种情况。

（1）线程处于阻塞状态。例如，在使用了sleep、调用锁的wait或者调用socket的receiver、accept等方法时，会使线程处于阻塞状态。在调用线程的interrupt方法时，会抛出InterruptException异常。我们通过代码捕获该异常，然后通过break跳出状态检测循环，可以有机会结束这个线程的执行。通常很多人认为只要调用interrupt方法就会结束线程，这实际上理解有误，一定要先捕获InterruptedException异常再通过break跳出循环，才能正常结束run方法。具体的实现代码如下：

```html
public class ThreadSafe extends Thread{
    public void run(){
        while(!isInterrupted()){//在非阻塞过程中通过判断中断标志来推出
            try{
                Thread.sleep(5*1000);//在阻塞过程中捕获中断异常来退出
            }catch(InterruptedException e){
                e.printStackTrace();
                break;//在捕获到异常后执行break跳出循环
            }
        }
    }
}
```

（2）线程未处于阻塞状态。此时，使用isInterrupted方法判断线程的中断标志来退出循环。在调用interrupt方法时，中断标志会被设置为true，并不能立刻退出线程，而是执行线程终止前的资源释放操作，等待资源释放完毕后退出该线程。

使用stop方法终止线程：不安全

在程序中可以直接调用Thread.stop方法强行终止线程，但这是很危险的，就像突然关闭计算机的电源，而不是正常关机一样，可能会产生不可预料的后果。

在程序使用Thread.stop方法终止线程时，该线程的子线程会抛出ThreadDeatherror错误，并且释放子线程持有的所有锁。加锁的代码块一般被用于保护数据的一致性，如果在调用Thread.stop方法后导致该线程所持有的所有锁突然释放而使锁资源不可控制，被保护的数据就可能出现不一致的情况，其他线程在使用这些被破坏的数据时，有可能使程序运行错误。因此，并不推荐采用这种方法终止线程。

### 7.线程上下文切换

CPU利用时间片轮询来为每个任务都服务一定的时间，然后把当前任务的状态保存下来，继续服务下一个任务。任务的状态保存及再加载就叫作线程的上下文切换。时间片轮转的方式使多个任务在同一颗CPU上执行变成了可能。

- 进程：指一个运行中的程序的实例。在一个进程内部可以有多个线程在同时运行，并与创建它的进程共享同一地址空间（一段内存区域）和其他资源。
- 上下文：指线程切换时CPU寄存器和程序计数器所保存的当前线程的信息。
- 寄存器：指CPU 内部容量较小但速度很快的内存区域（与之对应的是CPU 外部相对较慢的RAM主内存）。寄存器通过对常用值（通常是运算的中间值）的快速访问来加快计算机程序运行的速度。
- 程序计数器：是一个专用的寄存器，用于表明指令序列中CPU 正在执行的位置，存储的值为正在执行的指令的位置或者下一个将被执行的指令的位置，这依赖于特定的系统。

在上下文切换过程中，CPU会停止处理当前运行的程序，并保存当前程序运行的具体位置以便之后继续运行。从这个角度来看，上下文切换有点像我们同时阅读几本书，在来回切换书本的同时我们需要记住每本书当前读到的页码。在程序中，上下文切换过程中的“页码”信息是保存在进程控制块（PCB）中的。PCB还经常被称作“切换帧”（switchframe）。“页码”信息会一直保存到CPU的内存中，直到他们被再次使用。

PCB-“切换桢”

上下文切换指的是内核（操作系统的核心）在CPU上对进程或者线程进行切换。上下文切换过程中的信息被保存在进程控制块（PCB-ProcessControl Block）中。PCB又被称作切换桢（SwitchFrame）。上下文切换的信息会一直被保存在CPU的内存中，直到被再次使用。上下文的切换流程如下。

（1）挂起一个进程，将这个进程在CPU中的状态（上下文信息）存储于内存的PCB中。

（2）在PCB中检索下一个进程的上下文并将其在CPU的寄存器中恢复。

（3）跳转到程序计数器所指向的位置（即跳转到进程被中断时的代码行）并恢复该进程。

时间片轮转方式使多个任务在同一CPU上的执行有了可能。

### 8.引起线程上下文切换的原因

- 当前正在执行的任务完成，系统的CPU正常调度下一个任务。
- 当前正在执行的任务遇到I/O 等阻塞操作，调度器挂起此任务，继续调度下一个任务。
- 多个任务并发抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续调度下一个任务。
- 用户的代码挂起当前任务，比如线程执行sleep方法，让出CPU。
- 硬件中断。

### 9.多线程如何共享数据

在Java中进行多线程通信主要是通过共享内存实现的，共享内存主要有三个关注点：可见性、有序性、原子性。Java内存模型（JVM）解决了可见性和有序性的问题，而锁解决了原子性的问题。在理想情况下，我们希望做到同步和互斥来实现数据在多线程环境下的一致性和安全性。常用的实现多线程数据共享的方式有将数据抽象成一个类，并将对这个数据的操作封装在类的方法中；将Runnable对象作为一个类的内部类，将共享数据作为这个类的成员变量。

将数据抽象成一个类，并将对这个数据的操作封装在类的方法中

这种方式只需要在方法上加synchronized关键字即可做到数据的同步。

在应用时需要注意的是，如果两个线程需要保证数据操作的原子性和一致性，就必须在传参时使用同一个data对象入参。这样无论启动多少个线程执行对data数据的操作，都能保证数据的一致性。

将Runnable对象作为一个类的内部类，将共享数据作为这个类的成员变量

将Runnable对象作为类的内部类，将共享数据作为这个类的成员变量，每个线程对共享数据的操作方法都被封装在该类的外部类中，以便实现对数据的各个操作的同步和互斥，作为内部类的各个Runnable对象调用外部类的这些方法。

### 10.线程调度

抢占式调度

抢占式调度指每个线程都以抢占的方式获取CPU资源并快速执行，在执行完毕后立刻释放CPU资源，具体哪些线程能抢占到CPU资源由操作系统控制，在抢占式调度模式下，每个线程对CPU资源的申请地位是相等，从概率上讲每个线程都有机会获得同样的CPU执行时间片并发执行。抢占式调度适用于多线程并发执行的情况，在这种机制下一个线程的堵塞不会导致整个进程性能下降。

协同式调度

协同式调度指某一个线程在执行完后主动通知操作系统将CPU资源切换到另一个线程上执行。线程对CPU的持有时间由线程自身控制，线程切换更加透明，更适合多个线程交替执行某些任务的情况。

协同式调度有一个缺点：如果其中一个线程因为外部原因（可能是磁盘I/O阻塞、网络I/O阻塞、请求数据库等待）运行阻塞，那么可能导致整个系统阻塞甚至崩溃。

Java线程调度的实现：抢占式

Java采用抢占式调度的方式实现内部的线程调度，Java会为每个线程都按照优先级高低分配不同的CPU时间片，且优先级高的线程优先执行。优先级低的线程只是获取CPU时间片的优先级被降低，但不会永久分配不到CPU时间片。Java的线程调度在保障效率的前提下尽可能保障线程调度的公平性。

线程让出CPU的情况

线程让出CPU的情况如下。

- 当前运行的线程主动放弃CPU，例如运行中的线程调用yield()放弃CPU 的使用权。
- 当前运行的线程进入阻塞状态，例如调用文件读取I/O 操作、锁等待、Socket等待。
- 当前线程运行结束，即运行完run()里面的任务。

### 11.进程调度算法

进程调度算法包括优先调度算法、高优先权优先调度算法和基于时间片的轮转调度算法。其中，优先调度算法分为先来先服务调度算法和短作业优先调度算法；高优先权优先调度算法分为非抢占式优先权算法、抢占式优先权调度算法和高响应比优先调度算法。基于时间片的轮转调度算法分为时间片轮转算法和多级反馈队列调度算法。

1.优先调度算法

优先调度算法包含先来先服务调度算法和短作业（进程）优先调度算法。

先来先服务调度算法（FCFS）

先来先服务调度算法指每次调度时都从队列中选择一个或多个最早进入该队列的作业，为其分配资源、创建进程和放入就绪队列。调度算法在获取到可用的CPU资源时会从就绪队列中选择一个最早进入队列的进程，为其分配CPU资源并运行。该算法优先运行最早进入的任务，实现简单且相对公平。

短作业优先调度算法

短作业优先调度算法指每次调度时都从队列中选择一个或若干个预估运行时间最短的作业，为其分配资源、创建进程和放入就绪队列。调度算法在获取到可用的CPU资源时，会从就绪队列中选出一个预估运行时间最短的进程，为其分配CPU资源并运行。该算法优先运行短时间作业，以提高CPU整体的利用率和系统运行效率，某些大任务可能会出现长时间得不到调度的情况。

2.高优先权优先调度算法

高优先权优先调度算法在定义任务的时候为每个任务都设置不同的优先权，在进行任务调度时优先权最高的任务首先被调度，这样资源的分配将更加灵活，具体包含非抢占式优先调度算法、抢占式优先调度算法和高响应比优先调度算法。

非抢占式优先调度算法

非抢占式优先调度算法在每次调度时都从队列中选择一个或多个优先权最高的作业，为其分配资源、创建进程和放入就绪队列。调度算法在获取到可用的CPU资源时会从就绪队列中选出一个优先权最高的进程，为其分配CPU资源并运行。进程在运行过程中一直持有该CPU，直到进程执行完毕或发生异常而放弃该CPU。该算法优先运行优先权高的作业，且一旦将CPU分配给某个进程，就不会主动回收CPU资源，直到任务主动放弃。

抢占式优先调度算法

抢占式优先调度算法首先把CPU资源分配给优先权最高的任务并运行，但如果在运行过程中出现比当前运行任务优先权更高的任务，调度算法就会暂停运行该任务并回收CPU资源，为其分配新的优先权更高的任务。该算法真正保障了CPU在整个运行过程中完全按照任务的优先权分配资源，这样如果临时有紧急作业，则也可以保障其第一时间被执行。

高响应比优先调度算法

高响应比优先调度算法使用了动态优先权的概念，即任务的执行时间越短，其优先权越高，任务的等待时间越长，优先权越高，这样既保障了快速、并发地执行短作业，也保障了优先权低但长时间等待的任务也有被调度的可能性。

该优先权的变化规律如下。

- 在作业的等待时间相同时，运行时间越短，优先权越高，在这种情况下遵循的是短作业优先原则。
- 在作业的运行时间相同时，等待时间越长，优先权越高，在这种情况下遵循的是先来先服务原则。
- 作业的优先权随作业等待时间的增加而不断提高，加大了长作业获取CPU 资源的可能性。

高响应比优先调度算法在保障效率（短作业优先能在很大程度上提高CPU的使用率和系统性能）的基础上尽可能提高了调度的公平性（随着任务等待时间的增加，优先权提高，遵循了先来先到原则）。

3.时间片的轮转调度算法

时间片的轮转调度算法将CPU资源分成不同的时间片，不同的时间片为不同的任务服务，具体包括时间片轮转法和多级反馈队列调度算法。

时间片轮转法

时间片轮转法指按照先来先服务原则从就绪队列中取出一个任务，并为该任务分配一定的CPU时间片去运行，在进程使用完CPU时间片后由一个时间计时器发出时钟中断请求，调度器在收到时钟中断请求信号后停止该进程的运行并将该进程放入就绪队列的队尾，然后从就绪队列的队首取出一个任务并为其分配CPU时间片去执行。这样，就绪队列中的任务就将轮流获取一定的CPU时间片去运行。

多级反馈队列调度算法

多级反馈队列调度算法在时间片轮询算法的基础上设置多个就绪队列，并为每个就绪队列都设置不同的优先权。队列的优先权越高，队列中的任务被分配的时间片就越大。默认第一个队列优先权最高，其他次之。

多级反馈队列调度算法的调度流程为：在系统收到新的任务后，首先将其放入第一个就绪队列的队尾，按先来先服务调度算法排队等待调度。若该进程在规定的CPU时间片内运行完成或者运行过程中出现错误，则退出进程并从系统中移除该任务；如果该进程在规定的CPU时间片内未运行完成，则将该进程转入第2队列的队尾调度执行；如果该进程在第2队列中运行一个CPU时间片后仍未完成，则将其放入第3队列，以此类推，在一个长作业从第1队列依次降到第n队列后，在第n队列中便以时间片轮转的方式运行。

多级反馈队列调度算法遵循以下原则。

- 仅在第一个队列为空时，调度器才调度第2队列中的任务。
- 仅在第1～(n-1)队列均为空时，调度器才会调度第n队列中的进程。
- 如果处理器正在为第n队列中的某个进程服务，此时有新进程进入优先权较高的队列（第1～(n-1)中的任何一个队列），则此时新进程将抢占正在运行的进程的处理器，即调度器停止正在运行的进程并将其放回第 n 队列的末尾，把处理器分配给新来的高优先权进程。

队列的末尾，把处理器分配给新来的高优先权进程。多级反馈调度算法相对来说比较复杂，它充分考虑了先来先服务调度算法和时间片轮询算法的优势，使得对进程的调度更加合理。

### 12.线程和进程的区别

- 进程：是指一个内存中运行的应用程序，每个进程都有一个独立的内存空间，一个应用程序可以同时运行多个进程；进程也是程序的一次执行过程，是系统运行程序的基本单位；系统运行一个程序即是一个进程从创建、运行到消亡的过程。
- 线程：线程是进程中的一个执行单元，负责当前进程中程序的执行，一个进程中至少有一个线程。一个进程中是可以有多个线程的，这个应用程序也可以称之为多线程程序。

简而言之：一个程序运行后至少有一个进程，一个进程中可以包含多个线程

### 13.java中如何获取到线程dump文件

死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： 1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java 2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈。

### 14.怎么检测一个线程是否持有对象监视器

Thread类提供了一个holdsLock(Object obj)方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着"某条线程"指的是当前线程。

### 15.Linux环境下如何查找哪个线程使用CPU最长

这是一个比较偏实践的问题，这种问题我觉得挺有意义的。可以这么做： （1）获取项目的pid，jps或者ps -ef | grep java，这个前面有讲过 （2）top -H -p pid，顺序不能改变 这样就可以打印出当前的项目，每条线程占用CPU时间的百分比。注意这里打出的是LWP，也就是操作系统原生线程的线程号。 使用"top -H -p pid"+"jps pid"可以很容易地找到某条占用CPU高的线程的线程堆栈，从而定位占用CPU高的原因，一般是因为不当的代码操作导致了死循环。 最后提一点，"top -H -p pid"打出来的LWP是十进制的，"jps pid"打出来的本地线程号是十六进制的，转换一下，就能定位到占用CPU高的线程的当前线程堆栈了。

### 16.一个线程如果出现了运行时异常会怎么样

如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放。

### 17.如何在两个线程之间共享数据

通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的

### 18.说一下你了解的几种进程间的通信方式

1. 管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
2. 高级管道popen：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。
3. 有名管道named pipe ：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
4. 消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
5. 共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。
6. 信号量Semaphore：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
7. 套接字Socket：套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。
8. 信号sinal： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。

### 19.线程优先级

线程可以划分优先级，优先级分为1-10的10个等级，数字越大优先级越高，优先级较高的线程得到CPU资源较多，也就是CPU 优先执行优先级较高的线程对象中的任务(其实并不是这样)。但是线程的优先级仍然无法保障线程的执行次序。只不过，优先级高的线程获取CPU资源的概率较大，优先级低的并非没机会执行。

优先级具有继承性

线程是有继承关系的，比如当A线程中启动B线程，那么B和A的优先级将是一样的。

setPriority设置优先级

更改线程的优先级。当使用了setPriority 设置了优先级，但是线程的优先级仍然无法保障线程的执行次序。只不过，优先级高的线程获取CPU资源的概率较大，优先级低的并非没机会执行。

### 20.出现阻塞的情况

1. 线程调用 sleep 方法,主动放弃占用的处理器资源。
2. 线程调用了阻塞式 IO 方法,在该方法返回前,该线程被阻塞。
3. 线程试图获得一个同步监视器,但该同步监视器正被其他线程所持有。
4. 线程等待某个通知。
5. 程序调用了 suspend 方法将该线程挂起。此方法容易导致死锁,尽量避免使用该方法。

### 21.线程B 怎么知道线程A 修改了变量

1、volatile 修饰变量 2、synchronized 修饰修改变量的方法 3、wait/notify 4、while 轮询

### 22.什么是线程调度器(Thread Scheduler)和时间分片(TimeSlicing)？

线程调度器是一个操作系统服务，它负责为Runnable 状态的线程分配CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。时间分 片是指将可用的CPU 时间分配给可用的Runnable 线程的过程。分配CPU 时间可以基于线程优先级或者线程等待的时间。线程调度并不受到Java 虚拟机控制，所 以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。

### 23.线程阻塞

线程可以阻塞于四种状态： 当线程执行 Thread.sleep() 时，它一直阻塞到指定的毫秒时间之后，或者阻塞被另一个线程打断； 当线程碰到一条 wait() 语句时，它会一直阻塞到接到通知 notify()、被中断或经过了指定毫秒时间为止（若制定了超时值的话） 线程阻塞与不同 I/O 的方式有多种。常见的一种方式是 InputStream 的 read() 方法，该方法一直阻塞到从流中读取一个字节的数据为止，它可以无限阻塞，因此不能指定超时时间； 线程也可以阻塞等待获取某个对象锁的排他性访问权限（即等待获得 synchronized 语句必须的锁时阻塞）。

### 24.多线程共用一个数据变量需要注意什么？

当我们在线程对象（Runnable）中定义了全局变量，run方法会修改该变量时，如果有多个线程同时使用该线程对象，那么就会造成全局变量的值被同时修改，造成错误. ThreadLocal 是JDK引入的一种机制，它用于解决线程间共享变量，使用 ThreadLocal 声明的变量，即使在线程中属于全局变量，针对每个线程来讲，这个变量也是独立的。 volatile 变量每次被线程访问时，都强迫线程从主内存中重读该变量的最新值，而当该变量发生修改变化时，也会强迫线程将最新的值刷新回主内存中。这样一来，不同的线程都能及时的看到该变量的最新值。

### 25.如何减少线程上下文切换

程序在执行时，多线程是 CPU 通过给每个线程分配 CPU 时间片来实现的，时间片是CPU分配给每个线程执行的时间，因时间片非常短，所以CPU 通过不停地切换线程执行。 线程不是越多就越好的，因为线程上下文切换是有性能损耗的，在使用多线程的同时需要考虑如何减少上下文切换 一般来说有以下几条经验

- 无锁并发编程。多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的 ID 按照Hash取模分段，不同的线程处理不同段的数据
- CAS算法。Java 的 Atomic 包使用 CAS 算法来更新数据，而不需要加锁。
- 控制线程数量。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态协程。在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换
- 协程可以看成是用户态自管理的“线程”。不会参与CPU时间调度，没有均衡分配到时间。非抢占式的

还可以考虑我们的应用是IO密集型的还是CPU密集型的。 如果是IO密集型的话，线程可以多一些。 如果是CPU密集型的话，线程不宜太多。

### 26.线程间通信和进程间通信

线程间通信

- synchronized 同步
  - 这种方式，本质上就是 “共享内存” 式的通信。多个线程需要访问同一个共享变量，谁拿到了锁（获得了访问权限），谁就可以执行。
- while 轮询的方式
  - 在这种方式下，线程A不断地改变条件，线程 ThreadB 不停地通过 while 语句检测这个条件(list.size()==5) 是否成立 ，从而实现了线程间的通信。但是这种方式会浪费 CPU 资源。之所以说它浪费资源，是因为 JVM 调度器将 CPU 交给线程B执行时，它没做啥“有用”的工作，只是在不断地测试某个条件是否成立。就类似于现实生活中，某个人一直看着手机屏幕是否有电话来了，而不是： 在干别的事情，当有电话来时，响铃通知TA电话来了。
- wait/notify 机制
  - 当条件未满足时，线程A调用 wait() 放弃CPU，并进入阻塞状态。（不像 while 轮询那样占用 CPU）
  - 当条件满足时，线程B调用 notify() 通知线程A，所谓通知线程A，就是唤醒线程A，并让它进入可运行状态。
- 管道通信
  - java.io.PipedInputStream 和 java.io.PipedOutputStream 进行通信

进程间通信

- 管道（Pipe） ：管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信。 命名管道（named pipe） ：命名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关 系 进程间的通信。命名管道在文件系统中有对应的文件名。命名管道通过命令mkfifo或系统调用mkfifo来创建。
- 信号（Signal） ：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送 信号给进程本身；Linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）。
- 消息（Message）队列 ：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺
- 共享内存 ：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。
- 内存映射（mapped memory） ：内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间来实现它。
- 信号量（semaphore） ：主要作为进程间以及同一进程不同线程之间的同步手段。
- 套接口（Socket） ：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：linux和System V的变种都支持套接字。

### 27.为什么wait和notify方法要在同步块中调用？

1. 只有在调用线程拥有某个对象的独占锁时，才能够调用该对象的wait(),notify()和notifyAll()方法。
2. 如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。
3. 还有一个原因是为了避免wait和notify之间产生竞态条件。

wait()方法强制当前线程释放对象锁。这意味着在调用某对象的wait()方法之前，当前线程必须已经获得该对象的锁。因此，线程必须在某个对象的同步方法或同步代码块中才能调用该对象的wait()方法。 在调用对象的notify()和notifyAll()方法之前，调用线程必须已经得到该对象的锁。因此，必须在某个对象的同步方法或同步代码块中才能调用该对象的notify()或notifyAll()方法。 调用wait()方法的原因通常是，调用线程希望某个特殊的状态(或变量)被设置之后再继续执行。调用notify()或notifyAll()方法的原因通常是，调用线程希望告诉其他等待中的线程:"特殊状态已经被设置"。 这个状态作为线程间通信的通道，它必须是一个可变的共享状态(或变量)。

### 28.Java中interrupted 和 isInterruptedd方法的区别？

interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。 当中断线程调用静态方法Thread.interrupted()来检查中断状态时，中断状态会被清零。 而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变 。

### 29.有三个线程T1,T2,T3,如何保证顺序执行？

在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。 实际上先启动三个线程中哪一个都行， 因为在每个线程的run方法中用join方法限定了三个线程的执行顺序

### 30.线程死亡（DEAD）

线程会以下面三种方式结束，结束后就是死亡状态。 正常结束

1. run()或 call()方法执行完成，线程正常结束。异常结束
2. 线程抛出一个未捕获的 Exception 或 Error。调用 stop
3. 直接调用该线程的 stop()方法来结束该线程—该方法通常容易导致死锁，不推荐使用。

### 31.多线程同步和互斥有几种实现方法，都是什么？

线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。 线程间的同步方法大体可分为两类：用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式就是不需要切换到内核态，只在用户态完成操作。用户模式下的方法有：原子操作（例如一个单一的全局变量），临界区。内核模式下的方法有：事件，信号量，互斥量。

### 32.Java 中你怎样唤醒一个阻塞的线程？

在 Java 发展史上曾经使用 suspend()、resume()方法对于线程进行阻塞唤醒，但随之出现很多问题，比较典型的还是死锁问题。解决方案可以使用以对象为目标的阻塞，即利用 Object 类的 wait()和 notify()方法实现线程阻塞。 首先，wait、notify 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取改对象的锁，直到获取成功才能往下执行；其次，wait、notify方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。

### 33.**什么是线程饥饿？**

饥饿，一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。

Java 中导致饥饿的原因：

- 高优先级线程吞噬所有的低优先级线程的 CPU 时间。
- 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。
- 线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。

### 34.使用 wait + notify 实现通知机制？

- 首先，wait、notify 方法是针对对象的，调用任意对象的 wait 方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify 方法则将随机解除该对象阻塞的线程，但它需要重新获取改对象的锁，直到获取成功才能往下执行。
- 其次，wait、notify 方法必须在 `synchronized` 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。

### 35.**sleep(0) 有什么用途？**

`Thread#sleep(0)` 方法，并非是真的要线程挂起 0 毫秒，意义在于这次调用 `Thread#sleep(0)` 方法，把当前线程确实的被冻结了一下，让其他线程有机会优先执行。`Thread#sleep(0)` 方法，是你的线程暂时放弃 CPU ，也就是释放一些未用的时间片给其他线程或进程使用，就相当于一个**让位动作**。

### 36.多线程同步和互斥有几种实现方法，都是什么？

1）**线程同步**

线程同步，是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。

线程间的同步方法，大体可分为两类：用户模式和内核模式。顾名思义：

- 内核模式，就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态。内核模式下的方法有：
  - 事件
  - 信号量
  - 互斥量
- 用户模式，就是不需要切换到内核态，只在用户态完成操作。用户模式下的方法有：
  - 原子操作（例如一个单一的全局变量）
  - 临界区

2）**线程互斥**

线程互斥，是指对于共享的进程系统资源，在各单个线程访问时的排它性。

- 当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。
- 线程互斥可以看成是一种特殊的线程同步。

### 37.你有哪些多线程开发良好的实践？

1、给线程命名。

这样可以方便找 bug 或追踪。OrderProcessor、QuoteProcessor、TradeProcessor 这种名字比 Thread-1、Thread-2、Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。

2、最小化同步范围。

锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。

3、优先使用 `volatile` ，而不是 `synchronized` 。

4、尽可能使用更高层次的并发工具而非 wait 和 notify 方法来实现线程通信。

5、优先使用并发容器，而非同步容器。

并发容器比同步容器的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到 Map ，我们应该首先想到用 ConcurrentHashMap 类。

6、考虑使用线程池。

### 38.实现Runnable接口和Callable接口的区别

如果想让线程池执行任务的话需要实现的Runnable接口或Callable接口。 Runnable接口或Callable接 口实现类都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。两者的区别在于 Runnable 接口不会返回结果但是 Callable 接口可以返回结果。 备注： 工具类Executors 可以实现Runnable 对象和Callable 对象之间的相互转换。 （ Executors.callable（Runnable task） 或Executors.callable（Runnable task，Object resule） ）。

### 39.实现Callable接口的重要类

PrivilegedCallable、PrivilegedCallableUsingCurrentClassLoader、RunnableAdapter和Task类下的TaskCallable。

PrivilegedCallable

PrivilegedCallable类是Callable接口的一个特殊实现类，它表明Callable对象有某种特权来访问系统的某种资源。

PrivilegedCallableUsingCurrentClassLoader

此类表示为在已经建立的特定访问控制和当前的类加载器下运行的Callable类

RunnableAdapter

RunnableAdapter类比较简单，给定运行的任务和结果，运行给定的任务并返回给定的结果。

TaskCallable

TaskCallable类是javafx.concurrent.Task类的静态内部类，TaskCallable类主要是实现了Callable接口并且被定义为FutureTask的类，并且在这个类中允许我们拦截call()方法来更新task任务的状态。

### 40.深度解析Future接口

Future是JDK1.5新增的异步编程接口，在Future接口中，总共定义了5个抽象方法。

cancel(boolean)

取消任务的执行，接收一个boolean类型的参数，成功取消任务，则返回true，否则返回false。当任务已经完成，已经结束或者因其他原因不能取消时，方法会返回false，表示任务取消失败。当任务未启动调用了此方法，并且结果返回true（取消成功），则当前任务不再运行。如果任务已经启动，会根据当前传递的boolean类型的参数来决定是否中断当前运行的线程来取消当前运行的任务。

isCancelled()

判断任务在完成之前是否被取消，如果在任务完成之前被取消，则返回true；否则，返回false。

这里需要注意一个细节：只有任务未启动，或者在完成之前被取消，才会返回true，表示任务已经被成功取消。其他情况都会返回false。

isDone()

判断任务是否已经完成，如果任务正常结束、抛出异常退出、被取消，都会返回true，表示任务已经完成。

get()

当任务完成时，直接返回任务的结果数据；当任务未完成时，等待任务完成并返回任务的结果数据。

get(long, TimeUnit)

当任务完成时，直接返回任务的结果数据；当任务未完成时，等待任务完成，并设置了超时等待时间。在超时时间内任务完成，则返回结果；否则，抛出TimeoutException异常。

### 41.SimpleDateFormat类的线程安全问题

SimpleDateFormat类为何不是线程安全的

通过对SimpleDateFormat类中的parse(String, ParsePosition)方法的分析可以得知，parse(String,ParsePosition)方法中存在几处为ParsePosition类中的索引赋值的操作。 一旦将SimpleDateFormat类定义成全局的静态变量，那么SimpleDateFormat类在多个线程间是共享的，这就导致ParsePosition类在多个线程间共享。在高并发场景下，一个线程对ParsePosition类中的索引进行修改，势必会影响到其他线程对ParsePosition类中索引的读操作。这就造成了线程的安全问题。

解决SimpleDateFormat类的线程安全问题

局部变量法

最简单的一种方式就是将SimpleDateFormat类对象定义成局部变量

这种方式在高并发下会创建大量的SimpleDateFormat类对象，影响程序的性能，所以，这种方式在实际生产环境不太被推荐。

synchronized锁方式

将SimpleDateFormat类对象定义成全局静态变量，此时所有线程共享SimpleDateFormat类对象，此时在调用格式化时间的方法时，对SimpleDateFormat对象进行同步即可。

虽然这种方式能够解决SimpleDateFormat类的线程安全问题，但是由于在程序的执行过程中，为SimpleDateFormat类对象加上了synchronized锁，导致同一时刻只能有一个线程执行parse(String)方法。此时，会影响程序的执行性能，在要求高并发的生产环境下，此种方式也是不太推荐使用的。

Lock锁方式

Lock锁方式与synchronized锁方式实现原理相同，都是在高并发下通过JVM的锁机制来保证程序的线程安全。

此种方式同样会影响高并发场景下的性能，不太建议在高并发的生产环境使用。

ThreadLocal方式

使用ThreadLocal存储每个线程拥有的SimpleDateFormat对象的副本，能够有效的避免多线程造成的线程安全问题

将每个线程使用的SimpleDateFormat副本保存在ThreadLocal中，各个线程在使用时互不干扰，从而解决了线程安全问题。

此种方式运行效率比较高，推荐在高并发业务场景的生产环境使用。

DateTimeFormatter方式

DateTimeFormatter是Java8提供的新的日期时间API中的类，DateTimeFormatter类是线程安全的，可以在高并发场景下直接使用DateTimeFormatter类来处理日期的格式化操作。

joda-time方式

joda-time是第三方处理日期时间格式化的类库，是线程安全的。如果使用joda-time来处理日期和时间的格式化，则需要引入第三方类库。

使用joda-time库来处理日期的格式化操作运行效率比较高，推荐在高并发业务场景的生产环境使用。

### 42.什么是线程调度器(Thread Scheduler)和时间分片(TimeSlicing )？

线程调度器是一个操作系统服务，它负责为Runnable 状态的线程分配CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。

同上一个问题，线程调度并不受到Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。

时间分片是指将可用的CPU 时间分配给可用的Runnable 线程的过程。分配CPU时间可以基于线程优先级或者线程等待的时间。

### 43.线程B 怎么知道线程A 修改了变量

1、volatile 修饰变量

2、synchronized 修饰修改变量的方法

3、wait/notify

4、while 轮询

### 44.线程的调度策略

线程调度器选择优先级最高的线程运行， 但是，如果发生以下情况， 就会终止线程的运行：

1、线程体中调用了yield 方法让出了对cpu 的占用权利

2、线程体中调用了sleep 方法使线程进入睡眠状态

3、线程由于IO 操作受到阻塞

4、另外一个更高优先级线程出现

5、在支持时间片的系统中，该线程的时间片用完

### 45.Java 线程数过多会造成什么异常？

1、线程的生命周期开销非常高

2、消耗过多的CPU 资源

如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争CPU

资源时还将产生其他性能的开销。

3、降低稳定性

JVM 在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括JVM 的启动参数、Thread 构造函数中请求栈的

大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。

### 46.什么是线程局部变量？

线程局部变量是局限于线程内部的变量， 属于线程自身所有，不在多个线程间共享。Java 提供ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方

式。但是在管理环境下（如web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线

程局部变量一旦在工作完成后没有释放， Java 应用就存在内存泄露的风险。

### 2.上下文切换

上下文切换（有时也称做进程切换或任务切换）是指 CPU 从一个进程（或线程）切换到另一个进程（或线程）。上下文是指**某一时间点 CPU 寄存器和程序计数器的内容。**

> 寄存器是cpu内部的少量的速度很快的闪存，通常存储和访问计算过程的中间值提高计算机程序的运行速度。
>
> 程序计数器是一个专用的寄存器，用于表明指令序列中 CPU 正在执行的位置，存的值为正在执行的指令的位置或者下一个将要被执行的指令的位置，具体实现依赖于特定的系统。
>
> 举例说明 线程A - B
>
> 1.先挂起线程A，将其在cpu中的状态保存在内存中。
>
> 2.在内存中检索下一个线程B的上下文并将其在 CPU 的寄存器中恢复,执行B线程。
>
> 3.当B执行完，根据程序计数器中指向的位置恢复线程A。

CPU通过为每个线程分配CPU时间片来实现多线程机制。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。

但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。

上下文切换通常是计算密集型的，意味着此操作会**消耗大量的 CPU 时间，故线程也不是越多越好**。如何减少系统中上下文切换次数，是提升多线程性能的一个重点课题。

### 9.线程组(ThreadGroup)

Java中用ThreadGroup来表示线程组，我们可以使用线程组对线程进行批量控制。

ThreadGroup和Thread的关系就如同他们的字面意思一样简单粗暴，每个Thread必然存在于一个ThreadGroup中，Thread不能独立于ThreadGroup存在。执行main()方法线程的名字是main，如果在new Thread时没有显式指定，那么默认将父线程（当前执行new Thread的线程）线程组设置为自己的线程组。

```html
Thread testThread = new Thread(() -> {
    System.out.println("testThread当前线程组名字：" +Thread.currentThread().getThreadGroup().getName());
    System.out.println("testThread线程名字：" +Thread.currentThread().getName());
});
```

ThreadGroup管理着它下面的Thread，ThreadGroup是一个标准的**向下引用**的树状结构，这样设计的原因是**防止"上级"线程被"下级"线程引用而无法有效地被GC回收**。

### 11.线程组的常用方法及数据结构

线程组的常用方法

**获取当前的线程组名字**

```html
Thread.currentThread().getThreadGroup().getName()
```

**复制线程组**

```html
// 复制一个线程数组到一个线程组
Thread[] threads = new Thread[threadGroup.activeCount()];
TheadGroup threadGroup = new ThreadGroup();
threadGroup.enumerate(threads);
```

线程组的数据结构

线程组还可以包含其他的线程组，不仅仅是线程。

总结来说，线程组是一个树状的结构，每个线程组下面可以有多个线程或者线程组。线程组可以起到统一控制线程的优先级和检查线程的权限的作用。

### 12.操作系统中的线程状态转换

在现在的操作系统中，线程是被视为轻量级进程的，所以**操作系统线程的状态其实和操作系统进程的状态是一致的**。

操作系统线程主要有以下三个状态：

- 就绪状态(ready)：线程正在等待使用CPU，经调度程序调用之后可进入running状态。
- 执行状态(running)：线程正在使用CPU。
- 等待状态(waiting): 线程经过等待事件的调用或者正在等待其他资源（如I/O）。

Java线程的6个状态

NEW

处于NEW状态的线程此时尚未启动。这里的尚未启动指的是还没调用Thread实例的start()方法。

```html
private void testStateNew() {
    Thread thread = new Thread(() -> {});
    System.out.println(thread.getState()); // 输出 NEW 
}
```

从上面可以看出，只是创建了线程而并没有调用start()方法，此时线程处于NEW状态。

**关于start()的两个引申问题**

1. 反复调用同一个线程的start()方法是否可行？
2. 假如一个线程执行完毕（此时处于TERMINATED状态），再次调用这个线程的start()方法是否可行？

两个问题的答案都是不可行，在调用一次start()之后，threadStatus的值会改变（threadStatus !=0），此时再次调用start()方法会抛出IllegalThreadStateException异常。

比如，threadStatus为2代表当前线程状态为TERMINATED。

RUNNABLE

表示当前线程正在运行中。处于RUNNABLE状态的线程在Java虚拟机中运行，也有可能在等待其他系统资源（比如I/O）。

**Java中线程的RUNNABLE状态**

Java线程的**RUNNABLE**状态其实是包括了传统操作系统线程的**ready**和**running**两个状态的。

BLOCKED

阻塞状态。处于BLOCKED状态的线程正等待锁的释放以进入同步区。

WAITING

等待状态。处于等待状态的线程变成RUNNABLE状态需要其他线程唤醒。

调用如下3个方法会使线程进入等待状态：

- Object.wait()：使当前线程处于等待状态直到另一个线程唤醒它；
- Thread.join()：等待线程执行完毕，底层调用的是Object实例的wait方法；
- LockSupport.park()：除非获得调用许可，否则禁用当前线程进行线程调度。

TIMED_WAITING

超时等待状态。线程等待一个具体的时间，时间到后会被自动唤醒。

调用如下方法会使线程进入超时等待状态：

- Thread.sleep(long millis)：使当前线程睡眠指定时间；
- Object.wait(long timeout)：线程休眠指定时间，等待期间可以通过notify()/notifyAll()唤醒；
- Thread.join(long millis)：等待当前线程最多执行millis毫秒，如果millis为0，则会一直执行；
- LockSupport.parkNanos(long nanos)： 除非获得调用许可，否则禁用当前线程进行线程调度指定时间；
- LockSupport.parkUntil(long deadline)：同上，也是禁止线程进行调度指定时间；

TERMINATED

终止状态。此时线程已执行完毕。

### 13.线程状态的转换

BLOCKED与RUNNABLE状态的转换

处于BLOCKED状态的线程是因为在等待锁的释放。假如这里有两个线程a和b，a线程提前获得了锁并且暂未释放锁，此时b就处于BLOCKED状态。

WAITING状态与RUNNABLE状态的转换

根据转换图我们知道有3个方法可以使线程从RUNNABLE状态转为WAITING状态。我们主要介绍下**Object.wait()**和**Thread.join()**。 **Object.wait()**

Object.wait()

调用wait()方法前线程必须持有对象的锁。

线程调用wait()方法时，会释放当前的锁，直到有其他线程调用notify()/notifyAll()方法唤醒等待锁的线程。

需要注意的是，其他线程调用notify()方法只会唤醒单个等待锁的线程，如有有多个线程都在等待这个锁的话不一定会唤醒到之前调用wait()方法的线程。

同样，调用notifyAll()方法唤醒所有等待锁的线程之后，也不一定会马上把时间片分给刚才放弃锁的那个线程，具体要看系统的调度。

**Thread.join()**

> 调用join()方法不会释放锁，会一直等待当前线程执行完毕（转换为TERMINATED状态）。

TIMED_WAITING与RUNNABLE状态转换

TIMED_WAITING与WAITING状态类似，只是TIMED_WAITING状态等待的时间是指定的。

**Thread.sleep(long)**

> 使当前线程睡眠指定时间。需要注意这里的“睡眠”只是暂时使线程停止执行，并不会释放锁。时间到后，线程会重新进入RUNNABLE状态。

**Object.wait(long)**

> wait(long)方法使线程进入TIMED_WAITING状态。这里的wait(long)方法与无参方法wait()相同的地方是，都可以通过其他线程调用notify()或notifyAll()方法来唤醒。
>
> 不同的地方是，有参方法wait(long)就算其他线程不来唤醒它，经过指定时间long之后它会自动唤醒，拥有去争夺锁的资格。

**Thread.join(long)**

> join(long)使当前线程执行指定时间，并且使线程进入TIMED_WAITING状态。

### 15.Java线程间的通信

锁与同步

线程同步是线程之间按照**一定的顺序**执行。

为了达到线程同步，我们可以使用锁来实现它。

等待/通知机制

上面一种基于“锁”的方式，线程需要不断地去尝试获得锁，如果失败了，再继续尝试。这可能会耗费服务器资源。

而等待/通知机制是另一种方式。

Java多线程的等待/通知机制是基于`Object`类的`wait()`方法和`notify()`, `notifyAll()`方法来实现的。

> notify()方法会随机叫醒一个正在等待的线程，而notifyAll()会叫醒所有正在等待的线程。

一个锁同一时刻只能被一个线程持有。而假如线程A现在持有了一个锁`lock`并开始执行，它可以使用`lock.wait()`让自己进入等待状态。这个时候，`lock`这个锁是被释放了的。

这时，线程B获得了`lock`这个锁并开始执行，它可以在某一时刻，使用`lock.notify()`，通知之前持有`lock`锁并进入等待状态的线程A，说“线程A你不用等了，可以往下执行了”。

> 需要注意的是，这个时候线程B并没有释放锁`lock`，除非线程B这个时候使用`lock.wait()`释放锁，或者线程B执行结束自行释放锁，线程A才能得到`lock`锁。

需要注意的是等待/通知机制使用的是使用同一个对象锁，如果你两个线程使用的是不同的对象锁，那它们之间是不能用等待/通知机制通信的。

信号量

volitile关键字能够保证内存的可见性，如果用volitile关键字声明了一个变量，在一个线程里面改变了这个变量的值，那其它线程是立马可见更改后的值的。

信号量的应用场景：

假如在一个停车场中，车位是我们的公共资源，线程就如同车辆，而看门的管理员就是起的“信号量”的作用。

因为在这种场景下，多个线程（超过2个）需要相互合作，我们用简单的“锁”和“等待通知机制”就不那么方便了。这个时候就可以用到信号量。

管道

管道是基于“管道流”的通信方式。JDK提供了`PipedWriter`、 `PipedReader`、 `PipedOutputStream`、 `PipedInputStream`。其中，前面两个是基于字符的，后面两个是基于字节流的。

管道通信的应用场景：

这个很好理解。使用管道多半与I/O流相关。当我们一个线程需要先另一个线程发送一个信息（比如字符串）或者文件等等时，就需要使用管道通信了。

### 17.Java内存模型的抽象结构

运行时内存的划分

对于每一个线程来说，栈都是私有的，而堆是共有的。

也就是说在栈中的变量（局部变量、方法定义参数、异常处理器参数）不会在线程之间共享，也就不会有内存可见性（下文会说到）的问题，也不受内存模型的影响。而在堆中的变量是共享的，本文称为共享变量。

所以，内存可见性是针对的**共享变量**。

既然堆是共享的，为什么在堆中会有内存不可见问题？

这是因为现代计算机为了高效，往往会在高速缓存区中缓存共享变量，因为cpu访问缓存区比访问内存要快得多。

> 线程之间的共享变量存在主内存中，每个线程都有一个私有的本地内存，存储了该线程以读、写共享变量的副本。本地内存是Java内存模型的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器等。

Java线程之间的通信由Java内存模型（简称JMM）控制，从抽象的角度来说，JMM定义了线程和主内存之间的抽象关系。

\1. 所有的共享变量都存在主内存中。 2. 每个线程都保存了一份该线程使用到的共享变量的副本。 3. 如果线程A与线程B之间要通信的话，必须经历下面2个步骤： 1. 线程A将本地内存A中更新过的共享变量刷新到主内存中去。 2. 线程B到主内存中去读取线程A之前已经更新过的共享变量。

**所以，线程A无法直接访问线程B的工作内存，线程间通信必须经过主内存。**

注意，根据JMM的规定，**线程对共享变量的所有操作都必须在自己的本地内存中进行，不能直接从主内存中读取**。

所以线程B并不是直接去主内存中读取共享变量的值，而是先在本地内存B中找到这个共享变量，发现这个共享变量已经被更新了，然后本地内存B去主内存中读取这个共享变量的新值，并拷贝到本地内存B中，最后线程B再读取本地内存B中的新值。

那么怎么知道这个共享变量的被其他线程更新了呢？这就是JMM的功劳了，也是JMM存在的必要性之一。**JMM通过控制主内存与每个线程的本地内存之间的交互，来提供内存可见性保证**。

Java中的volatile关键字可以保证多线程操作共享变量的可见性以及禁止指令重排序，synchronized关键字不仅保证可见性，同时也保证了原子性（互斥性）。在更底层，JMM通过内存屏障来实现内存的可见性以及禁止重排序。为了程序员的方便理解，提出了happens-before，它更加的简单易懂，从而避免了程序员为了理解内存可见性而去学习复杂的重排序规则以及这些规则的具体实现方法。

JMM与Java内存区域划分的区别与联系

- 区别

  两者是不同的概念层次。JMM是抽象的，他是用来描述一组规则，通过这个规则来控制各个变量的访问方式，围绕原子性、有序性、可见性等展开的。而Java运行时内存的划分是具体的，是JVM运行Java程序时，必要的内存划分。

- 联系

  都存在私有数据区域和共享数据区域。一般来说，JMM中的主内存属于共享数据区域，他是包含了堆和方法区；同样，JMM中的本地内存属于私有数据区域，包含了程序计数器、本地方法栈、虚拟机栈。

**实际上，他们表达的是同一种含义，这里不做区分。**

### 18.在java中守护线程和本地线程区别？

java中的线程分为两种：守护线程（Daemon）和⽤⼾线程（User）。 任何线程都可以设置为守护线程和⽤⼾线程，通过⽅法Thread.setDaemon(bool on)；true则把该线程设置为守护线程，反之则 为⽤⼾线程。Thread.setDaemon()必须在Thread.start()之前调⽤，否则运⾏时会抛出异常。 两者的区别： 唯⼀的区别是判断虚拟机(JVM)何时离开，Daemon是为其他线程提供服务，如果全部的User Thread已经撤离，Daemon 没有 可服务的线程，JVM撤离。也可以理解为守护线程是JVM⾃动创建的线程（但不⼀定），⽤⼾线程是程序创建的线程；⽐如JVM 的垃圾回收线程是⼀个守护线程，当所有线程已经撤离，不再产⽣垃圾，守护线程⾃然就没事可⼲了，当垃圾回收线程是Java虚 拟机上仅剩的线程时，Java虚拟机会⾃动离开。 扩展：Thread Dump打印出来的线程信息，含有daemon字样的线程即为守护进程，可能会有：服务守护进程、编译守护进程、 windows下的监听Ctrl+break的守护进程、Finalizer守护进程、引⽤处理守护进程、GC守护进程。

### 4、死锁与活锁的区别，死锁与饥饿的区别？

死锁：是指两个或两个以上的进程（或线程）在执⾏过程中，因争夺资源⽽造成的⼀种互相等待的现象，若⽆外⼒作⽤，它们都 将⽆法推进下去。 产⽣死锁的必要条件： 互斥条件：所谓互斥就是进程在某⼀时间内独占资源。 请求与保持条件：⼀个进程因请求资源⽽阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得资源，在末使⽤完之前，不能强⾏剥夺。 循环等待条件:若⼲进程之间形成⼀种头尾相接的循环等待资源关系。 活锁：任务或者执⾏者没有被阻塞，由于某些条件没有满⾜，导致⼀直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， ⽽处于死锁的实体表现为等待；活锁有可能⾃⾏ 解开，死锁则不能。 饥饿：⼀个或者多个线程因为种种原因⽆法获得所需要的资源，导致⼀直⽆法执⾏的状态。 Java中导致饥饿的原因： ⾼优先级线程吞噬所有的低优先级线程的CPU时间。 线程被永久堵塞在⼀个等待进⼊同步块的状态，因为其他线程总是能在它之前持续地对该同步块进⾏访问。 线程在等待⼀个本⾝也处于永久等待完成的对象(⽐如调⽤这个对象的wait⽅法)，因为其他线程总是被持续地获得唤醒。

### 7、为什么使⽤Executor框架？

I. 每次执⾏任务创建线程 new Thread()⽐较消耗性能，创建⼀个线程是⽐较耗时、耗资源的。 II. 调⽤ new Thread()创建的线程缺乏管理，被称为野线程，⽽且可以⽆限制的创建，线程之间的相互竞争会导致过多占⽤系 统资源⽽导致系统瘫痪，还有线程之间的频繁交替也会消耗很多系统资源。 III. 接使⽤new Thread() 启动的线程不利于扩展，⽐如定时执⾏、定期执⾏、定时定期执⾏、线程中断等都不便实现。

### 8、在Java中Executor和Executors的区别？

Executors ⼯具类的不同⽅法按照我们的需求创建了不同的线程池，来满⾜业务的需求。 Executor 接⼝对象能执⾏我们的线程任务。 ExecutorService接⼝继承了Executor接⼝并进⾏了扩展，提供了更多的⽅法我们能获得任务执⾏的状态并且可以获取任务的返 回值。 使⽤ThreadPoolExecutor 可以创建⾃定义线程池。 Future 表⽰异步计算的结果，他提供了检查计算是否完成的⽅法，以等待计算的完成，并可以使⽤get()⽅法获取计算的结果。

### 13、什么是Callable和Future?

Callable接⼝类似于Runnable，从名字就可以看出来了，但是Runnable不会返回结果，并且⽆法抛出返回结果的异常，⽽ Callable功能更强⼤⼀些，被线程执⾏后，可以返回值，这个返回值可以被Future拿到，也就是说，Future可以拿到异步执⾏任 务的返回值。 可以认为是带有回调的Runnable。 Future接⼝表⽰异步任务，是还没有完成的任务给出的未来结果。所以说Callable⽤于产⽣结果，Future⽤于获取结果。

### 16、多线程同步和互斥有⼏种实现⽅法，都是什么？

线程同步是指线程之间所具有的⼀种制约关系，⼀个线程的执⾏依赖另⼀个线程的消息，当它没有得到另⼀个线程的消息时应等 待，直到消息到达时才被唤醒。 线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若⼲个线程都要使⽤某⼀共享资源时，任何时刻最 多只允许⼀个线程去使⽤，其它要使⽤该资源的线程必须等待，直到占⽤资源者释放该资源。线程互斥可以看成是⼀种特殊的线 程同步。 线程间的同步⽅法⼤体可分为两类：⽤⼾模式和内核模式。顾名思义，内核模式就是指利⽤系统内核对象的单⼀性来进⾏同步， 使⽤时需要切换内核态与⽤⼾态，⽽⽤⼾模式就是不需要切换到内核态，只在⽤⼾态完成操作。 ⽤⼾模式下的⽅法有：原⼦操作（例如⼀个单⼀的全局变量），临界区。内核模式下的⽅法有：事件，信号量，互斥量。

### 19、为什么我们调⽤start()⽅法时会执⾏run()⽅法，为什么我们不能直接调⽤run()⽅法？

当你调⽤start()⽅法时你将创建新的线程，并且执⾏在run()⽅法⾥的代码。 但是如果你直接调⽤run()⽅法，它不会创建新的线程也不会执⾏调⽤线程的代码，只会把run⽅法当作普通⽅法去执⾏。

### 22、什么是不可变对象，它对写并发应.有什么帮助？

不可变对象(Immutable Objects)即对象.旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象 (Mutable Objects)。 不可变对象的类即为不可变类(Immutable Class)。Java平台类库中包含许多不可变类，如String、基本类型的包装类、 BigInteger和BigDecimal等。 不可变对象天.是线程安全的。它们的常量（域）是在构造函数中创建的。既然它们的状态.法修改，这些常量永远不会变。 不可变对象永远是线程安全的。 只有满.如下状态，.个对象才是不可变的； 它的状态不能在创建后再被修改； 所有域都是final类型；并且， 它被正确创建（创建期间没有发.this引.的逸出）。

### 41、⼀个线程运⾏时发⽣异常会怎样？

如果异常没有被捕获该线程将会停⽌执⾏。Thread.UncaughtExceptionHandler是⽤于处理未捕获异常造成线程突然中断情况 的⼀个内嵌接⼝。当⼀个未捕获异常将造成线程中断的时候JVM会使⽤Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException()⽅法进⾏处理。

### 49、Java中的同步集合与并发集合有什么区别？

同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更⾼。在Java1.5之前程序员们 只有同步集合来⽤且在多线程并发的时候会导致争⽤，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap， 不仅提供线程安全还⽤锁分离和内部分区等现代技术提⾼了可扩展性。

## 线程池

### 1.线程池的工作原理

Java线程池主要用于管理线程组及其运行状态，以便Java虚拟机更好地利用CPU资源。Java线程池的工作原理为：JVM先根据用户的参数创建一定数量的可运行的线程任务，并将其放入队列中，在线程创建后启动这些任务，如果线程数量超过了最大线程数量（用户设置的线程池大小），则超出数量的线程排队等候，在有任务执行完毕后，线程池调度器会发现有可用的线程，进而再次从队列中取出任务并执行。

线程池的主要作用是线程复用、线程资源管理、控制操作系统的最大并发数，以保证系统高效（通过线程资源复用实现）且安全（通过控制最大线程并发数实现）地运行。

线程复用

在Java中，每个Thread类都有一个start方法。在程序调用start方法启动线程时，Java虚拟机会调用该类的run方法。前面说过，在Thread类的run方法中其实调用了Runnable对象的run方法，因此可以继承Thread类，在start方法中不断循环调用传递进来的Runnable对象，程序就会不断执行run方法中的代码。可以将在循环方法中不断获取的Runnable对象存放在Queue中，当前线程在获取下一个Runnable对象之前可以是阻塞的，这样既能有效控制正在执行的线程个数，也能保证系统中正在等待执行的其他线程有序执行。这样就简单实现了一个线程池，达到了线程复用的效果。

### 2.线程池的核心组件和核心类

Java线程池主要由以下4个核心组件组成。

- 线程池管理器：用于创建并管理线程池。
- 工作线程：线程池中执行具体任务的线程。
- 任务接口：用于定义工作线程的调度和执行策略，只有线程实现了该接口，线程中的任务才能够被线程池调度。
- 任务队列：存放待处理的任务，新的任务将会不断被加入队列中，执行完成的任务将被从队列中移除。

Java中的线程池是通过Executor框架实现的，在该框架中用到了Executor、Executors、ExecutorService、ThreadPoolExecutor、Callable、Future、FutureTask这几个核心类。

其中，ThreadPoolExecutor是构建线程的核心方法，该方法的定义如下：

```html
public ThreadPoolExecutor(int corePoolSize,int maxinumPoolSize,long keepAliveTime,TimeUnit unit,BlockingQueue<Runnable> workQueue){             
    this(corePoolSize,maxinumPoolSize,keepAliveTime,unit,workQueue,Executors.defaultThreadFactory(),
defaultHandler);
}
```

ThreadPoolExecutor构造函数的具体参数

- corePoolSize:线程池中核心线程的数量
- maxinumPoolSize：线程池中最大线程的数量
- keepAliveTime：当前线程数量超过corePoolSize时，空闲线程的存活时间
- unit：keepAliveTime的时间单位
- workQueue：任务队列，被提交但尚未被执行的任务存放的地方
- threadFactory：线程工程，用于创建线程，可使用默认的线程工厂或自定义线程工厂
- handler：由于任务过多或其他原因导致线程池无法处理时的任务拒接策略

### 3.Java线程池的工作流程

Java线程池的工作流程为：

1.线程池刚被创建时，只是向系统申请一个用于执行线程队列和管理线程池的线程资源。线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。

2.在调用execute()添加一个任务时，线程池会按照以下流程执行任务。

- 如果正在运行的线程数量少于corePoolSize（用户定义的核心线程数），线程池就会立刻创建线程并执行该线程任务。
- 如果正在运行的线程数量大于等于corePoolSize，该任务就将被放入阻塞队列中。
- 在阻塞队列已满且正在运行的线程数量少于maximumPoolSize时，线程池会创建非核心线程立刻执行该线程任务。
- 在阻塞队列已满且正在运行的线程数量大于等于maximumPoolSize时，线程池将拒绝执行该线程任务并抛出RejectExecutionException异常。

3.在线程任务执行完毕后，该任务将被从线程池队列中移除，线程池将从队列中取下一个线程任务继续执行。

4.在线程处于空闲状态的时间超过keepAliveTime时间时，正在运行的线程数量超过corePoolSize，该线程将会被认定为空闲线程并停止。因此在线程池中所有线程任务都执行完毕后，线程池会收缩到corePoolSize大小。

### 4.线程池的拒绝策略

若线程池中的核心线程数被用完且阻塞队列已排满，则此时线程池的线程资源已耗尽，线程池将没有足够的线程资源执行新的任务。为了保证操作系统的安全，线程池将通过拒绝策略处理新添加的线程任务。JDK内置的拒绝策略有AbortPolicy、CallerRunsPolicy、DiscardOldestPolicy、DiscardPolicy这 4种，默认的拒绝策略在ThreadPoolExecutor中作为内部类提供。在默认的拒绝策略不能满足应用的需求时，可以自定义拒绝策略。

AbortPolicy

AbortPolicy直接抛出异常，阻止线程正常运行。

CallerRunsPolicy

CallerRunsPolicy的拒绝策略为：如果被丢弃的线程任务未关闭，则执行该线程任务。注意，CallerRunsPolicy拒绝策略不会真的丢弃任务。

DiscardOldestPolicy

DiscardOldestPolicy的拒绝策略为：移除线程队列中最早的一个线程任务，并尝试提交当前任务。

DiscardPolicy

DiscardPolicy的拒绝策略为：丢弃当前的线程任务而不做任何处理。如果系统允许在资源不足的情况下丢弃部分任务，则这将是保障系统安全、稳定的一种很好的方案。

自定义拒绝策略

以上4种拒绝策略均实现了RejectedExecutionHandler接口，若无法满足实际需要，则用户可以自己扩展RejectedExecutionHandler接口来实现拒绝策略，并捕获异常来实现自定义拒绝策略。下面实现一个自定义拒绝策略DiscardOldestNPolicy，该策略根据传入的参数丢弃最老的N个线程，以便在出现异常时释放更多的资源，保障后续线程任务整体、稳定地运行。

### 5.5种常用的线程池

Java定义了Executor接口并在该接口中定义了execute()用于执行一个线程任务，然后通过ExecutorService实现Executor接口并执行具体的线程操作。ExecutorService接口有多个实现类可用于创建不同的线程池。

Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。

- newCachedThreadPool：可缓存的线程池
- newFixedThreadPool：固定大小的线程池
- newScheduledThreadPool：可做任务调度的线程池
- newSingleThreadExecutor：单个线程的线程池
- newWorkStealingPool：足够大小的线程池，JDK1.8新增

newCachedThreadPool

newCachedThreadPool用于创建一个缓存线程池。之所以叫缓存线程池，是因为它在创建新线程时如果有可重用的线程，则重用它们，否则重新创建一个新的线程并将其添加到线程池中。对于执行时间很短的任务而言，newCachedThreadPool线程池能很大程度地重用线程进而提高系统的性能。

在线程池的keepAliveTime时间超过默认的60秒后，该线程会被终止并从缓存中移除，因此在没有线程任务运行时，newCachedThreadPool将不会占用系统的线程资源。

在创建线程时需要执行申请CPU和内存、记录线程状态、控制阻塞等多项工作，复杂且耗时。因此，在有执行时间很短的大量任务需要执行的情况下，newCachedThreadPool能够很好地复用运行中的线程（任务已经完成但未关闭的线程）资源来提高系统的运行效率。具体的创建方式如下：

```html
ExecutorService cachedThreadPool=Executors.newCachedThreadPool();
```

newFixedThreadPool

newFixedThreadPool用于创建一个固定线程数量的线程池，并将线程资源存放在队列中循环使用。在newFixedThreadPool线程池中，若处于活动状态的线程数量大于等于核心线程池的数量，则新提交的任务将在阻塞队列中排队，直到有可用的线程资源，具体的创建方式如下：

```html
ExecutorService fixedThreadPool=Executors.newFixedThreadPool(5);
```

newScheduledThreadPool

newScheduledThreadPool创建了一个可定时调度的线程池，可设置在给定的延迟时间后执行或者定期执行某个线程任务：

```html
ScheduledExecutorService scheduledThreadPool=Executors.newScheduledThreadPool(3);
//创建一个延迟3秒执行的线程
scheduledThreadPool.schedule(new Runnable(){
    @Override
    public void run(){      
    }
},3,TimeUnit.SECONDS);
//创建一个延迟1秒执行且每3秒执行一次的线程
scheduledThreadPool.scheduleAtFixedRate(new Runnable(){
    @Override
    public void run(){      
    }
},1,3,TimeUnit.SECONDS);
```

newSingleThreadExecutor

newSingleThreadExecutor线程池会保证永远有且只有一个可用的线程，在该线程停止或发生异常时，newSingleThreadExecutor线程池会启动一个新的线程来代替该线程继续执行任务：

```html
ExecutorService singleThread=Executors.newSingleThreadExecutor();
```

newWorkStealingPool

JDK1.8新增。

newWorkStealingPool创建持有足够线程的线程池来达到快速运算的目的，在内部通过使用多个队列来减少各个线程调度产生的竞争。这里所说的有足够的线程指JDK根据当前线程的运行需求向操作系统申请足够的线程，以保障线程的快速执行，并很大程度地使用系统资源，提高并发计算的效率，省去用户根据CPU资源估算并行度的过程。当然，如果开发者想自己定义线程的并发数，则也可以将其作为参数传入。

### 6.如果在线程池中使用无界阻塞队列会发生什么问题？

调用超时，队列变得越来越大，此时会导致内存飙升起来，而且还可能会导致你会OOM，内存溢出。

### 7.线程池都有哪些状态？

RUNNING：这是最正常的状态，接受新的任务，处理等待队列中的任务。 SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。 STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程。 TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()。 TERMINATED：terminated()方法结束后，线程池的状态就会变成这个。

### 8.线程池中 submit() 和 execute() 方法有什么区别？

execute()：只能执行 Runnable 类型的任务。 submit()：可以执行 Runnable 和 Callable 类型的任务。 Callable 类型的任务可以获取执行的返回值，而 Runnable 执行无返回值。

### 9.什么是 Callable、Future、FutureTask ？

1）**Callable**

Callable 接口，类似于 Runnable ，从名字就可以看出来了，但是Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。

简单来说，可以认为是带有回调的 Runnable 。

2）**Future**

Future 接口，表示异步任务，是还没有完成的任务给出的未来结果。所以说 Callable 用于产生结果，Future 用于获取结果。

3）**FutureTask**

在 Java 并发程序中，FutureTask 表示一个可以取消的异步运算。

- 它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。
- 一个 FutureTask 对象，可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是继承了 Runnable 接口，所以它可以提交给 Executor 来执行。

### 10.**在 Java 中 Executor 和 Executors 的区别？**

- Executors 是 Executor 的工具类，不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。
- Executor 接口对象，能执行我们的线程任务。
  - ExecutorService 接口，继承了 Executor 接口，并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。
    - 使用 ThreadPoolExecutor ，可以创建自定义线程池。
  - Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 `#get()` 方法，获取计算的结果。

### 11.**如果你提交任务时，线程池队列已满，这时会发生什么？**

- 如果你使用的 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务。
- 如果你使用的是有界队列比方说 ArrayBlockingQueue 的话，任务首先会被添加到 ArrayBlockingQueue 中，ArrayBlockingQueue满了，则会使用拒绝策略 RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy 。

### 12.创建线程池

使用Executors工具类创建线程池

使用ThreadPoolExecutor类创建线程池

ThreadPoolExecutor类继承自AbstractExecutorService，也就是说，ThreadPoolExecutor类具有AbstractExecutorService类的全部功能。

使用ForkJoinPool类创建线程池

本质上调用的是ForkJoinPool类的构造方法类创建线程池，而从代码结构上来看ForkJoinPool类继承自AbstractExecutorService抽象类。

使用ScheduledThreadPoolExecutor类创建线程池

### 14.线程池已有线程在执行任务时再次提交任务会如何执行呢？

SynchronousQueue的原理，TransferStack来进行数据传递

put + take的方式，才能实现他原本希望实现的一个效果，如果put的时候没有人在take，此时就会将head指针指向put操作，put线程就park挂起

再次另外一个线程来put，head指针指向最新的线程put的数据，因为没有人take，所以也是挂起

他其实是用来干两个线程之间的数据传递的同步，put + take，take的时候，如果没人put，此时你会阻塞，直到有人put，是最后一次take的人先可以获取到别人put的数据，栈的效果

put的时候，如果没人take，此时你会阻塞住，多人put，最后一个put是在栈顶，此时有人take，先获取最后一次put的数据，栈，后进先出，queue，反过来，先入先出，默认的实现是栈的实现

SynchronousQueue的offer，他是不会阻塞的，如果没有take，offer的时候直接就是返回null，就是入队失败

此时如果有已经有一个非core线程在执行任务，再来第二个任务要提交，此时会如何呢？当前线程数量 = 1，corePoolSize = 0，1 < 0？不成立，所以此时还是不会直接创建一个core线程出来

此时还是会直接创建一个非core的线程来直接执行提交的任务

### 15.非core线程执行完之后如何尝试从队列获取下一个任务

当前线程数量，一定是大于corePoolSize的，corePoolSize是0，无论你有多少个线程，都是属于非core线程，都会大于，这里的timed就是true

非core线程在从队列获取任务的时候走的是poll，非阻塞的，而且有超时时间，keepAliveTime，如果超过指定时间没获取到任务，此时就会当前线程就会自动释放掉，此时会进入SynchronousQueue的栈

他会位于栈顶，但是挂起当前线程的时间超时时间是60s，超过60s的话，就认为此次poll就失败了，null，此时就会返回一个null，此时就自动触发这个非core线程的释放了

### 16.cached线程池有线程的时候提交任务到队列又如何被执行

如果没有线程在从队列获取任务，此时无论你提交多少任务，SynchronousQueue入队都是失败的，offer都是返回false，此时都会直接创建线程来执行任务的，但是如果有线程空闲出来就会尝试从队列poll任务

如果队列为空，此时最多会等待60s空闲去poll一个任务出来，如果超过了60s没有poll到任务，此时这个线程自己就会释放掉

此时你提交任务会如何呢？

此时会通过SynchronousQueue实现一个match配对，offer入队的任务会给最后一个poll任务的线程去执行，如果有线程在poll队列的话，那么你入队都是可以成功的，会交给某个线程来执行的

### 17.不停往cached线程池提交任务时会导致CPU负载过高吗？

答案是：会，就可能会在系统高峰期导致大量的线程被创建出来，然后就是导致机器的CPU负载过高，更有甚至，就是线程太多导致内存溢出，线程太多了，导致CPU负载飙升。队列无限增长，内存飙升

### 18.cached线程池又会不会触发拒绝任务提交的机制呢？

答案是：不会

第一种fixed，是基于有限固定数量的线程处理源源不断涌入的任务，但是呢，无界队列，所以任务可以无限制的涌入和排队

第二种cached，是在需要的时候无限制的创建新的线程来处理新的任务，提交的任务几乎是不会排队的，永远能最快速度的得到执行，入队的时候先看看有没有人空闲在poll，如果有立马执行

4核8G，虚拟机，一般来说，线程池开启线程来异步处理任务，200以内，100~200的时候，线程机器的CPU负载就很高了，内存队列排队个几十万个任务，也还好，内存也没撑爆，但是如果你的线程一旦达到四五百个，线上机器的CPU负载过高的报警

### 19.如何根据系统的业务场景需求定制自己的线程池

如果你的业务场景有一些需求的话，你完全可以自己直接构造ThreadPoolExecutor的线程池，传入一些参数符合你自己的业务需求的，fixed和cached、scheduled可以满足大部分场景的线程池的使用了

fixed，但是你希望队列是有界的，此时你就可以自己定制了

corePoolSize：线程池里的线程是0，少于这个数量，他会自动创建这个数量的线程的，只要线程池里的线程是少于corePoolSize，他后面就会以take阻塞的方式从队列里获取任务，这些线程是不会释放了

任务一定会直接入队，此时你是用什么队列？无界队列，有界队列，同步队列，LinkedBlockingQueue和SynchronousQueue，如果你看一些开源项目里对线程池的定制的源码的话

有界队列的话，此时如果队列满了，入队失败，就会尝试以非core方式创建线程，直接执行任务，最多线程池里的线程不能超过maximumPoolSize，如果线程池里的线程总数一旦超过了corePoolSize之后

超出corePoolSize数量的那些线程，在从队列获取的时候是走的poll + 超时时间（keepAliveTime）的方式，也就是如果一定时间空闲获取不到任务，自动退出释放线程，维持空闲的线程数量在小于等于corePoolSize

如果等待队列也满了，而且线程数量达到了maximumPoolSize，此时会直接执行reject策略，拒绝你提交任务

创建线程的时候，可以用自己的threadFactory，定制你的线程是不是要设置为daemon，需要不需要线程组的概念

### 31.线程池

为什么要使用线程池

使用线程池主要有以下三个原因：

1. 创建/销毁线程需要消耗系统资源，线程池可以**复用已创建的线程**。
2. **控制并发的数量**。并发数量过多，可能会导致资源消耗过多，从而造成服务器崩溃。（主要原因）
3. **可以对线程做统一管理**。

线程池的原理

Java中的线程池顶层接口是`Executor`接口，`ThreadPoolExecutor`是这个接口的实现类。

ThreadPoolExecutor提供的构造方法

一共有四个构造方法：

涉及到5~7个参数，我们先看看必须的5个参数是什么意思：

- **int corePoolSize**：该线程池中**核心线程数最大值**

  > 核心线程：线程池中有两类线程，核心线程和非核心线程。核心线程默认情况下会一直存在于线程池中，即使这个核心线程什么都不干（铁饭碗），而非核心线程如果长时间的闲置，就会被销毁（临时工）。

- **int maximumPoolSize**：该线程池中**线程总数最大值** 。

  > 该值等于核心线程数量 + 非核心线程数量。

- **long keepAliveTime**：**非核心线程闲置超时时长**。

  > 非核心线程如果处于闲置状态超过该值，就会被销毁。如果设置allowCoreThreadTimeOut(true)，则会也作用于核心线程。

- **TimeUnit unit**：keepAliveTime的单位。

TimeUnit是一个枚举类型 ，包括以下属性：

> NANOSECONDS ： 1微毫秒 = 1微秒 / 1000 MICROSECONDS ： 1微秒 = 1毫秒 / 1000 MILLISECONDS ： 1毫秒 = 1秒 /1000 SECONDS ： 秒 MINUTES ： 分 HOURS ： 小时 DAYS ： 天

- **BlockingQueue workQueue**：阻塞队列，维护着**等待执行的Runnable任务对象**。

  常用的几个阻塞队列：

  1. **LinkedBlockingQueue**

     链式阻塞队列，底层数据结构是链表，默认大小是`Integer.MAX_VALUE`，也可以指定大小。

  2. **ArrayBlockingQueue**

     数组阻塞队列，底层数据结构是数组，需要指定队列的大小。

  3. **SynchronousQueue**

     同步队列，内部容量为0，每个put操作必须等待一个take操作，反之亦然。

  4. **DelayQueue**

     延迟队列，该队列中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素 。

还有两个非必须的参数。

- **ThreadFactory threadFactory**

  创建线程的工厂 ，用于批量创建线程，统一在创建线程时设置一些参数，如是否守护线程、线程的优先级等。如果不指定，会新建一个默认的线程工厂。

- **RejectedExecutionHandler handler**

  **拒绝处理策略**，线程数量大于最大线程数就会采用拒绝处理策略，四种拒绝处理的策略为 ：

  1. **ThreadPoolExecutor.AbortPolicy**：**默认拒绝处理策略**，丢弃任务并抛出RejectedExecutionException异常。
  2. **ThreadPoolExecutor.DiscardPolicy**：丢弃新来的任务，但是不抛出异常。
  3. **ThreadPoolExecutor.DiscardOldestPolicy**：丢弃队列头部（最旧的）的任务，然后重新尝试执行程序（如果再次失败，重复此过程）。
  4. **ThreadPoolExecutor.CallerRunsPolicy**：由调用线程处理该任务。

ThreadPoolExecutor的策略

线程池本身有一个调度线程，这个线程就是用于管理布控整个线程池里的各种任务和事务，例如创建线程、销毁线程、任务队列管理、线程队列管理等等。

故线程池也有自己的状态。`ThreadPoolExecutor`类中定义了一个`volatile int`变量**runState**来表示线程池的状态 ，分别为RUNNING、SHURDOWN、STOP、TIDYING 、TERMINATED。

- 线程池创建后处于**RUNNING**状态。

- 调用shutdown()方法后处于**SHUTDOWN**状态，线程池不能接受新的任务，清除一些空闲worker,会等待阻塞队列的任务完成。

- 调用shutdownNow()方法后处于**STOP**状态，线程池不能接受新的任务，中断所有线程，阻塞队列中没有被执行的任务全部丢弃。此时，poolsize=0,阻塞队列的size也为0。

- 当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为**TIDYING**状态。接着会执行terminated()函数。

  > ThreadPoolExecutor中有一个控制状态的属性叫ctl，它是一个AtomicInteger类型的变量。

- 线程池处在TIDYING状态时，**执行完terminated()方法之后**，就会由 **TIDYING -> TERMINATED**， 线程池被设置为TERMINATED状态。

线程池主要的任务处理流程

处理任务的核心方法是`execute`.

**为什么要二次检查线程池的状态?**

在多线程的环境下，线程池的状态是时刻发生变化的。很有可能刚获取线程池状态后线程池状态就改变了。判断是否将`command`加入`workqueue`是线程池之前的状态。倘若没有二次检查，万一线程池处于非**RUNNING**状态（在多线程环境下很有可能发生），那么`command`永远不会执行。

**总结一下处理流程**

1. 线程总数量 < corePoolSize，无论线程是否空闲，都会新建一个核心线程执行任务（让核心线程数量快速达到corePoolSize，在核心线程数量 < corePoolSize时）。**注意，这一步需要获得全局锁。**
2. 线程总数量 >= corePoolSize时，新来的线程任务会进入任务队列中等待，然后空闲的核心线程会依次去缓存队列中取任务来执行（体现了**线程复用**）。
3. 当缓存队列满了，说明这个时候任务已经多到爆棚，需要一些“临时工”来执行这些任务了。于是会创建非核心线程去执行这个任务。**注意，这一步需要获得全局锁。**
4. 缓存队列满了， 且总线程数达到了maximumPoolSize，则会采取上面提到的拒绝策略进行处理。

ThreadPoolExecutor如何做到线程复用的？

我们知道，一个线程在创建的时候会指定一个线程任务，当执行完这个线程任务之后，线程自动销毁。但是线程池却可以复用线程，即一个线程执行完线程任务后不销毁，继续执行另外的线程任务。**那么，线程池如何做到线程复用呢？**

原来，ThreadPoolExecutor在创建线程时，会将线程封装成**工作线程worker**,并放入**工作线程组**中，然后这个worker反复从阻塞队列中拿任务去执行。

### 32.四种常见的线程池

newCachedThreadPool

```html
public static ExecutorService newCachedThreadPool() {
    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,60L, TimeUnit.SECONDS,new SynchronousQueue<Runnable>());
}
```

`CacheThreadPool`的**运行流程**如下：

1. 提交任务进线程池。
2. 因为**corePoolSize**为0的关系，不创建核心线程，线程池最大为Integer.MAX_VALUE。
3. 尝试将任务添加到**SynchronousQueue**队列。
4. 如果SynchronousQueue入列成功，等待被当前运行的线程空闲后拉取执行。如果当前没有空闲线程，那么就创建一个非核心线程，然后从SynchronousQueue拉取任务并在当前线程执行。
5. 如果SynchronousQueue已有任务在等待，入列操作将会阻塞。

当需要执行很多**短时间**的任务时，CacheThreadPool的线程复用率比较高， 会显著的**提高性能**。而且线程60s后会回收，意味着即使没有任务进来，CacheThreadPool并不会占用很多资源。

newFixedThreadPool

```html
public static ExecutorService newFixedThreadPool(int nThreads) {
        return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());
}
```

核心线程数量和总线程数量相等，都是传入的参数nThreads，所以只能创建核心线程，不能创建非核心线程。因为LinkedBlockingQueue的默认大小是Integer.MAX_VALUE，故如果核心线程空闲，则交给核心线程处理；如果核心线程不空闲，则入列等待，直到核心线程空闲。

**与CachedThreadPool的区别**：

- 因为 corePoolSize == maximumPoolSize ，所以FixedThreadPool只会创建核心线程。 而CachedThreadPool因为corePoolSize=0，所以只会创建非核心线程。
- 在 getTask() 方法，如果队列里没有任务可取，线程会一直阻塞在 LinkedBlockingQueue.take() ，线程不会被回收。 CachedThreadPool会在60s后收回。
- 由于线程不会被回收，会一直卡在阻塞，所以**没有任务的情况下， FixedThreadPool占用资源更多**。
- 都几乎不会触发拒绝策略，但是原理不同。FixedThreadPool是因为阻塞队列可以很大（最大为Integer最大值），故几乎不会触发拒绝策略；CachedThreadPool是因为线程池很大（最大为Integer最大值），几乎不会导致线程数量大于最大线程数，故几乎不会触发拒绝策略。

newSingleThreadExecutor

```html
public static ExecutorService newSingleThreadExecutor() {
    return new FinalizableDelegatedExecutorService(new ThreadPoolExecutor(1, 1,0L, TimeUnit.MILLISECONDS,
           new LinkedBlockingQueue<Runnable>()));
}
```

有且仅有一个核心线程（ corePoolSize == maximumPoolSize=1），使用了LinkedBlockingQueue（容量很大），所以，**不会创建非核心线程**。所有任务按照**先来先执行**的顺序执行。如果这个唯一的线程不空闲，那么新来的任务会存储在任务队列里等待执行。

newScheduledThreadPool

创建一个定长线程池，支持定时及周期性任务执行。

```html
public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {    
    return new ScheduledThreadPoolExecutor(corePoolSize);
}
//ScheduledThreadPoolExecutor():
public ScheduledThreadPoolExecutor(int corePoolSize) {    
    super(corePoolSize, Integer.MAX_VALUE,DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,new DelayedWorkQueue());}
```

四种常见的线程池基本够我们使用了，但是《阿里把把开发手册》不建议我们直接使用Executors类中的线程池，而是通过`ThreadPoolExecutor`的方式，这样的处理方式让写的同学需要更加明确线程池的运行规则，规避资源耗尽的风险。

但如果你及团队本身对线程池非常熟悉，又确定业务规模不会大到资源耗尽的程度（比如线程数量或任务队列长度可能达到Integer.MAX_VALUE）时，其实是可以使用JDK提供的这几个接口的，它能让我们的代码具有更强的可读性。

### 40.线程池大小确定

**线程池数量的确定一直是困扰着程序员的一个难题，大部分程序员在设定线程池大小的时候就是随心而定。**

很多人甚至可能都会觉得把线程池配置过大一点比较好！我觉得这明显是有问题的。就拿我们生活中非常常见的一例子来说：**并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。** 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了**上下文切换**成本。不清楚什么是上下文切换的话，可以看我下面的介绍。

> 上下文切换：
>
> 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。**任务从保存到再加载的过程就是一次上下文切换**。
>
> 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。
>
> Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。

**类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。**

**如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。**

**但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。**

有一个简单并且适用面比较广的公式：

- **CPU 密集型任务(N+1)：** 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。
- **I/O 密集型任务(2N)：** 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。

**如何判断是 CPU 密集任务还是 IO 密集任务？**

CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。单凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。

### 45.线程池最佳实践

1.使用 `ThreadPoolExecutor`的构造函数声明线程池

**1. 线程池必须手动通过 `ThreadPoolExecutor`的构造函数来声明，避免使用`Executors`类的 `newFixedThreadPool` 和 `newCachedThreadPool` ，因为可能会有 OOM 的风险。**

> Executors 返回线程池对象的弊端如下：
>
> - **`FixedThreadPool` 和 `SingleThreadExecutor`** ： 允许请求的队列长度为 `Integer.MAX_VALUE`,可能堆积大量的请求，从而导致 OOM。
> - **CachedThreadPool 和 ScheduledThreadPool** ： 允许创建的线程数量为 `Integer.MAX_VALUE` ，可能会创建大量线程，从而导致 OOM。

说白了就是：**使用有界队列，控制线程创建数量。**

除了避免 OOM 的原因之外，不推荐使用 `Executors`提供的两种快捷的线程池的原因还有：

1. 实际使用中需要根据自己机器的性能、业务场景来手动配置线程池的参数比如核心线程数、使用的任务队列、饱和策略等等。
2. 我们应该显示地给我们的线程池命名，这样有助于我们定位问题。

2.监测线程池运行状态

你可以通过一些手段来检测线程池的运行状态比如 SpringBoot 中的 Actuator 组件。

3.建议不同类别的业务用不同的线程池

一般建议是不同的业务使用不同的线程池，配置线程池的时候根据当前业务的情况对当前线程池进行配置，因为不同的业务的并发以及对资源的使用情况都不同，重心优化系统性能瓶颈相关的业务。

4.别忘记给线程池命名

初始化线程池的时候需要显示命名（设置线程池名称前缀），有利于定位问题。

默认情况下创建的线程名字类似 pool-1-thread-n 这样的，没有业务含义，不利于我们定位问题。

5.正确配置线程池参数

### 46.**一个线程池中的线程异常了，那么线程池会怎么处理这个线程?**

当一个线程池里面的线程异常后:

> 1. 当执行方式是execute时,可以看到堆栈异常的输出。
> 2. 当执行方式是submit时,堆栈异常没有输出。但是调用Future.get()方法时，可以捕获到异常。
> 3. 不会影响线程池里面其他线程的正常执行。
> 4. 线程池会把这个线程移除掉，并创建一个新的线程放到线程池中。

### 47.如何设置线程池参数？

1. **corePoolSize**：the number of threads to keep in the pool, even if they are idle, unless {@code allowCoreThreadTimeOut} is set

   （核心线程数大小：不管它们创建以后是不是空闲的。线程池需要保持 corePoolSize 数量的线程，除非设置了 allowCoreThreadTimeOut。）

2. **maximumPoolSize**：the maximum number of threads to allow in the pool。

   （最大线程数：线程池中最多允许创建 maximumPoolSize 个线程。）

3. **keepAliveTime**：when the number of threads is greater than the core, this is the maximum time that excess idle threads will wait for new tasks before terminating。

   （存活时间：如果经过 keepAliveTime 时间后，超过核心线程数的线程还没有接受到新的任务，那就回收。）

4. **unit：**the time unit for the {@code keepAliveTime} argument

   （keepAliveTime 的时间单位。）

5. **workQueue**：the queue to use for holding tasks before they are executed. This queue will hold only the {@code Runnable} tasks submitted by the {@code execute} method。

   （存放待执行任务的队列：当提交的任务数超过核心线程数大小后，再提交的任务就存放在这里。它仅仅用来存放被 execute 方法提交的 Runnable 任务。所以这里就不要翻译为工作队列了，好吗？不要自己给自己挖坑。）

6. **threadFactory**：the factory to use when the executor creates a new thread。

   （线程工程：用来创建线程工厂。比如这里面可以自定义线程名称，当进行虚拟机栈分析时，看着名字就知道这个线程是哪里来的，不会懵逼。）

7. **handler** ：the handler to use when execution is blocked because the thread bounds and queue capacities are reached。

   （拒绝策略：当队列里面放满了任务、最大线程数的线程都在工作时，这时继续提交的任务线程池就处理不了，应该执行怎么样的拒绝策略。）

上面的 7 个参数中，我们主要需要关心的参数是：

*corePoolSize、*maximumPoolSize、workQueue（队列长度）。

### 48.核心线程数会被回收吗？需要什么设置？

核心线程数默认是不会被回收的，如果需要回收核心线程数，需要调用下面的方法：

allowCoreThreadTimeOut 该值默认为 false。

## 锁

### 1.Java中的锁

Java中的锁主要用于保障多并发线程情况下数据的一致性。在多线程编程中为了保障数据的一致性，我们通常需要在使用对象或者方法之前加锁，这时如果有其他线程也需要使用该对象或者该方法，则首先要获得锁，如果某个线程发现锁正在被其他线程使用，就会进入阻塞队列等待锁的释放，直到其他线程执行完成并释放锁，该线程才有机会再次获取锁进行操作。这样就保障了在同一时刻只有一个线程持有该对象的锁并修改对象，从而保障数据的安全。

锁从乐观和悲观的角度可分为乐观锁和悲观锁，从获取资源的公平性角度可分为公平锁和非公平锁，从是否共享资源的角度可分为共享锁和独占锁，从锁的状态的角度可分为偏向锁、轻量级锁和重量级锁。同时，在JVM中还巧妙设计了自旋锁以更快地使用CPU资源。

### 2.乐观锁

乐观锁采用乐观的思想处理数据，在每次读取数据时都认为别人不会修改该数据，所以不会上锁，但在更新时会判断在此期间别人有没有更新该数据，通常采用在写时先读出当前版本号然后加锁的方法。具体过程为：比较当前版本号与上一次的版本号，如果版本号一致，则更新，如果版本号不一致，则重复进行读、比较、写操作。

Java中的乐观锁大部分是通过CAS（Compare And Swap，比较和交换）操作实现的，CAS是一种原子更新操作，在对数据操作之前首先会比较当前值跟传入的值是否一样，如果一样则更新，否则不执行更新操作，直接返回失败状态。

### 3.悲观锁

悲观锁采用悲观思想处理数据，在每次读取数据时都认为别人会修改数据，所以每次在读写数据时都会上锁，这样别人想读写这个数据时就会阻塞、等待直到拿到锁。

Java中的悲观锁大部分基于AQS（Abstract Queued Synchronized，抽象的队列同步器）架构实现。AQS定义了一套多线程访问共享资源的同步框架，许多同步类的实现都依赖于它，例如常用的Synchronized、ReentrantLock、Semaphore、CountDownLatch等。该框架下的锁会先尝试以CAS乐观锁去获取锁，如果获取不到，则会转为悲观锁（如RetreenLock）。

### 4.自旋锁

自旋锁认为：如果持有锁的线程能在很短的时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞、挂起状态，只需等一等（也叫作自旋），在等待持有锁的线程释放锁后即可立即获取锁，这样就避免了用户线程在内核状态的切换上导致的锁时间消耗。

线程在自旋时会占用CPU，在线程长时间自旋获取不到锁时，将会产CPU的浪费，甚至有时线程永远无法获取锁而导致CPU资源被永久占用，所以需要设定一个自旋等待的最大时间。在线程执行的时间超过自旋等待的最大时间后，线程会退出自旋模式并释放其持有的锁。

如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。

自旋锁的优缺点

- 优点：自旋锁可以减少CPU 上下文的切换，对于占用锁的时间非常短或锁竞争不激烈的代码块来说性能大幅度提升，因为自旋的CPU 耗时明显少于线程阻塞、挂起、再唤醒时两次CPU上下文切换所用的时间。
- 缺点：在持有锁的线程占用锁时间过长或锁的竞争过于激烈时，线程在自旋过程中会长时间获取不到锁资源，将引起CPU 的浪费。所以在系统中有复杂锁依赖的情况下不适合采用自旋锁。

自旋锁的时间阈值

自旋锁用于让当前线程占着CPU的资源不释放，等到下次自旋获取锁资源后立即执行相关操作。但是如何选择自旋的执行时间呢？如果自旋的执行时间太长，则会有大量的线程处于自旋状态且占用CPU资源，造成系统资源浪费。因此，对自旋的周期选择将直接影响到系统的性能！

JDK的不同版本所采用的自旋周期不同，JDK 1.5为固定DE时间，JDK1.6引入了适应性自旋锁。适应性自旋锁的自旋时间不再是固定值，而是由上一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的，可基本认为一个线程上下文切换的时间是就一个最佳时间。同时JVM还针对当前CPU的负荷情况做了较多的优化，如果平均负载小于CPUs则一直自旋，如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞，如果正在自旋的线程发现Owner发生了变化则延迟自旋时间（自旋计数）或进入阻塞，如果CPU处于节电模式则停止自旋，自旋时间的最坏情况是CPU的存储延迟（CPU A存储了一个数据，到CPU B得知这个数据直接的时间差），自旋时会适当放弃线程优先级之间的差异。

自旋锁的开启

JDK1.6中-XX:+UseSpinning开启；

-XX:PreBlockSpin=10 为自旋次数；

JDK1.7后，去掉此参数，由jvm控制；

适应自旋锁

JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。

### 5.tryLock、lock和lockInterruptibly的区别

tryLock、lock和lockInterruptibly的区别如下。

- tryLock 若有可用锁， 则获取该锁并返回true ， 否则返回false，不会有延迟或等待；tryLock(long timeout,TimeUnit unit)可以增加时间限制，如果超过了指定的时间还没获得锁，则返回false。
- lock若有可用锁，则获取该锁并返回true，否则会一直等待直到获取可用锁。
- 在锁中断时lockInterruptibly会抛出异常，lock不会。

### 6.Condition类和Object类锁方法区别

- Condition类的awiat方法和Object类的wait方法等效
- Condition类的signal方法和Object类的notify方法等效
- Condition类的signalAll方法和Object类的notifyAll方法等效
- ReentrantLock类可以唤醒指定条件的线程，而object的唤醒是随机的

### 7.可重入锁（递归锁）

可重入锁也叫作递归锁，指在同一线程中，在外层函数获取到该锁之后，内层的递归函数仍然可以继续获取该锁。在Java环境下，ReentrantLock和synchronized都是可重入锁。

如果锁具备可重入性，则称作为可重入锁。像synchronized和ReentrantLock都是可重入锁， 可重入性在我看来实际上表明了锁的分配机制：基于线程的分配，而不是基于方法调用的分配。举个简单的例子，当一个线程执行到某个synchronized方法时，比如说method1，而在

method1中会调用另外一个synchronized方法method2，此时线程不必重新去申请锁，而是可以直接执行方法method2。

上述代码中的两个方法method1和method2都用synchronized修饰了，假如某一时刻，线程A 执行到了method1，此时线程A获取了这个对象的锁，而由于method2也是synchronized方 法，假如synchronized不具备可重入性，此时线程A需要重新申请锁。但是这就会造成一个问题，因为线程A已经持有了该对象的锁，而又在申请获取该对象的锁，这样就会线程A一直等待永远不会获取到的锁。

而由于synchronized和Lock都具备可重入性，所以不会发生上述现象。

### 8.公平锁与非公平锁

- 公平锁（Fair Lock）指在分配锁前检查是否有线程在排队等待获取该锁，优先将锁分配给排队时间最长的线程。
- 非公平锁（Nonfair Lock）指在分配锁时不考虑线程排队等待的情况，直接尝试获取锁，在获取不到锁时再排到队尾等待。

非公平锁性能比公平锁高5~10倍，因为公平锁需要在多核的情况下维护一个锁线程等待队列，基于该队列进行锁的分配，因此效率比非公平锁低很多。

Java中的synchronized是非公平锁，ReentrantLock默认的lock方法采用的是非公平锁。

### 9.读写锁：ReadWriteLock

在Java中通过Lock接口及对象可以方便地为对象加锁和释放锁，但是这种锁不区分读写，叫作普通锁。为了提高性能，Java提供了读写锁。读写锁分为读锁和写锁两种，多个读锁不互斥，读锁与写锁互斥。在读的地方使用读锁，在写的地方使用写锁，在没有写锁的情况下，读是无阻塞的。 如果系统要求共享数据可以同时支持很多线程并发读，但不能支持很多线程并发写，那么使用读锁能很大程度地提高效率；如果系统要求共享数据在同一时刻只能有一个线程在写，且在写的过程中不能读取该共享数据，则需要使用写锁。 一般做法是分别定义一个读锁和一个写锁，在读取共享数据时使用读锁，在使用完成后释放读锁，在写共享数据时使用写锁，在使用完成后释放写锁。在Java 中， 通过读写锁的接口java.util.concurrent.locks.ReadWriteLoc 的实现类ReentrantReadWriteLock来完成对读写锁的定义和使用。

```html
private final ReentrantReadWriteLock rwlock=new ReentrantReadWriteLock();
//定义读锁
private final Lock readLock=rwlock.readLock();
//定义写锁
private final Lock writeLock=rwlock.writeLock();
//在读数据时加读锁
readLock.lock();
//在写数据时加写锁
writeLock.lock();
```

### 10.共享锁和独占锁

Java并发包提供的加锁模式分为独占锁和共享锁。

- 独占锁：也叫互斥锁，每次只允许一个线程持有该锁，ReentrantLock为独占锁的实现。独占锁是一种悲观保守的加锁策略，它避免了读/读冲突，如果某个只读线程获取锁，则其他读线程都只能等待，这种情况下就限制了不必要的并发性，因为读操作并不会影响数据的一致性。
- 共享锁：允许多个线程同时获取该锁，并发访问共享资源。ReentrantReadWriteLock中的读锁为共享锁的实现。共享锁则是一种乐观锁，它放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。

ReentrantReadWriteLock的加锁和解锁操作最终都调用内部类Sync提供的方法。Sync对象通过继承AQS（Abstract Queued Synchronizer）进行实现。

AQS的内部类Node定义了两个常量SHARED和EXCLUSIVE，分别标识AQS队列中等待线程的锁获取模式。

独占锁是一种悲观的加锁策略，同一时刻只允许一个读线程读取锁资源，限制了读操作的并发性；因为并发读线程并不会影响数据的一致性，因此共享锁采用了乐观的加锁策略，允许多个执行读操作的线程同时访问共享资源。

java的并发包中提供了ReadWriteLock，读-写锁。它允许一个资源可以被多个读操作访问，或者被一个写操作访问，但两者不能同时进行。

### 11.重量级锁（Mutex Lock）

重量级锁是基于操作系统的互斥量（Mutex Lock）而实现的锁，会导致进程在用户态与内核态之间切换，相对开销较大。

synchronized在内部基于监视器锁（Monitor）实现，监视器锁基于底层的操作系统的Mutex Lock实现，因此synchronized属于重量级锁。重量级锁需要在用户态和核心态之间做转换，所以synchronized的运行效率不高。

因此，这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”。JDK中对Synchronized做的种种优化，其核心都是为了减少这种重量级锁的使用。

### 12.轻量级锁

JDK在 1.6版本以后，为了减少获取锁和释放锁所带来的性能消耗及提高性能，引入了轻量级锁和偏向锁。

锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。

锁升级

随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。

获取锁

1. 判断当前对象是否处于无锁状态（hashcode、0、01），若是，则JVM首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word）；否则执行步骤（3）；
2. JVM利用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指正，如果成功表示竞争到锁，则将锁标志位变成00（表示此对象处于轻量级锁状态），执行同步操作；如果失败则执行步骤（3）；
3. 判断当前对象的Mark Word是否指向当前线程的栈帧，如果是则表示当前线程已经持有当前对象的锁，则直接执行同步代码块；否则只能说明该锁对象已经被其他线程抢占了，这时轻量级锁需要膨胀为重量级锁，锁标志位变成10，后面等待的线程将会进入阻塞状态；

释放锁

轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下：

1. 取出在获取轻量级锁保存在Displaced Mark Word中的数据；
2. 用CAS操作将取出的数据替换当前对象的Mark Word中，如果成功，则说明释放锁成功，否则执行（3）；
3. 如果CAS操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。

### 13.偏向锁

除了在多线程之间存在竞争获取锁的情况，还会经常出现同一个锁被同一个线程多次获取的情况。偏向锁用于在某个线程获取某个锁之后，消除这个线程锁重入的开销，看起来似乎是这个线程得到了该锁的偏向（偏袒）。

偏向锁的主要目的是在同一个线程多次获取某个锁的情况下尽量减少轻量级锁的执行路径，因为轻量级锁的获取及释放需要多次CAS（Compareand Swap）原子操作，而偏向锁只需要在切换ThreadID时执行一次CAS原子操作，因此可以提高锁的运行效率。

在出现多线程竞争锁的情况时，JVM会自动撤销偏向锁，因此偏向锁的撤销操作的耗时必须少于节省下来的CAS原子操作的耗时。

综上所述，轻量级锁用于提高线程交替执行同步块时的性能，偏向锁则在某个线程交替执行同步块时进一步提高性能。

锁的状态总共有4种：无锁、偏向锁、轻量级锁和重量级锁。随着锁竞争越来越激烈，锁可能从偏向锁升级到轻量级锁，再升级到重量级锁，但在Java中锁只单向升级，不会降级。

获取锁

1. 检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01；
2. 若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，则执行步骤（5），否则执行步骤（3）；
3. 如果线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，否则执行线程（4）；
4. 通过CAS竞争锁失败，证明当前存在多线程竞争情况，当到达全局安全点，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码块；
5. 执行同步代码块

释放锁

偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。偏向锁的撤销需要等待全局安全点（这个时间点是上没有正在执行的代码）。其步骤如下：

1. 暂停拥有偏向锁的线程，判断锁对象石是否还处于被锁定状态；
2. 撤销偏向苏，恢复到无锁状态（01）或者轻量级锁的状态；

### 14.分段锁

分段锁并非一种实际的锁，而是一种思想，用于将数据分段并在每个分段上都单独加锁，把锁进一步细粒度化，以提高并发效率。ConcurrentHashMap在内部就是使用分段锁实现的。

### 15.同步锁与死锁

同步锁

当多个线程同时访问同一个数据时，很容易出现问题。为了避免这种情况出现，我们要保证线程同步互斥，就是指并发执行的多个线程，在同一时间内只允许一个线程访问共享数据。Java中可以使用synchronized关键字来取得一个对象的同步锁。

死锁

何为死锁，就是多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。

### 16.如何进行锁优化

减少锁持有的时间

减少锁持有的时间指只在有线程安全要求的程序上加锁来尽量减少同步代码块对锁的持有时间。

减小锁粒度

减小锁粒度指将单个耗时较多的锁操作拆分为多个耗时较少的锁操作来增加锁的并行度，减少同一个锁上的竞争。在减少锁的竞争后，偏向锁、轻量级锁的使用率才会提高。减小锁粒度最典型的案例就是ConcurrentHashMap中的分段锁。

锁分离

锁分离指根据不同的应用场景将锁的功能进行分离，以应对不同的变化，最常见的锁分离思想就是读写锁（ReadWriteLock），它根据锁的功能将锁分离成读锁和写锁，这样读读不互斥，读写互斥，写写互斥，既保证了线程的安全性，又提高了性能。

操作分离思想可以进一步延伸为只要操作互不影响，就可以进一步拆分，比如LinkedBlockingQueue从头部取出数据，并从尾部加入数据。

锁粗化

锁粗化指为了保障性能，会要求尽可能将锁的操作细化以减少线程持有锁的时间，但是如果锁分得太细，将会导致系统频繁获取锁和释放锁，反而影响性能的提升。在这种情况下，建议将关联性强的锁操作集中起来处理，以提高系统整体的效率。

将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如上面实例：vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。

锁消除

在开发中经常会出现在不需要使用锁的情况下误用了锁操作而引起性能下降，这多数是因为程序编码不规范引起的。这时，我们需要检查并消除这些不必要的锁来提高系统的性能。

为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。

### 17.锁

在Java5 中，专门提供了锁对象，利用锁可以方便的实现资源的封锁，用来控制对竞争资源并发访问的控制，这些内容主要集中在java.util.concurrent.locks 包下面，里面有三个重要的接口Condition 、Lock 、ReadWriteLock 。 Condition: Condition 将 Object 监视器方法（ wait 、notify 和 notifyAll ）分解成截然不同的对象，以便通过将这些对象与任意 Lock 实现组合使用，为每个对象提供多个等待 set （ waitset）。 Lock: Lock 实现提供了比使用 synchronized 方法和语句可获得的更广泛的锁定操作。 ReadWriteLock： ReadWriteLock 维护了一对相关的锁定，一个用于只读操作，另一个用于写入操作。

Lock 接口以及对象，使用它，很优雅的控制了竞争资源的安全访问，但是这种锁不区分读写，称这种锁为普通锁。为了提高性能， Java 提供了读写锁，在读的地方使用读锁，在写的地方使用写锁，灵活控制，在一定程度上提高了程序的执行效率。

Java 中读写锁有个接口java.util.concurrent.locks.ReadWriteLock ，也有具体的实现ReentrantReadWriteLock 。

### 18.锁状态

一个对象有 4 种锁状态:无锁状态，偏向锁状态，轻量级锁状态，重量级锁状态。

无锁状态

无锁状态应该比较好理解，无锁就是没有锁，任何线程都可以尝试修改

偏向锁状态

Java 对象头

Java 的锁都是基于对象的，那是怎么告诉程序我是个锁呢？就不得不来说， Java 对象头 每个 Java 对象都有对 象头，如果是非数组类型，就用 2 个字宽来存储对象头，如果是数组，就用 3 个字宽来存储对象头。在 32 位处理器中，一个字宽是 32 位;在 64 位处理器中，字宽就是 64 位咯~对象头的内容就是下面这样:

| 长度      | 内容                   | 说明                           |
| --------- | ---------------------- | ------------------------------ |
| 32/64 bit | Mark Word              | 存储对象的 hashCode 或锁信息等 |
| 32/64 bit | Class Metadata Address | 存储到对象类型数据的指针       |
| 32/64 bit | Array length           | 数组的长度(如果是数组)         |

Mark Word 的内容:

| 锁状态   | 29 bit/61 bit              | 1 bit 是否是偏向锁         | 2 bit 锁标志位 |
| -------- | -------------------------- | -------------------------- | -------------- |
| 无锁     |                            | 0                          | 01             |
| 偏向锁   | 线程ID                     | 1                          | 01             |
| 轻量级锁 | 指向栈中锁记录的指针       | 此时这一位不用于标识偏向锁 | 00             |
| 重量级锁 | 指向互斥量(重量级锁)的指针 | 此时这一位不用于标识偏向锁 | 10             |

从上面表格中，应该能够看到，是偏向锁时， Mark Word 存储的是偏向锁的线程 ID ；是轻量级锁时， Mark Word 存储的是指向线 程栈中 Lock Record 的指针；是重量级锁时， Mark Word 存储的是指向堆中的 monitor 对象的指针

在大多数情况下,锁不仅不存在多线程竞争，而且总是由同一线程多次获得

基于此，就引入了偏向锁的概念 所以啥是偏向锁呢？用大白话说就是，我现在给锁设置一个变量，当一个线程请求的时候，发现这个锁是 true ，也就是说这个时 候没有所谓的资源竞争，那也不用走什么加锁/解锁的流程了，直接拿来用就行。但是如果这个锁是 false 的话，说明存在其他线 程竞争资源，那咱们再走正规的流程

具体的实现原理: 当一个线程第一次进入同步块时，会在对象头和栈帧中的锁记录中存储锁偏向的线程 ID 。当下次该线程进入这个同步块时，会检查 锁的 Mark Word 里面存放的是不是自己的线程 ID。如果是，说明线程已经获得了锁，那么这个线程在进入和退出同步块时，都不需 要花费 CAS 操作来加锁和解锁；如果不是，说明有另外一个线程来竞争这个偏向锁，这时就会尝试使用 CAS 来替换 Mark Word 里面 的线程 ID 为新线程的 ID 。此时会有两种情况: 替换成功，说明之前的线程不存在了，那么 Mark Word 里面的线程 ID 为新线程的 ID ，锁不会升级，此时仍然为偏向锁 替换失败，说明之前的线程仍然存在，那就暂停之前的线程，设置偏向锁标识为 0 ，并设置锁标志位为 00 ，升级为轻量级锁， 按照轻量级锁的方式进行竞争锁

撤销偏向锁

偏向锁使用了一种等到竞争出现时才释放锁的机制。也就说，如果没有人来和我竞争锁的时候，那么这个锁就是我独有的，当其他线 程尝试和我竞争偏向锁时，我会释放这个锁 在偏向锁向轻量级锁升级时，首先会暂停拥有偏向锁的线程，重置偏向锁标识，看起来这个过程挺简单的，但是开销是很大的，因为: 首先需要在一个安全点停止拥有锁的线程 然后遍历线程栈，如果存在锁记录的话，就需要修复锁记录和 Mark Word ，变成无锁状态 最后唤醒被停止的线程，把偏向锁升级成轻量级锁 你以为就是升级一个轻量级锁？ too young too simple 偏向锁向轻量级锁升级的过程中，是非常耗费资源的，如果应用程序中所有的锁通常都处于竞争状态，偏向锁此时就是一个累赘，此 时就可以通过 JVM 参数关闭偏向锁: -XX:-UseBiasedLocking=false ，那么程序默认会进入轻量级锁状态

轻量级锁

如果多个线程在不同时段获取同一把锁，也就是不存在锁竞争的情况，那么 JVM 就会使用轻量级锁来避免线程的阻塞与唤醒

轻量级锁加锁

JVM 会为每个线程在当前线程的栈帧中创建用于存储锁记录的空间，称之为 Displaced Mark Word 。如果一个线程获得锁的时候发现 是轻量级锁，就会将锁的 Mark Word 复制到自己的 Displaced Mark Word 中。之后线程会尝试用 CAS 将锁的 Mark Word 替换为指向 锁记录的指针。 如果替换成功，当前线程获得锁，那么整个状态还是 轻量级锁 状态 如果替换失败了呢？说明 Mark Word 被替换成了其他线程的锁记录，那就尝试使用自旋来获取锁.(自旋是说，线程不断地去尝试获取 锁，一般都是用循环来实现的) 自旋是耗费 CPU 的，如果一直获取不到锁，线程就会一直自旋， CPU 那么宝贵的资源就这么被白白浪费了 解决这个问题最简单的办法就是指定自旋的次数，比如如果没有替换成功，那就循环 10 次，还没有获取到，那就进入阻塞状态 但是 JDK 采用了一个更加巧妙的方法—适应性自旋。就是说，如果这次线程自旋成功了，那我下次自旋次数更多一些，因为我这次自 旋成功，说明我成功的概率还是挺大的，下次自旋次数就更多一些，那么如果自旋失败了，下次我自旋次数就减少一些，就比如，已 经看到了失败的前兆，那我就先溜，而不是非要“不撞南墙不回头” 自旋失败之后，线程就会阻塞，同时锁会升级成重量级锁

轻量级锁释放:

在释放锁时，当前线程会使用 CAS 操作将 Displaced Mark Word 中的内容复制到锁的 Mark Word 里面。如果没有发生竞争，这个复 制的操作就会成功;如果有其他线程因为自旋多次导致轻量级锁升级成了重量级锁， CAS 操作就会失败，此时会释放锁同时唤醒被阻 塞的过程

重量级锁

重量级锁依赖于操作系统的互斥量( mutex )来实现。但是操作系统中线程间状态的转换需要相对比较长的时间(因为操作系统需要从用 户态切换到内核态，这个切换成本很高),所以重量级锁效率很低，但是有一点就是，被阻塞的线程是不会消耗 CPU 的 每一个对象都可以当做一个锁，那么当多个线程同时请求某个对象锁时，它会怎么处理呢？ 对象锁会设置集中状态来区分请求的线程: Contention List:所有请求锁的线程将被首先放置到该竞争队列 Entry List: Contention List 中那些有资格成为候选人的线程被移到 Entry List 中 Wait Set:调用 wait 方法被阻塞的线程会被放置到 Wait Set 中 OnDeck:任何时刻最多只能有一个线程正在竞争锁，该线程称为 OnDeck Owner:获得锁的线程称为 Owner !Owner:释放锁的线程 当一个线程尝试获得锁时，如果这个锁被占用，就会把该线程封装成一个 ObjectWaiter 对象插入到 Contention List 队列的队首， 然后调用 park 函数挂起当前线程 当线程释放锁时，会从 Contention List 或者 Entry List 中挑选一个线程进行唤醒

如果线程在获得锁之后，调用了 Object.wait 方法，就会将该线程放入到 WaitSet 中，当被 Object.notify 唤醒后，会将线程从 WaitSet 移动到 Contention List 或者 Entry List 中。 但是，当调用一个锁对象的 wait 或 notify 方法时,如果当前锁的状态是偏向锁或轻量级锁，则会先膨胀成重量级锁

总结

synchronized 关键字是通过 monitorenter 和 monitorexit 两种指令来保证锁的。 当一个线程准备获取共享资源时:

- 首先检查 MarkWord 里面放的是不是自己的 ThreadID ，如果是，说明当前线程处于 “偏向锁”
- 如果不是，锁升级，这时使用 CAS 操作来执行切换，新的线程根据 MarkWord 里面现有的 ThreadID 来通知之前的线程暂停，将MarkWord 的内容置为空
- 然后，两个线程都将锁对象 HashCode 复制到自己新建的用于存储锁的记录空间中，接着开始通过 CAS 操作，把锁对象的MarkWord 的内容修改为自己新建的记录空间地址，以这种方式竞争 MarkWord ，成功执行 CAS 的线程获得资源，失败的则进入自旋
  - 自旋的线程在自旋过程中，如果成功获得资源（也就是之前获得资源的线程执行完毕，释放了共享资源），那么整个状态依然是 轻量级锁 的状态
  - 如果没有获得资源，就进入 重量级锁 的状态，此时，自旋的线程进行阻塞，等待之前线程执行完成并且唤醒自己

### 19.锁降级

其实在 HotSpot JVM 中，是支持锁降级的 锁降级发生在 Stop The World 期间，当 JVM 进入安全点的时候，会检查有没有闲置的锁，如果有就会尝试进行降级

### 20.**JVM对java的原生锁做了哪些优化?**

在Java6之前, Monitor的实现完全依赖底层操作系统的互斥锁来实现.

由于Java层面的线程与操作系统的原生线程有映射关系,如果要将一个线程进行阻塞或唤起都需要操作系统的协助,这就需要从用户态切换到内核态来执行,这种切换代价十分昂贵,很耗处理器时间,现代JDK中做了大量的优化。 一种优化是使用`自旋锁`,即在把线程进行阻塞操作之前先让线程自旋等待一段时间,可能在等待期间其他线程已经解锁,这时就无需再让线程执行阻塞操作,避免了用户态到内核态的切换。 现代JDK中还提供了三种不同的 Monitor实现,也就是三种不同的锁:

- 偏向锁(Biased Locking)
- 轻量级锁
- 重量级锁

这三种锁使得JDK得以优化 Synchronized的运行,当JVM检测到不同的竞争状况时,会自动切换到适合的锁实现,这就是锁的升级、降级。当没有竞争出现时,默认会使用偏向锁。 JVM会利用CAS操作,在对象头上的 Mark Word部分设置线程ID,以表示这个对象偏向于当前线程,所以并不涉及真正的互斥锁,因为在很多应用场景中,大部分对象生命周期中最多会被一个线程锁定,使用偏向锁可以降低无竞争开销。 如果有另一线程试图锁定某个被偏向过的对象,JVM就撤销偏向锁,切换到轻量级锁实现。 轻量级锁依赖CAS操作 Mark Word来试图获取锁,如果重试成功,就使用普通的轻量级锁否则,进一步升级为重量级锁。

### 21.**怎么检测一个线程是否拥有锁？**

调用 `Thread#holdsLock(Object obj)` **静态**方法，如果当且仅当当前线程拥有某个具体对象的锁会返回 `true` 。

### 22.锁消除

为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制。但是，在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。

锁消除的依据是**逃逸分析的数据支持**。变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用锁，但是我们在使用一些 JDK 的内置 API 时，如 StringBuffer、Vector、HashTable 等，这个时候会存在**隐性的加锁操作**。比如 StringBuffer 的 `#append(..)`方法，Vector 的 `add(...)` 方法。

### 23.锁粗化

我们知道在使用同步锁的时候，需要让同步块的作用范围尽可能小：仅在共享数据的实际作用域中才进行同步。这样做的目的，是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。

在大多数的情况下，上述观点是正确的，LZ 也一直坚持着这个观点。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗话化的概念。

锁粗化概念比较好理解，就是将**多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁**。

如上面实例：`vector` 每次 add 的时候都需要加锁操作，JVM 检测到对同一个对象（`vector`）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到 `for` 循环之外。

### 24.锁的升级

锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态。它们会随着竞争的激烈而逐渐升级。**注意，锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率**。

**重量级锁**

重量级锁通过对象内部的监视器（Monitor）实现。

其中，Monitor 的**本质**是，依赖于底层操作系统的 [Mutex Lock](http://dreamrunner.org/blog/2014/06/29/qian-tan-mutex-lock/) 实现。操作系统实现线程之间的切换，需要从用户态到内核态的切换，切换成本非常高。

**轻量级锁**

引入轻量级锁的主要目的，是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。

当关闭偏向锁功能或者多个线程竞争偏向锁，导致**偏向锁升级为轻量级锁**，则会尝试获取轻量级锁，其步骤如下：

**获取锁**

1. 判断当前对象是否处于无锁状态？**若是**，则 JVM 首先将在当前线程的栈帧中，建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的 Mark Word的 拷贝（官方把这份拷贝加了一个 Displaced 前缀，即 Displaced Mark Word）；**否则**，执行步骤（3）；
2. JVM 利用 CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record 的指正。如果**成功**，表示竞争到锁，则将锁标志位变成 `00`（表示此对象处于轻量级锁状态），执行同步操作；如果**失败**，则执行步骤（3）；
3. 判断当前对象的 Mark Word 是否指向当前线程的栈帧？如果**是**，则表示当前线程已经持有当前对象的锁，则直接执行同步代码块；**否则**，只能说明该锁对象已经被其他线程抢占了，当前线程便尝试使用**自旋**来获取锁。若自旋后没有获得锁，此时轻量级锁会升级为重量级锁，锁标志位变成 `10`，当前线程会被阻塞。

**释放锁**

轻量级锁的释放也是通过 CAS 操作来进行的，主要步骤如下：

1. 取出在获取轻量级锁保存在 Displaced Mark Word 中 数据。
2. 使用 CAS 操作将取出的数据替换当前对象的 Mark Word 中。如果**成功**，则说明释放锁成功；**否则**，执行（3）。
3. 如果 CAS 操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。

对于轻量级锁，其性能提升的依据是：“**对于绝大部分的锁，在整个生命周期内都是不会存在竞争的**”。如果打破这个依据则除了互斥的开销外，还有额外的 CAS 操作，因此在有多线程竞争的情况下，轻量级锁比重量级锁更慢。

### 25.偏向锁

引入偏向锁主要**目的**是：为了在无多线程竞争的情况下，尽量减少不必要的轻量级锁执行路径。

上面提到了轻量级锁的加锁解锁操作，是需要依赖多次 CAS 原子指令的。那么偏向锁是如何来减少不必要的 CAS 操作呢？我们可以查看 Mark Word 的数据结构就明白了。

> 我们可以看到**偏向锁**时，Mark Word 的数据结构为：线程 ID、Epoch( 偏向锁的时间戳 )、对象分带年龄、是否是偏向锁( `1` )、锁标识位( `01` )。

只需要检查是否为偏向锁、锁标识为以及 ThreadID 即可，处理流程如下：

**获取偏向锁**

1. 检测 Mark Word是 否为可偏向状态，即是否为偏向锁的标识位为 `1` ，锁标识位为 `01` 。
2. 若为可偏向状态，则测试线程 ID 是否为当前线程 ID ？如果**是**，则执行步骤（5）；**否则**，执行步骤（3）。
3. 如果线程 ID 不为当前线程 ID ，则通过 CAS 操作竞争锁。竞争**成功**，则将 Mark Word 的线程 ID 替换为**当前**线程 ID ，则执行步骤（5）；**否则**，执行线程（4）。
4. 通过 CAS 竞争锁失败，证明当前存在多线程竞争情况，当到达全局安全点，获得偏向锁的线程被挂起，**偏向锁升级为轻量级锁**，然后被阻塞在安全点的线程继续往下执行同步代码块。
5. 执行同步代码块

**撤销偏向锁**

**偏向锁的释放采用了一种只有竞争才会释放锁的机制**，线程是**不会主动**去释放偏向锁，需要等待其他线程来竞争。

偏向锁的撤销需要等待**全局安全点**（这个时间点是上没有正在执行的代码）。其步骤如下：

1. 暂停拥有偏向锁的线程，判断锁对象是否还处于被锁定状态。
2. 撤销偏向锁，恢复到无锁状态（ `01` ）或者轻量级锁的状态。

最后唤醒暂停的线程。

**关闭偏向锁**

偏向锁在 JDK 1.6 以上，默认开启。开启后程序启动几秒后才会被激活，可使用 JVM 参数 `-XX：BiasedLockingStartupDelay = 0` 来关闭延迟。

如果确定锁**通常处于竞争状态**，则可通过JVM参数 `-XX:-UseBiasedLocking=false` 关闭偏向锁，那么默认会进入轻量级锁。

- 优势：偏向锁只需要在置换 ThreadID 的时候依赖一次 CAS 原子指令，其余时刻不需要 CAS 指令(相比其他锁)。
- 隐患：由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的 CAS 原子指令的性能消耗（这个通常只能通过大量压测才可知）。
- 对比：轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能

### 26.**乐观锁一定就是好的吗？**

乐观锁避免了悲观锁独占对象的现象，同时也提高了并发性能，但它也有缺点： 1.乐观锁只能保证一个共享变量的原子操作。如果多一个或几个变量，乐观锁将变得力不从心， 但互斥锁能轻易解决,不管对象数量多少及对象颗粒度大小。 2.长时间自旋可能导致开销大。假如CAS长时间不成功而一直自旋,会给CPU带来很大的开销。 3.ABA问题。CAS的核心思想是通过比对内存值与预期值是否一样而判断内存值是否被改过，但这个判断逻辑不严谨,假如内存值原来是A，后来被一条线程改为B，最后又被改成了A，则CAS认为此内存值并没有发生改变，但实际上是有被其他线程改过的，这种情况对依赖过程值的情景的运算结果影响很大。 解决的思路是引入版本号,每次变量更新都把版本号加一。

### 27.可中断锁

可中断锁：顾名思义，就是可以相应中断的锁。

在Java中，synchronized就不是可中断锁，而Lock是可中断锁。

如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。

在前面演示lockInterruptibly()的用法时已经体现了Lock的可中断性。

### 28.什么是死锁

当两个线程锁住了当前资源，但都需要对方的资源才能进行下一步操作，这个时候两方就会一直等待对方的资源释放。这就形成了死锁。这些永远在互相等待的进程称为死锁进程。

死锁产生的条件：

1. 互斥:资源的锁是排他性的，加锁期间只能有一个线程拥有该资源。其他线程只能等待锁释放才能尝试获取该资源。
2. 请求和保持:当前线程已经拥有至少一个资源，但其同时又发出新的资源请求，而被请求的资源被其他线程拥有。此时进入保持当前资源并等待下个资源的状态。
3. 不剥夺：线程已拥有的资源，只能由自己释放，不能被其他线程剥夺。
4. 循环等待：是指有多个线程互相的请求对方的资源，但同时拥有对方下一步所需的资源。形成一种循环，类似2)请求和保持。但此处指多个线程的关系。并不是指单个线程一直在循环中等待。

### 29.发现排查死锁情况

我们利用jdk提供的工具定位死锁问题：

1. jps显示所有当前Java虚拟机进程名及pid.
2. jstack打印进程堆栈信息。

解决办法

1. （互斥）尽量少用互斥锁，能加读锁，不加写锁。当然这条无法避免。
2. （请求和保持）采用资源静态分配策略（进程资源静态分配方式是指一个进程在建立时就分配了它需要的全部资源）.我们尽量不让线程同时去请求多个锁，或者在拥有一个锁又请求不到下个锁时，不保持等待，先释放资源等待一段时间在重新请求。
3. （不剥夺）允许进程剥夺使用其他进程占有的资源。优先级。
4. （循环等待）尽量调整获得锁的顺序，不发生嵌套资源请求。加入超时。

### 7.如何基于独占锁安全的将线程放入池子中以及启动任务

如果基于加独占锁来安全的创建线程以及放入线程池里，启动这个新的线程

Worker就是工作线程，他是一个AQS的子类，本身自己就可以直接基于AQS来实现独占锁的机制，负责执行你提交的Runnable任务，是一个组件，里面保存了一个核心的state，代表了工作线程的状态

Worker内部是基于threadFactory（有一个默认的线程工厂），你也可以自己手动指定一个线程工厂，此时就会按照线程工厂的策略来创建一个线程，放在Worker的内部，代表了执行任务的工作线程

ThreadPoolExecutor，核心的数据结构，就是维护了一堆线程的线程池，如果要操作这个核心数据结构，就必须要在ThreadPoolExecutor的层面加一个独占锁，此时只能是一个线程尝试来操作核心数据结构

### 8.提交到线程池的任务是如何完成执行以及指标统计的

CAS递增线程数量 -> 创建Worker（theadFactory，AQS）-> 加入线程池（HashSet，独占锁） -> start线程 -> 执行提交的Runnable任务

Worker内部的thread线程，是通过threadFactory构造出来的，就是一个最最普通的thread，一旦启动之后，就会执行自己内部的Runnable业务逻辑，也就是我们传递进去的那个Runnable业务逻辑

只要thread start成功之后，这个提交任务到线程池的方法逻辑就结束掉了

Worker执行一个任务的时候，会通过自己的AQS机制更新一下自己的状态，相当于更新自己当前执行的线程是谁。state = 1，state = 0

### 9.核心线程满了之后如何将任务压入近乎无界的队列等待

比如说线程池里的线程数量已经是4了，再次提交第5个任务的时候，就会发现线程数量 = corePoolSize了，就不会再次创建新的线程了，入队等待的时候，走的是LinkedBLockingQueue的offer()方法

offer()方法实际上并不是阻塞的，非阻塞的

如果此时队列满，不是卡住等待人家take掉一个元素，而是直接返回false

使用的是近乎于无界的队列，所以基本上可以认为你offer入队的时候，永远是返回true，入队永远不会有满一说，入队都是成功的，可以无限的不停的入队

### 10.线程池中的工作线程如何从队列获取任务来执行

只要线程池的线程数量达到了corePoolSize之后，接下来任务都是直接入队的，无界队列，不停的入队，线程池里启动的线程是如何不停的从队列里获取任务来执行的呢

如果要允许线程的超时，两个条件满足一个就可以：配置项（allowCoreThreadTimedOut），当前线程数量超过了corePoolSize

allowCoreThreadTimedOut，默认是false，不允许core线程因为获取不到任务就超时退出；如果你设置为true的话，就会导致线程池中的core线程，如果超过一定的时间获取不到队列中的任务

就会认为是core线程空闲了一段时间，此时core线程就会允许退出，自己释放掉自己

当前线程数量是否超过了corePoolSize的数量，默认情况下是不会的，CAS保证在corePoolSize范围内的线程数量，但是cached线程池的时候，他是允许创建额外的非core线程，最多是达到maximumPoolSize的数量

超过了corePoolSize数量的额外的线程，都是允许timeout，也就是说，如果一定时间范围内没有从队列获取到任务，说明空闲了一段时间，此时就自动释放掉

fixed线程池中，暂时你可以认为，正常情况下是不会出现这个情况的，线程数量就是corePoolSize的数量

Runnable r = timed ?

workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :

workQueue.take();

fixed线程池的线程在从队列获取任务的时候，两个条件都不会满足，此时就是做的阻塞式的方法从LinkedBlockingQueue里获取任务

### 11.面试题：无界队列线程池在远程服务异常时内存飙升怎么办

最后可能会导致JVM OOM，系统崩溃

如果要解决这个问题从哪个角度来切入，你可以考虑自己定制线程池，使用有界队列，不要使用无界队列，可以限制内存空间的使用，避免系统崩溃；无界队列，直接支撑前端的请求，不允许有界阻塞，到最后大不了就是系统崩溃

远程服务异常的话，可能会导致另外一个问题，你的线程不停的异常报错和崩溃退出

如果你提交到线程池的任务报错了，抛了异常出来，在线程池执行的过程中，Worker层面接收到一个异常，会直接throw抛出去

只要有一个Worker异常挂掉，此时就会把这个worker从线程池里给挪出去，然后判断一下，如果当前线程数量 < corePoolSize，就会重新创建一个Worker和线程放入线程池中，自己再搞一个补位

### 12.线程池关闭的过程中会涉及到什么？

关闭这个线程池，在整个系统都关闭的时候会来关闭这个线程池

Worker本身是一个AQS，执行任务的时候，state = 1；执行完一个任务，state = 0

尝试将Worker的state = 1，如果成功了，就说明这个Worker当前的state = 0，说明这个Worker当前是空闲的状态，没有执行任何一个任务，此时就可以中断掉这个Worker，实际上来说Worker内部的线程就会退出

正在执行任务的Worker，tryLock失败了，state = 1，不要去中断他了，让他执行完这个任务再说

任务队列里还有一些任务，等待那些任务会被执行完毕，把队列里的任务都执行完毕，如果队列里的任务都执行完毕了之后，再次执行getTask()的时候会有问题

5.乐观锁和悲观锁的理解及如何实现，有哪些实现方式？

悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传

统的关系型数据库里边就用到了很多这种锁机制， 比如行锁，表锁等，读锁， 写锁等，都是在做操作之前先上锁。再比如Java 里面的同步原语synchronized 关

键字的实现也是悲观锁。

乐观锁：顾名思义， 就是很乐观， 每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数

据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition 机制，其实都是提供的乐观锁。在

Java中java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式CAS 实现的。

乐观锁的实现方式：

1、使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识， 不一致时可以采取丢弃和再次尝试的策略。

2、java 中的Compare and Swap 即CAS ，当多个线程尝试使用CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败， 失败的

线程并不会被挂起，而是被告知这次竞争中失败， 并可以再次尝试。CAS 操作中包含三个操作数—— 需要读写的内存位置（ V）、进行比较的预期原值（ A）

和拟写入的新值(B)。如果内存位置V 的值与预期原值A 相匹配，那么处理器会自动将该位置值更新为新值B。否则处理器不做任何操作。

CAS 缺点：

1、ABA 问题

2、循环时间长开销大：

对于资源竞争严重（线程冲突严重） 的情况， CAS 自旋的概率会比较大， 从而浪费更多的CPU 资源，效率低于synchronized。

3、只能保证一个共享变量的原子操作：

当对一个共享变量执行操作时，我们可以使用循环CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环CAS 就无法保证操作的原子性， 这个时候就可

以用锁。

### 23.几种锁

Java 6 为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁“。在Java 6 以前，所有的锁都是”重量级“锁。所以在Java 6 及其以后，一个对象其实有四种锁状态，它们级别由低到高依次是：

1. 无锁状态
2. 偏向锁状态
3. 轻量级锁状态
4. 重量级锁状态

无锁就是没有对资源进行锁定，任何线程都可以尝试去修改它，无锁在这里不再细讲。

几种锁会随着竞争情况逐渐升级，锁的升级很容易发生，但是锁降级发生的条件会比较苛刻，锁降级发生在Stop The World期间，当JVM进入安全点的时候，会检查是否有闲置的锁，然后进行降级。

Java对象头

ava的锁都是基于对象的。首先我们来看看一个对象的“锁”的信息是存放在什么地方的。

每个Java对象都有对象头。如果是非数组类型，则用2个字宽来存储对象头，如果是数组，则会用3个字宽来存储对象头。在32位处理器中，一个字宽是32位；在64位虚拟机中，一个字宽是64位。对象头的内容如下表：

| 长度     | 内容                   | 说明                         |
| -------- | ---------------------- | ---------------------------- |
| 32/64bit | Mark Word              | 存储对象的hashCode或锁信息等 |
| 32/64bit | Class Metadata Address | 存储到对象类型数据的指针     |
| 32/64bit | Array length           | 数组的长度（如果是数组）     |

我们主要来看看Mark Word的格式：

| 锁状态   | 29 bit 或 61 bit             | 1 bit 是否是偏向锁？       | 2 bit 锁标志位 |
| -------- | ---------------------------- | -------------------------- | -------------- |
| 无锁     |                              | 0                          | 01             |
| 偏向锁   | 线程ID                       | 1                          | 01             |
| 轻量级锁 | 指向栈中锁记录的指针         | 此时这一位不用于标识偏向锁 | 00             |
| 重量级锁 | 指向互斥量（重量级锁）的指针 | 此时这一位不用于标识偏向锁 | 10             |
| GC标记   |                              | 此时这一位不用于标识偏向锁 | 11             |

可以看到，当对象状态为偏向锁时，`Mark Word`存储的是偏向的线程ID；当状态为轻量级锁时，`Mark Word`存储的是指向线程栈中`Lock Record`的指针；当状态为重量级锁时，`Mark Word`为指向堆中的monitor对象的指针。

偏向锁

大多数情况下**锁不仅不存在多线程竞争，而且总是由同一线程多次获得**，于是引入了偏向锁。

偏向锁会偏向于第一个访问锁的线程，如果在接下来的运行过程中，该锁没有被其他的线程访问，则持有偏向锁的线程将永远不需要触发同步。也就是说，**偏向锁在资源无竞争情况下消除了同步语句，连CAS操作都不做了，提高了程序的运行性能。**

> 大白话就是对锁置个变量，如果发现为true，代表资源无竞争，则无需再走各种加锁/解锁流程。如果为false，代表存在其他线程竞争资源，那么就会走后面的流程。

**实现原理**

一个线程在第一次进入同步块时，会在对象头和栈帧中的锁记录里存储锁的偏向的线程ID。当下次该线程进入这个同步块时，会去检查锁的Mark Word里面是不是放的自己的线程ID。

如果是，表明该线程已经获得了锁，以后该线程在进入和退出同步块时不需要花费CAS操作来加锁和解锁 ；如果不是，就代表有另一个线程来竞争这个偏向锁。这个时候会尝试使用CAS来替换Mark Word里面的线程ID为新线程的ID，这个时候要分两种情况：

- 成功，表示之前的线程不存在了， Mark Word里面的线程ID为新线程的ID，锁不会升级，仍然为偏向锁；
- 失败，表示之前的线程仍然存在，那么暂停之前的线程，设置偏向锁标识为0，并设置锁标志位为00，升级为轻量级锁，会按照轻量级锁的方式进行竞争锁。

> CAS: Compare and Swap
>
> 比较并设置。用于在硬件层面上提供原子性操作。在 Intel 处理器中，比较并交换通过指令cmpxchg实现。 比较是否和给定的数值一致，如果一致则修改，不一致则不修改。

撤销偏向锁

偏向锁使用了一种**等到竞争出现才释放锁的机制**，所以当其他线程尝试竞争偏向锁时， 持有偏向锁的线程才会释放锁。

偏向锁升级成轻量级锁时，会暂停拥有偏向锁的线程，重置偏向锁标识，这个过程看起来容易，实则开销还是很大的，大概的过程如下：

1. 在一个安全点（在这个时间点上没有字节码正在执行）停止拥有锁的线程。
2. 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Mark Word，使其变成无锁状态。
3. 唤醒被停止的线程，将当前锁升级成轻量级锁。

所以，如果应用程序里所有的锁通常出于竞争状态，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭：

```html
-XX:UseBiasedLocking=false。
```

轻量级锁

多个线程在不同时段获取同一把锁，即不存在锁竞争的情况，也就没有线程阻塞。针对这种情况，JVM采用轻量级锁来避免线程的阻塞与唤醒。

轻量级锁的加锁

JVM会为每个线程在当前线程的栈帧中创建用于存储锁记录的空间，我们称为Displaced Mark Word。如果一个线程获得锁的时候发现是轻量级锁，会把锁的Mark Word复制到自己的Displaced Mark Word里面。

然后线程尝试用CAS将锁的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示Mark Word已经被替换成了其他线程的锁记录，说明在与其它线程竞争锁，当前线程就尝试使用自旋来获取锁。

> 自旋：不断尝试去获取锁，一般用循环来实现。

自旋是需要消耗CPU的，如果一直获取不到锁的话，那该线程就一直处在自旋状态，白白浪费CPU资源。解决这个问题最简单的办法就是指定自旋的次数，例如让其循环10次，如果还没获取到锁就进入阻塞状态。

但是JDK采用了更聪明的方式——适应性自旋，简单来说就是线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。

自旋也不是一直进行下去的，如果自旋到一定程度（和JVM、操作系统相关），依然没有获取到锁，称为自旋失败，那么这个线程会阻塞。同时这个锁就会**升级成重量级锁**。

**轻量级锁的释放：**

在释放锁时，当前线程会使用CAS操作将Displaced Mark Word的内容复制回锁的Mark Word里面。如果没有发生竞争，那么这个复制的操作会成功。如果有其他线程因为自旋多次导致轻量级锁升级成了重量级锁，那么CAS操作会失败，此时会释放锁并唤醒被阻塞的线程。

重量级锁

重量级锁依赖于操作系统的互斥量（mutex） 实现的，而操作系统中线程间状态的转换需要相对比较长的时间，所以重量级锁效率很低，但被阻塞的线程不会消耗CPU。

前面说到，每一个对象都可以当做一个锁，当多个线程同时请求某个对象锁时，对象锁会设置几种状态用来区分请求的线程：

```html
Contention List：所有请求锁的线程将被首先放置到该竞争队列
Entry List：Contention List中那些有资格成为候选人的线程被移到Entry List
Wait Set：那些调用wait方法被阻塞的线程被放置到Wait Set
OnDeck：任何时刻最多只能有一个线程正在竞争锁，该线程称为OnDeck
Owner：获得锁的线程称为Owner
!Owner：释放锁的线程
```

当一个线程尝试获得锁时，如果该锁已经被占用，则会将该线程封装成一个`ObjectWaiter`对象插入到Contention List的队列的队首，然后调用`park`函数挂起当前线程。

当线程释放锁时，会从Contention List或EntryList中挑选一个线程唤醒，被选中的线程叫做`Heir presumptive`即假定继承人，假定继承人被唤醒后会尝试获得锁，但`synchronized`是非公平的，所以假定继承人不一定能获得锁。这是因为对于重量级锁，线程先自旋尝试获得锁，这样做的目的是为了减少执行操作系统同步操作带来的开销。如果自旋不成功再进入等待队列。这对那些已经在等待队列中的线程来说，稍微显得不公平，还有一个不公平的地方是自旋线程可能会抢占了Ready线程的锁。

果线程获得锁后调用`Object.wait`方法，则会将线程加入到WaitSet中，当被`Object.notify`唤醒后，会将线程从WaitSet移动到Contention List或EntryList中去。需要注意的是，当调用一个锁对象的`wait`或`notify`方法时，**如当前锁的状态是偏向锁或轻量级锁则会先膨胀成重量级锁**。

总结锁的升级流程

每一个线程在准备获取共享资源时： 第一步，检查MarkWord里面是不是放的自己的ThreadId ,如果是，表示当前线程是处于 “偏向锁” 。

第二步，如果MarkWord不是自己的ThreadId，锁升级，这时候，用CAS来执行切换，新的线程根据MarkWord里面现有的ThreadId，通知之前线程暂停，之前线程将Markword的内容置为空。

第三步，两个线程都把锁对象的HashCode复制到自己新建的用于存储锁的记录空间，接着开始通过CAS操作， 把锁对象的MarKword的内容修改为自己新建的记录空间的地址的方式竞争MarkWord。

第四步，第三步中成功执行CAS的获得资源，失败的则进入自旋 。

第五步，自旋的线程在自旋过程中，成功获得资源(即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于 轻量级锁的状态，如果自旋失败 。

第六步，进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己。

各种锁的优缺点对比

| 锁       | 优点                                                         | 缺点                                             | 适用场景                             |
| -------- | ------------------------------------------------------------ | ------------------------------------------------ | ------------------------------------ |
| 偏向锁   | 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 | 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 | 适用于只有一个线程访问同步块场景。   |
| 轻量级锁 | 竞争的线程不会阻塞，提高了程序的响应速度。                   | 如果始终得不到锁竞争的线程使用自旋会消耗CPU。    | 追求响应时间。同步块执行速度非常快。 |
| 重量级锁 | 线程竞争不使用自旋，不会消耗CPU。                            | 线程阻塞，响应时间缓慢。                         | 追求吞吐量。同步块执行速度较长。     |

### 24.乐观锁与悲观锁的概念

锁可以从不同的角度分类。其中，乐观锁和悲观锁是一种分类方式。

**悲观锁：**

悲观锁就是我们常说的锁。对于悲观锁来说，它总是认为每次访问共享资源时会发生冲突，所以必须对每次数据操作加上锁，以保证临界区的程序同一时间只能有一个线程在执行。

**乐观锁：**

乐观锁又称为“无锁”，顾名思义，它是乐观派。乐观锁总是假设对共享资源的访问没有冲突，线程可以不停地执行，无需加锁也无需等待。而一旦多个线程发生冲突，乐观锁通常是使用一种称为CAS的技术来保证线程执行的安全性。

由于无锁操作中没有锁的存在，因此不可能出现死锁的情况，也就是说**乐观锁天生免疫死锁**。

乐观锁多用于“读多写少“的环境，避免频繁加锁影响性能；而悲观锁多用于”写多读少“的环境，避免频繁失败和重试影响性能。

### 37.JDK中有关锁的一些接口和类

JDK中关于并发的类大多都在`java.util.concurrent`（以下简称juc）包下。而juc.locks包看名字就知道，是提供了一些并发锁的工具类的。

抽象类AQS/AQLS/AOS

首先我们看**AQS**（AbstractQueuedSynchronizer），它是在JDK 1.5 发布的，提供了一个“队列同步器”的基本功能实现。而AQS里面的“资源”是用一个`int`类型的数据来表示的，有时候我们的业务需求资源的数量超出了`int`的范围，所以在JDK 1.6 中，多了一个**AQLS**（AbstractQueuedLongSynchronizer）。它的代码跟AQS几乎一样，只是把资源的类型变成了`long`类型。

AQS和AQLS都继承了一个类叫**AOS**（AbstractOwnableSynchronizer）。这个类也是在JDK 1.6 中出现的。这个类只有几行简单的代码。从源码类上的注释可以知道，它是用于表示锁与持有者之间的关系（独占模式）。可以看一下它的主要方法：

```html
// 独占模式，锁的持有者  
private transient Thread exclusiveOwnerThread;  
// 设置锁持有者  
protected final void setExclusiveOwnerThread(Thread t) {      
    exclusiveOwnerThread = t;  
}  
// 获取锁的持有线程  
protected final Thread getExclusiveOwnerThread() {      
    return exclusiveOwnerThread;  
}
```

接口Condition/Lock/ReadWriteLock

juc.locks包下共有三个接口：`Condition`、`Lock`、`ReadWriteLock`。其中，Lock和ReadWriteLock从名字就可以看得出来，分别是锁和读写锁的意思。Lock接口里面有一些获取锁和释放锁的方法声明，而ReadWriteLock里面只有两个方法，分别返回“读锁”和“写锁”：

```html
public interface ReadWriteLock {
    Lock readLock();
    Lock writeLock();
}
```

Lock接口中有一个方法是可以获得一个`Condition`:

```html
Condition newCondition();
```

之前我们提到了每个对象都可以用继承自`Object`的**wait/notify**方法来实现**等待/通知机制**。而Condition接口也提供了类似Object监视器的方法，通过与**Lock**配合来实现等待/通知模式。

| 对比项                                         | Object监视器                  | Condition                                                   |
| ---------------------------------------------- | ----------------------------- | ----------------------------------------------------------- |
| 前置条件                                       | 获取对象的锁                  | 调用Lock.lock获取锁，调用Lock.newCondition获取Condition对象 |
| 调用方式                                       | 直接调用，比如object.notify() | 直接调用，比如condition.await()                             |
| 等待队列的个数                                 | 一个                          | 多个                                                        |
| 当前线程释放锁进入等待状态                     | 支持                          | 支持                                                        |
| 当前线程释放锁进入等待状态，在等待状态中不中断 | 不支持                        | 支持                                                        |
| 当前线程释放锁并进入超时等待状态               | 支持                          | 支持                                                        |
| 当前线程释放锁并进入等待状态直到将来的某个时间 | 不支持                        | 支持                                                        |
| 唤醒等待队列中的一个线程                       | 支持                          | 支持                                                        |
| 唤醒等待队列中的全部线程                       | 支持                          | 支持                                                        |

Condition和Object的wait/notify基本相似。其中，Condition的await方法对应的是Object的wait方法，而Condition的**signal/signalAll**方法则对应Object的notify/notifyAll()。但Condition类似于Object的等待/通知机制的加强版。我们来看看主要的方法：

| 方法名称               | 描述                                                         |
| ---------------------- | ------------------------------------------------------------ |
| await()                | 当前线程进入等待状态直到被通知（signal）或者中断；当前线程进入运行状态并从await()方法返回的场景包括：（1）其他线程调用相同Condition对象的signal/signalAll方法，并且当前线程被唤醒；（2）其他线程调用interrupt方法中断当前线程； |
| awaitUninterruptibly() | 当前线程进入等待状态直到被通知，在此过程中对中断信号不敏感，不支持中断当前线程 |
| awaitNanos(long)       | 当前线程进入等待状态，直到被通知、中断或者超时。如果返回值小于等于0，可以认定就是超时了 |
| awaitUntil(Date)       | 当前线程进入等待状态，直到被通知、中断或者超时。如果没到指定时间被通知，则返回true，否则返回false |
| signal()               | 唤醒一个等待在Condition上的线程，被唤醒的线程在方法返回前必须获得与Condition对象关联的锁 |
| signalAll()            | 唤醒所有等待在Condition上的线程，能够从await()等方法返回的线程必须先获得与Condition对象关联的锁 |

ReentrantLock

ReentrantLock是一个非抽象类，它是Lock接口的JDK默认实现，实现了锁的基本功能。从名字上看，它是一个”可重入“锁，从源码上看，它内部有一个抽象类`Sync`，是继承了AQS，自己实现的一个同步器。同时，ReentrantLock内部有两个非抽象类`NonfairSync`和`FairSync`，它们都继承了Sync。从名字上看得出，分别是”非公平同步器“和”公平同步器“的意思。这意味着ReentrantLock可以支持”公平锁“和”非公平锁“。

通过看着两个同步器的源码可以发现，它们的实现都是”独占“的。都调用了AOS的`setExclusiveOwnerThread`方法，所以ReentrantLock的锁的”独占“的，也就是说，它的锁都是”排他锁“，不能共享。

在ReentrantLock的构造方法里，可以传入一个`boolean`类型的参数，来指定它是否是一个公平锁，默认情况下是非公平的。这个参数一旦实例化后就不能修改，只能通过`isFair()`方法来查看。

ReentrantReadWriteLock

这个类也是一个非抽象类，它是ReadWriteLock接口的JDK默认实现。它与ReentrantLock的功能类似，同样是可重入的，支持非公平锁和公平锁。不同的是，它还支持”读写锁“。

可以看到，它同样是内部维护了两个同步器。且维护了两个Lock的实现类ReadLock和WriteLock。从源码可以发现，这两个内部类用的是外部类的同步器。

ReentrantReadWriteLock实现了读写锁，但它有一个小弊端，就是在“写”操作的时候，其它线程不能写也不能读。我们称这种现象为“写饥饿”。

StampedLock

`StampedLock`类是在Java 8 才发布的，也是Doug Lea大神所写，有人号称它为锁的性能之王。它没有实现Lock接口和ReadWriteLock接口，但它其实是实现了“读写锁”的功能，并且性能比ReentrantReadWriteLock更高。StampedLock还把读锁分为了“乐观读锁”和“悲观读锁”两种。

前面提到了ReentrantReadWriteLock会发生“写饥饿”的现象，但StampedLock不会。它是怎么做到的呢？它的核心思想在于，**在读的时候如果发生了写，应该通过重试的方式来获取新的值，而不应该阻塞写操作。这种模式也就是典型的无锁编程思想，和CAS自旋的思想一样**。这种操作方式决定了StampedLock在读线程非常多而写线程非常少的场景下非常适用，同时还避免了写饥饿情况的发生。

可以看到，StampedLock获取锁会返回一个`long`类型的变量，释放锁的时候再把这个变量传进去。

StampedLock用这个long类型的变量的前7位（LG_READERS）来表示读锁，每获取一个悲观读锁，就加1（RUNIT），每释放一个悲观读锁，就减1。而悲观读锁最多只能装128个（7位限制），很容易溢出，所以用一个int类型的变量来存储溢出的悲观读锁。

写锁用state变量剩下的位来表示，每次获取一个写锁，就加0000 1000 0000（WBIT）。需要注意的是，**写锁在释放的时候，并不是减WBIT，而是再加WBIT**。这是为了**让每次写锁都留下痕迹**，解决CAS中的ABA问题，也为**乐观锁检查变化**validate方法提供基础。

乐观读锁就比较简单了，并没有真正改变state的值，而是在获取锁的时候记录state的写状态，在操作完成后去检查state的写状态部分是否发生变化，上文提到了，每次写锁都会留下痕迹，也是为了这里乐观锁检查变化提供方便。

总的来说，StampedLock的性能是非常优异的，基本上可以取代ReentrantReadWriteLock的作用。

### 38.并发集合容器

同步容器与并发容器

我们知道在java.util包下提供了一些容器类，而Vector和HashTable是线程安全的容器类，但是这些容器实现同步的方式是通过对方法加锁(sychronized)方式实现的，这样读写均需要锁操作，导致性能低下。

而即使是Vector这样线程安全的类，在面对多线程下的复合操作的时候也是需要通过客户端加锁的方式保证原子性。

并发容器是Java 5 提供的在多线程编程下用于代替同步容器，针对不同的应用场景进行设计，提高容器的并发访问性，同时定义了线程安全的复合操作。

并发容器类介绍

并发Map

ConcurrentMap接口

ConcurrentMap接口继承了Map接口，在Map接口的基础上又定义了四个方法：

**putIfAbsent：**与原有put方法不同的是，putIfAbsent方法中如果插入的key相同，则不替换原有的value值；

**remove：**与原有remove方法不同的是，新remove方法中增加了对value的判断，如果要删除的key-value不能与Map中原有的key-value对应上，则不会删除该元素;

**replace(K,V,V)：**增加了对value值的判断，如果key-oldValue能与Map中原有的key-value对应上，才进行替换操作；

**replace(K,V)：**与上面的replace不同的是，此replace不会对Map中原有的key-value进行比较，如果key存在则直接替换；

ConcurrentHashMap类

ConcurrentHashMap同HashMap一样也是基于散列表的map，但是它提供了一种与HashTable完全不同的加锁策略提供更高效的并发性和伸缩性。

ConcurrentHashMap提供了一种粒度更细的加锁机制来实现在多线程下更高的性能，这种机制叫分段锁(Lock Striping)。

提供的优点是：在并发环境下将实现更高的吞吐量，而在单线程环境下只损失非常小的性能。

可以这样理解分段锁，就是**将数据分段，对每一段数据分配一把锁**。当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。

有些方法需要跨段，比如size()、isEmpty()、containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁。

ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，HashEntry则用于存储键值对数据。

一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构（同HashMap一样，它也会在长度达到8的时候转化为红黑树）的元素， 每个Segment守护者一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。

ConcurrentNavigableMap接口与ConcurrentSkipListMap类

ConcurrentNavigableMap接口继承了NavigableMap接口，这个接口提供了针对给定搜索目标返回最接近匹配项的导航方法。

ConcurrentNavigableMap接口的主要实现类是ConcurrentSkipListMap类。从名字上来看，它的底层使用的是跳表（SkipList）的数据结构。关于跳表的数据结构这里不做太多介绍，它是一种”空间换时间“的数据结构，可以使用CAS来保证并发安全性。

并发Queue

JDK并没有提供线程安全的List类，因为对List来说，**很难去开发一个通用并且没有并发瓶颈的线程安全的List**。因为即使简单的读操作，拿contains() 这样一个操作来说，很难搜索的时候如何避免锁住整个list。

所以退一步，JDK提供了对队列和双端队列的线程安全的类：ConcurrentLinkedDeque和ConcurrentLinkedQueue。因为队列相对于List来说，有更多的限制。这两个类是使用CAS来实现线程安全的。

并发Set

JDK提供了ConcurrentSkipListSet，是线程安全的有序的集合。底层是使用ConcurrentSkipListMap实现。

谷歌的guava框架实现了一个线程安全的ConcurrentHashSet：

```html
Set<String> s = Sets.newConcurrentHashSet();
```

### 39.CopyOnWrite

什么是CopyOnWrite容器

在说到CopyOnWrite容器之前我们先来谈谈什么是CopyOnWrite机制，CopyOnWrite是计算机设计领域中的一种优化策略，也是一种在并发场景下常用的设计思想——写入时复制思想。

那什么是写入时复制思想呢？就是当有多个调用者同时去请求一个资源数据的时候，有一个调用者出于某些原因需要对当前的数据源进行修改，这个时候系统将会复制一个当前数据源的副本给调用者修改。

CopyOnWrite容器即**写时复制的容器**,当我们往一个容器中添加元素的时候，不直接往容器中添加，而是将当前容器进行copy，复制出来一个新的容器，然后向新容器中添加我们需要的元素，最后将原容器的引用指向新容器。

这样做的好处在于，我们可以在并发的场景下对容器进行"读操作"而不需要"加锁"，从而达到读写分离的目的。从JDK 1.5 开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器 ，分别是CopyOnWriteArrayList和CopyOnWriteArraySet 。

CopyOnWriteArrayList

**优点**： CopyOnWriteArrayList经常被用于“读多写少”的并发场景，是因为CopyOnWriteArrayList无需任何同步措施，大大增强了读的性能。在Java中遍历线程非安全的List(如：ArrayList和 LinkedList)的时候，若中途有别的线程对List容器进行修改，那么会抛出ConcurrentModificationException异常。CopyOnWriteArrayList由于其"读写分离"，遍历和修改操作分别作用在不同的List容器，所以在使用迭代器遍历的时候，则不会抛出异常。

**缺点**： 第一个缺点是CopyOnWriteArrayList每次执行写操作都会将原容器进行拷贝了一份，数据量大的时候，内存会存在较大的压力，可能会引起频繁Full GC（ZGC因为没有使用Full GC）。比如这些对象占用的内存比较大200M左右，那么再写入100M数据进去，内存就会多占用300M。

第二个缺点是CopyOnWriteArrayList由于实现的原因，写和读分别作用在不同新老容器上，在写操作执行过程中，读不会阻塞，但读取到的却是老容器的数据。

现在我们来看一下CopyOnWriteArrayList的add操作源码，它的逻辑很清晰，就是先把原容器进行copy，然后在新的副本上进行“写操作”，最后再切换引用，在此过程中是加了锁的。

remove的逻辑是将要remove元素之外的其他元素拷贝到新的副本中，然后切换引用，再将原容器的引用指向新的副本中，因为remove操作也是“写操作”所以也是要加锁的。

CopyOnWrite容器有**数据一致性**的问题，它只能保证**最终数据一致性**。

所以如果我们希望写入的数据马上能准确地读取，请不要使用CopyOnWrite容器。

### 40.通信工具类

JDK中提供了一些工具类以供开发者使用。这样的话我们在遇到一些常见的应用场景时就可以使用这些工具类，而不用自己再重复造轮子了。它们都在java.util.concurrent包下。

| 类             | 作用                                       |
| -------------- | ------------------------------------------ |
| Semaphore      | 限制线程的数量                             |
| Exchanger      | 两个线程交换数据                           |
| CountDownLatch | 线程等待直到计数器减为0时开始工作          |
| CyclicBarrier  | 作用跟CountDownLatch类似，但是可以重复使用 |
| Phaser         | 增强的CyclicBarrier                        |

Semaphore

Semaphore介绍

Semaphore翻译过来是信号的意思。顾名思义，这个工具类提供的功能就是多个线程彼此“打信号”。而这个“信号”是一个`int`类型的数据，也可以看成是一种“资源”。

可以在构造函数中传入初始资源总数，以及是否使用“公平”的同步器。默认情况下，是非公平的。

最主要的方法是acquire方法和release方法。acquire()方法会申请一个permit，而release方法会释放一个permit。当然，你也可以申请多个acquire(int permits)或者释放多个release(int permits)。

每次acquire，permits就会减少一个或者多个。如果减少到了0，再有其他线程来acquire，那就要阻塞这个线程直到有其它线程release permit为止。

Semaphore往往用于资源有限的场景中，去限制线程的数量。

Semaphore原理

Semaphore内部有一个继承了AQS的同步器Sync，重写了`tryAcquireShared`方法。在这个方法里，会去尝试获取资源。

如果获取失败（想要的资源数量小于目前已有的资源数量），就会返回一个负数（代表尝试获取资源失败）。然后当前线程就会进入AQS的等待队列。

Exchanger

Exchanger类用于两个线程交换数据。它支持泛型，也就是说你可以在两个线程之间传送任何数据。

当一个线程调用exchange方法后，它是处于阻塞状态的，只有当另一个线程也调用了exchange方法，它才会继续向下执行。看源码可以发现它是使用park/unpark来实现等待状态的切换的，但是在使用park/unpark方法之前，使用了CAS检查，估计是为了提高性能。

Exchanger一般用于两个线程之间更方便地在内存中交换数据，因为其支持泛型，所以我们可以传输任何的数据，比如IO流或者IO缓存。根据JDK里面的注释的说法，可以总结为一下特性：

- 此类提供对外的操作是同步的；
- 用于成对出现的线程之间交换数据；
- 可以视作双向的同步队列；
- 可应用于基因算法、流水线设计等场景。

Exchanger类还有一个有超时参数的方法，如果在指定时间内没有另一个线程调用exchange，就会抛出一个超时异常。

```html
public V exchange(V x, long timeout, TimeUnit unit)
```

那么问题来了，Exchanger只能是两个线程交换数据吗？那三个调用同一个实例的exchange方法会发生什么呢？答案是只有前两个线程会交换数据，第三个线程会进入阻塞状态。

需要注意的是，exchange是可以重复使用的。也就是说。两个线程可以使用Exchanger在内存中不断地再交换数据。

CountDownLatch

CountDownLatch介绍

先来解读一下CountDownLatch这个类名字的意义。CountDown代表计数递减，Latch是“门闩”的意思。也有人把它称为“屏障”。而CountDownLatch这个类的作用也很贴合这个名字的意义，假设某个线程在执行任务之前，需要等待其它线程完成一些前置任务，必须等所有的前置任务都完成，才能开始执行本线程的任务。

CountDownLatch原理

其实CountDownLatch类的原理挺简单的，内部同样是一个基层了AQS的实现类Sync，且实现起来还很简单，可能是JDK里面AQS的子类中最简单的实现了，有兴趣的读者可以去看看这个内部类的源码。

需要注意的是构造器中的**计数值（count）实际上就是闭锁需要等待的线程数量**。这个值只能被设置一次，而且CountDownLatch**没有提供任何机制去重新设置这个计数值**。

CyclicBarrier

CyclicBarrier介绍

CyclicBarrirer从名字上来理解是“循环的屏障”的意思。前面提到了CountDownLatch一旦计数值`count`被降为0后，就不能再重新设置了，它只能起一次“屏障”的作用。而CyclicBarrier拥有CountDownLatch的所有功能，还可以使用`reset()`方法重置屏障。

CyclicBarrier Barrier被破坏

如果在参与者（线程）在等待的过程中，Barrier被破坏，就会抛出BrokenBarrierException。可以用`isBroken()`方法检测Barrier是否被破坏。

1. 如果有线程已经处于等待状态，调用reset方法会导致已经在等待的线程出现BrokenBarrierException异常。并且由于出现了BrokenBarrierException，将会导致始终无法等待。
2. 如果在等待的过程中，线程被中断，也会抛出BrokenBarrierException异常，并且这个异常会传播到其他所有的线程。
3. 如果在执行屏障操作过程中发生异常，则该异常将传播到当前线程中，其他线程会抛出BrokenBarrierException，屏障被损坏。
4. 如果超出指定的等待时间，当前线程会抛出 TimeoutException 异常，其他线程会抛出BrokenBarrierException异常。

CyclicBarrier原理

CyclicBarrier虽说功能与CountDownLatch类似，但是实现原理却完全不同，CyclicBarrier内部使用的是Lock + Condition实现的等待/通知模式。详情可以查看这个方法的源码：

```html
private int dowait(boolean timed, long nanos)
```

Phaser

Phaser介绍

Phaser这个单词是“移相器，相位器”的意思（好吧，笔者并不懂这是什么玩意，下方资料来自百度百科）。这个类是从JDK 1.7 中出现的。

> 移相器（Phaser）能够对波的相位进行调整的一种装置。任何传输介质对在其中传导的波动都会引入相移，这是早期模拟移相器的原理；现代电子技术发展后利用A/D、D/A转换实现了数字移相，顾名思义，它是一种不连续的移相技术，但特点是移相精度高。 移相器在雷达、导弹姿态控制、加速器、通信、仪器仪表甚至于音乐等领域都有着广泛的应用

CyclicBarrier，可以发现它在构造方法里传入“任务总量”`parties`之后，就不能修改这个值了，并且每次调用`await()`方法也只能消耗一个`parties`计数。但Phaser可以动态地调整任务总量！

名词解释：

- party：对应一个线程，数量可以通过register或者构造参数传入;
- arrive：对应一个party的状态，初始时是unarrived，当调用`arriveAndAwaitAdvance()`或者 `arriveAndDeregister()`进入arrive状态，可以通过`getUnarrivedParties()`获取当前未到达的数量;
- register：注册一个party，每一阶段必须所有注册的party都到达才能进入下一阶段;
- deRegister：减少一个party。
- phase：阶段，当所有注册的party都arrive之后，将会调用Phaser的`onAdvance()`方法来判断是否要进入下一阶段。

Phaser终止的两种途径，Phaser维护的线程执行完毕或者`onAdvance()`返回`true` 此外Phaser还能维护一个树状的层级关系，构造的时候new Phaser(parentPhaser)，对于Task执行时间短的场景（竞争激烈），也就是说有大量的**party**, 那可以把每个Phaser的任务量设置较小，多个Phaser共同继承一个父Phaser。

另外Phaser类用来控制某个阶段的线程数量很有用，但它并在意这个阶段具体有哪些线程arrive，只要达到它当前阶段的parties值，就触发屏障。所以我这里的案例虽然制定了特定的线程（加载新手教程）来更直观地表述Phaser的功能，但是其实Phaser是没有分辨具体是哪个线程的功能的，它在意的只是数量

Phaser原理

Phaser类的原理相比起来要复杂得多。它内部使用了两个基于Fork-Join框架的原子类辅助.

## synchronized

### 1.synchronized同步锁

synchronized关键字用于为Java对象、方法、代码块提供线程安全的操作。synchronized它可以把任意一个非NULL的对象当作锁。synchronized属于独占式的悲观锁，同时属于可重入锁。在使用synchronized修饰对象时，同一时刻只能有一个线程对该对象进行访问；在synchronized修饰方法、代码块时，同一时刻只能有一个线程执行该方法体或代码块，其他线程只有等待当前线程执行完毕并释放锁资源后才能访问该对象或执行同步代码块。

Java中的每个对象都有个monitor对象，加锁就是在竞争monitor对象。对代码块加锁是通过在前后分别加上monitorenter和monitorexit指令实现的，对方法是否加锁是通过一个标记位来判断的。

synchronized的作用范围

- synchronized作用于成员变量和非静态方法时，锁住的是对象的实例，即this对象。
- synchronized作用于静态方法时，锁住的是Class实例，因为静态方法属于Class而不属于对象；
- synchronized作用于一个代码块时，锁住的是所有代码块中配置的对象。

### 2.synchronized的实现原理

在synchronized内部包括ContentionList、EntryList、WaitSet、OnDeck、Owner、!Owner这6个区域，每个区域的数据都代表锁的不同状态。

- ContentionList：锁竞争队列，所有请求锁的线程都被放在竞争队列中。
- EntryList：竞争候选列表，在Contention List中有资格成为候选者来竞争锁资源的线程被移动到了Entry List中。
- WaitSet：等待集合，调用wait方法后被阻塞的线程将被放在WaitSet中。
- OnDeck：竞争候选者，在同一时刻最多只有一个线程在竞争锁资源，该线程的状态被称为OnDeck。
- Owner：竞争到锁资源的线程被称为Owner状态线程。
- !Owner：在Owner线程释放锁后，会从Owner的状态变成!Owner。

synchronized在收到新的锁请求时首先自旋，如果通过自旋也没有获取锁资源，则将被放入锁竞争队列ContentionList中。 为了防止锁竞争时ContentionList尾部的元素被大量的并发线程进行CAS 访问而影响性能， Owner 线程会在释放锁资源时将ContentionList中的部分线程移动到EntryList中，并指定EntryList中的某个线程（一般是最先进入的线程）为OnDeck线程。Owner线程并没有直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，让OnDeck线程重新竞争锁。在Java中把该行为称为“竞争切换”，该行为牺牲了公平性，但提高了性能。 获取到锁资源的OnDeck线程会变为Owner线程，而未获取到锁资源的线程仍然停留在EntryList中。 Owner线程在被wait方法阻塞后，会被转移到WaitSet队列中，直到某个时刻被notify 方法或者notifyAll 方法唤醒， 会再次进入EntryList中。ContentionList、EntryList、WaitSet中的线程均为阻塞状态， 该阻塞是由操作系统来完成的（ 在Linux 内核下是采用pthread_mutex_lock内核函数实现的）。 Owner线程在执行完毕后会释放锁的资源并变为!Owner状态。

在synchronized中，在线程进入ContentionList之前，等待的线程会先尝试以自旋的方式获取锁， 如果获取不到就进入ContentionList，该做法对于已经进入队列的线程是不公平的，因此synchronized是非公平锁。另外，自旋获取锁的线程也可以直接抢占OnDeck线程的锁资源。 synchronized是一个重量级操作，需要调用操作系统的相关接口，性能较低，给线程加锁的时间有可能超过获取锁后具体逻辑代码的操作时间。 JDK 1.6对synchronized做了很多优化，引入了适应自旋、锁消除、锁粗化、轻量级锁及偏向锁等以提高锁的效率。锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。这种升级过程叫作锁膨胀。在JDK 1.6 中默认开启了偏向锁和轻量级锁， 可以通过-XX:UseBiasedLocking禁用偏向锁。

### 3.多线程中 synchronized 锁升级的原理是什么？

synchronized 锁升级原理：在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断threadid 是否与其线程 id 一致。 如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。 锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。

### 4.synchronized 底层实现原理？

Java中每一个对象都可以作为锁，这是synchronized实现同步的基础：

1. 普通同步方法，锁是当前实例对象
2. 静态同步方法，锁是当前类的class对象
3. 同步方法块，锁是括号里面的对象

同步代码块是使用monitorenter和monitorexit指令实现的，同步方法（在这看不出来需要看JVM底层实现）依靠的是方法修饰符上的ACC_SYNCHRONIZED实现。

- 同步代码块：monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁；
- 同步方法：synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Klass做为锁对象。

### 5.synchronized 和 Lock 有什么区别？

synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。 synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。

### 6.Java对象头、monitor

Java对象头和monitor是实现synchronized的基础！

Java对象头

synchronized用的锁是存在Java对象头里的，那么什么是Java对象头呢？Hotspot虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键

Mark Word

Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。

Monitor

什么是Monitor？我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。

Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。

### 7.synchronized

到当程序声明 synchronized 代码块时，编译成的字节码会包含 monitorenter 和 monitorexit 指令，这两种指令会消 耗操作数栈上的一个引用类型的元素(也就是 synchronized 关键字括号里面的引用)，作为所要加锁解锁的锁对象。如果看的比较仔细 的话，上面有一个 monitorenter 指令和两个 monitorexit 指令，这是 Java 虚拟机为了确保获得的锁不管是在正常执行路径，还 是在异常执行路径上都能够解锁。 关于 monitorenter 和 monitorexit ，可以理解为每个锁对象拥有一个锁计数器和一个指向持有该锁的线程指针: 当程序执行 monitorenter 时，如果目标锁对象的计数器为 0 ，说明这个时候它没有被其他线程所占有，此时如果有线程来请求使 用， Java 虚拟机就会分配给该线程，并且把计数器的值加 1 目标锁对象计数器不为 0 时，如果锁对象持有的线程是当前线程， Java 虚拟机可以将其计数器加 1 ，如果不是呢？那很抱 歉，就只能等待，等待持有线程释放掉 当执行 monitorexit 时， Java 虚拟机就将锁对象的计数器减 1 ，当计数器减到 0 时，说明这个锁就被释放掉了，此时如果有其他 线程来请求，就可以请求成功 为什么采用这种方式呢？是为了允许同一个线程重复获取同一把锁。 比如，一个 Java 类中拥有好多个 synchronized 方法，那这些 方法之间的相互调用，不管是直接的还是间接的，都会涉及到对同一把锁的重复加锁操作。这样去设计的话，就可以避免这种情况。

### 8.Synchronized 同步方法的八种使用场景

八种使用场景：

1. 两个线程同时访问同一个对象的同步方法
2. 两个线程同时访问两个对象的同步方法
3. 两个线程同时访问（一个或两个）对象的静态同步方法
4. 两个线程分别同时访问（一个或两个）对象的同步方法和非同步方法
5. 两个线程访问同一个对象中的同步方法，同步方法又调用一个非同步方法
6. 两个线程同时访问同一个对象的不同的同步方法
7. 两个线程分别同时访问静态synchronized和非静态synchronized方法
8. 同步方法抛出异常后，JVM会自动释放锁的情况

场景一：两个线程同时访问同一个对象的同步方法

分析：这种情况是经典的对象锁中的方法锁，两个线程争夺同一个对象锁，所以会相互等待，是线程安全的。

**两个线程同时访问同一个对象的同步方法，是线程安全的。**

场景二：两个线程同时访问两个对象的同步方法

这种场景就是对象锁失效的场景，原因出在访问的是两个对象的同步方法，那么这两个线程分别持有的两个线程的锁，所以是互相不会受限的。加锁的目的是为了让多个线程竞争同一把锁，而这种情况多个线程之间不再竞争同一把锁，而是分别持有一把锁，所以我们的结论是：

**两个线程同时访问两个对象的同步方法，是线程不安全的。**

**问题在此：**

两个线程（thread1、thread2），访问两个对象（instance1、instance2）的同步方法（method()）,两个线程都有各自的锁，不能形成两个线程竞争一把锁的局势，所以这时，synchronized修饰的方法method()和不用synchronized修饰的效果一样（不信去把synchronized关键字去掉，运行结果一样），所以此时的method()只是个普通方法。

**如何解决这个问题：**

若要使锁生效，只需将method()方法用static修饰，这样就形成了类锁，多个实例（instance1、instance2）共同竞争一把类锁，就可以使两个线程串行执行了。这也就是下一个场景要讲的内容。

场景三：两个线程同时访问（一个或两个）对象的静态同步方法

这个场景解决的是场景二中出现的线程不安全问题，即用类锁实现：

**两个线程同时访问（一个或两个）对象的静态同步方法，是线程安全的。**

场景四：两个线程分别同时访问（一个或两个）对象的同步方法和非同步方法

这个场景是两个线程其中一个访问同步方法，另一个访问非同步方法，此时程序会不会串行执行呢，也就是说是不是线程安全的呢？ 我们可以确定是线程不安全的，如果方法不加`synchronized`都是安全的，那就不需要同步方法了。验证下我们的结论：

**两个线程分别同时访问（一个或两个）对象的同步方法和非同步方法，是线程不安全的。**

运行结果：

两个线程是并行执行的，所以是线程不安全的。

结果分析

问题在于此：method1没有被synchronized修饰，所以不会受到锁的影响。即便是在同一个对象中，当然在多个实例中，更不会被锁影响了。结论：

**非同步方法不受其它由synchronized修饰的同步方法影响**

你可能想到一个类似场景：多个线程访问同一个对象中的同步方法，同步方法又调用一个非同步方法，这个场景会是线程安全的吗？

场景五：两个线程访问同一个对象中的同步方法，同步方法又调用一个非同步方法

结果分析：

我们可以看出，普通方法被两个线程并行执行，不是线程安全的。这是为什么呢？

因为如果非同步方法，有任何其他线程直接调用，而不是仅在调用同步方法时，才调用非同步方法，此时会出现多个线程并行执行非同步方法的情况，线程就不安全了。

对于同步方法中调用非同步方法时，要想保证线程安全，就必须保证非同步方法的入口，仅出现在同步方法中。但这种控制方式不够优雅，若被不明情况的人直接调用非同步方法，就会导致原有的线程同步不再安全。所以不推荐大家在项目中这样使用，但我们要理解这种情况，并且我们要用语义明确的、让人一看就知道这是同步方法的方式，来处理线程安全的问题。

所以，最简单的方式，是在非同步方法上，也加上`synchronized`关键字，使其变成一个同步方法，这样就变成了《场景五：两个线程同时访问同一个对象的不同的同步方法》，这种场景下，大家就很清楚的看到，同一个对象中的两个同步方法，不管哪个线程调用，都是线程安全的了。

所以结论是：

**两个线程访问同一个对象中的同步方法，同步方法又调用一个非同步方法，仅在没有其他线程直接调用非同步方法的情况下，是线程安全的。若有其他线程直接调用非同步方法，则是线程不安全的。**

场景六：两个线程同时访问同一个对象的不同的同步方法

这个场景也是在探讨对象锁的作用范围，对象锁的作用范围是对象中的所有同步方法。所以，当访问同一个对象中的多个同步方法时，结论是：

**两个线程同时访问同一个对象的不同的同步方法时，是线程安全的。**

结果分析：

两个方法（method0()和method1()）的synchronized修饰符，虽没有指定锁对象，但默认锁对象为this对象为锁对象， 所以对于同一个实例（instance），两个线程拿到的锁是同一把锁，此时同步方法会串行执行。这也是`synchronized`关键字的可重入性的一种体现。

场景七：两个线程分别同时访问静态synchronized和非静态synchronized方法

这种场景的本质也是在探讨两个线程获取的是不是同一把锁的问题。静态`synchronized`方法属于类锁，锁对象是`（*.class）`对象，非静态`synchronized`方法属于对象锁中的方法锁，锁对象是`this`对象。两个线程拿到的是不同的锁，自然不会相互影响。结论：

**两个线程分别同时访问静态synchronized和非静态synchronized方法，线程不安全。**

场景八：同步方法抛出异常后，JVM会自动释放锁的情况

本场景探讨的是`synchronized`释放锁的场景：

**只有当同步方法执行完或执行时抛出异常这两种情况，才会释放锁。**

所以，在一个线程的同步方法中出现异常的时候，会释放锁，另一个线程得到锁，继续执行。而不会出现一个线程抛出异常后，另一个线程一直等待获取锁的情况。这是因为JVM在同步方法抛出异常的时候，会自动释放锁对象。

结果分析：

可以看出线程还是串行执行的，说明是线程安全的。而且出现异常后，不会造成死锁现象，JVM会自动释放出现异常线程的锁对象，其他线程获取锁继续执行。

### 9.**Synchronized的原理是什么？**

Synchronized是由JVM实现的一种实现互斥同步的方式，查看被Synchronized修饰过的程序块编译后的字节码，会发现，被Synchronized修饰过的程序块，在编译前后被编译器生成了`monitorenter`和`monitorexit`两个字节码指令。 在虚拟机执行到monitorenter指令时，首先要尝试获取对象的锁：如果这个对象没有锁定，或者当前线程已经拥有了这个对象的锁，把锁的计数器+1； 当执行monitorexit指令时，将锁计数器-1；当计数器为0时，锁就被释放了。如果获取对象失败了，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。 Java中Synchronize通过在对象头设置标志，达到了获取锁和释放锁的目的。

### 10.**为什么说Synchronized是非公平锁？**

非公平主要表现在获取锁的行为上，并非是按照申请锁的时间前后给等待线程分配锁的，每当锁被释放后，任何一个线程都有机会竞争到锁，这样做的目的是为了提高执行性能，缺点是可能会产生线程饥饿现象。

### 11.**同步方法和同步块，哪个是更好的选择？**

同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。

同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。

### 12.说说自己是怎么使用 synchronized 关键字，在项目中用到了吗

synchronized关键字最主要的三种使用方式： 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例 对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如 果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的 静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁 是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上 锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因 为JVM中，字符串常量池具有缓存功能！

### 5.synchronized 关键字的底层原理

**synchronized 关键字底层原理属于 JVM 层面。**

synchronized 同步语句块的情况

```html
public class SynchronizedDemo {
    public void method() {
        synchronized (this) {
            System.out.println("synchronized 代码块");
        }
    }
}
```

**`synchronized` 同步语句块的实现使用的是 `monitorenter` 和 `monitorexit` 指令，其中 `monitorenter` 指令指向同步代码块的开始位置，`monitorexit` 指令则指明同步代码块的结束位置。**

当执行 `monitorenter` 指令时，线程试图获取锁也就是获取 **对象监视器 `monitor`** 的持有权。

> 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 `ObjectMonitor`对象。
>
> 另外，**`wait/notify`等方法也依赖于`monitor`对象，这就是为什么只有在同步的块或者方法中才能调用`wait/notify`等方法，否则会抛出`java.lang.IllegalMonitorStateException`的异常的原因。**

在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。

在执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。

`synchronized` 修饰方法的的情况

```html
public class SynchronizedDemo2 {
    public synchronized void method() {
        System.out.println("synchronized 方法");
    }
}
```

`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。

总结

`synchronized` 同步语句块的实现使用的是 `monitorenter` 和 `monitorexit` 指令，其中 `monitorenter` 指令指向同步代码块的开始位置，`monitorexit` 指令则指明同步代码块的结束位置。

`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。

**不过两者的本质都是对对象监视器 monitor 的获取。**

### 6.JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗

JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。

锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。

### 7.Java对象头(存储锁类型)

在HotSpot虚拟机中, 对象在内存中的布局分为三块区域: 对象头, 示例数据和对其填充.

对象头中包含两部分: MarkWord 和 类型指针.

如果是数组对象的话, 对象头还有一部分是存储数组的长度.

多线程下synchronized的加锁就是对同一个对象的对象头中的MarkWord中的变量进行CAS操作.

MarkWord

Mark Word用于存储对象自身的运行时数据, 如HashCode, GC分代年龄, 锁状态标志, 线程持有的锁, 偏向线程ID等等. 占用内存大小与虚拟机位长一致(32位JVM -> MarkWord是32位, 64位JVM->MarkWord是64位).

类型指针

类型指针指向对象的类元数据, 虚拟机通过这个指针确定该对象是哪个类的实例.

对象头的长度

| 长度     | 内容                   | 说明                           |
| -------- | ---------------------- | ------------------------------ |
| 32/64bit | MarkWord               | 存储对象的hashCode或锁信息等   |
| 32/64bit | Class Metadada Address | 存储对象类型数据的指针         |
| 32/64bit | Array Length           | 数组的长度(如果当前对象是数组) |

如果是数组对象的话, 虚拟机用3个字宽(32/64bit + 32/64bit + 32/64bit)存储对象头; 如果是普通对象的话, 虚拟机用2字宽存储对象头(32/64bit + 32/64bit).

### 8.优化后synchronized锁的分类

级别从低到高依次是:

1. 无锁状态
2. 偏向锁状态
3. 轻量级锁状态
4. 重量级锁状态

锁可以升级, 但不能降级. 即: 无锁 -> 偏向锁 -> 轻量级锁 -> 重量级锁是单向的.

下面看一下每个锁状态时, 对象头中的MarkWord这一个字节中的内容是什么. 以32位为例.

无锁状态

| 25bit          | 4bit         | 1bit(是否是偏向锁) | 2bit(锁标志位) |
| -------------- | ------------ | ------------------ | -------------- |
| 对象的hashCode | 对象分代年龄 | 0                  | 01             |

偏向锁状态

| 23bit  | 2bit  | 4bit         | 1bit | 2bit |
| ------ | ----- | ------------ | ---- | ---- |
| 线程ID | epoch | 对象分代年龄 | 1    | 01   |

轻量级锁状态

| 30bit                | 2bit |
| -------------------- | ---- |
| 指向栈中锁记录的指针 | 00   |

重量级锁状态

| 30bit                      | 2bit |
| -------------------------- | ---- |
| 指向互斥量(重量级锁)的指针 | 10   |

### 9.锁的升级(进化)

偏向锁

偏向锁是针对于一个线程而言的, 线程获得锁之后就不会再有解锁等操作了, 这样可以省略很多开销. 假如有两个线程来竞争该锁话, 那么偏向锁就失效了, 进而升级成轻量级锁了.

为什么要这样做呢? 因为经验表明, 其实大部分情况下, 都会是同一个线程进入同一块同步代码块的. 这也是为什么会有偏向锁出现的原因.

在Jdk1.6中, 偏向锁的开关是默认开启的, 适用于只有一个线程访问同步块的场景.

偏向锁的加锁

当一个线程访问同步块并获取锁时, 会在锁对象的对象头和栈帧中的锁记录里存储锁偏向的线程ID, 以后该线程进入和退出同步块时不需要进行CAS操作来加锁和解锁, 只需要简单的测试一下锁对象的对象头的MarkWord里是否存储着指向当前线程的偏向锁(线程ID是当前线程), 如果测试成功, 表示线程已经获得了锁; 如果测试失败, 则需要再测试一下MarkWord中偏向锁的标识是否设置成1(表示当前是偏向锁), 如果没有设置, 则使用CAS竞争锁, 如果设置了, 则尝试使用CAS将锁对象的对象头的偏向锁指向当前线程.

偏向锁的撤销

偏向锁使用了一种等到竞争出现才释放锁的机制, 所以当其他线程尝试竞争偏向锁时, 持有偏向锁的线程才会释放锁. 偏向锁的撤销需要等到全局安全点(在这个时间点上没有正在执行的字节码). 首先会暂停持有偏向锁的线程, 然后检查持有偏向锁的线程是否存活, 如果线程不处于活动状态, 则将锁对象的对象头设置为无锁状态; 如果线程仍然活着, 则锁对象的对象头中的MarkWord和栈中的锁记录要么重新偏向于其它线程要么恢复到无锁状态, 最后唤醒暂停的线程(释放偏向锁的线程).

总结

偏向锁在Java6及更高版本中是默认启用的, 但是它在程序启动几秒钟后才激活. 可以使用-XX:BiasedLockingStartupDelay=0来关闭偏向锁的启动延迟, 也可以使用-XX:-UseBiasedLocking=false来关闭偏向锁, 那么程序会直接进入轻量级锁状态.

轻量级锁

当出现有两个线程来竞争锁的话, 那么偏向锁就失效了, 此时锁就会膨胀, 升级为轻量级锁.

轻量级锁加锁

线程在执行同步块之前, JVM会先在当前线程的栈帧中创建用户存储锁记录的空间, 并将对象头中的MarkWord复制到锁记录中. 然后线程尝试使用CAS将对象头中的MarkWord替换为指向锁记录的指针. 如果成功, 当前线程获得锁; 如果失败, 表示其它线程竞争锁, 当前线程便尝试使用自旋来获取锁, 之后再来的线程, 发现是轻量级锁, 就开始进行自旋.

轻量级锁解锁

轻量级锁解锁时, 会使用原子的CAS操作将当前线程的锁记录替换回到对象头, 如果成功, 表示没有竞争发生; 如果失败, 表示当前锁存在竞争, 锁就会膨胀成重量级锁.

总结

总结一下加锁解锁过程, 有线程A和线程B来竞争对象c的锁(如: synchronized(c){} ), 这时线程A和线程B同时将对象c的MarkWord复制到自己的锁记录中, 两者竞争去获取锁, 假设线程A成功获取锁, 并将对象c的对象头中的线程ID(MarkWord中)修改为指向自己的锁记录的指针, 这时线程B仍旧通过CAS去获取对象c的锁, 因为对象c的MarkWord中的内容已经被线程A改了, 所以获取失败. 此时为了提高获取锁的效率, 线程B会循环去获取锁, 这个循环是有次数限制的, 如果在循环结束之前CAS操作成功, 那么线程B就获取到锁, 如果循环结束依然获取不到锁, 则获取锁失败, 对象c的MarkWord中的记录会被修改为重量级锁, 然后线程B就会被挂起, 之后有线程C来获取锁时, 看到对象c的MarkWord中的是重量级锁的指针, 说明竞争激烈, 直接挂起.

解锁时, 线程A尝试使用CAS将对象c的MarkWord改回自己栈中复制的那个MarkWord, 因为对象c中的MarkWord已经被指向为重量级锁了, 所以CAS失败. 线程A会释放锁并唤起等待的线程, 进行新一轮的竞争.

锁的比较

| 锁       | 优点                                                         | 缺点                                            | 适用场景                           |
| -------- | ------------------------------------------------------------ | ----------------------------------------------- | ---------------------------------- |
| 偏向锁   | 加锁和解锁不需要额外的消耗, 和执行非同步代码方法的性能相差无几. | 如果线程间存在锁竞争, 会带来额外的锁撤销的消耗. | 适用于只有一个线程访问的同步场景   |
| 轻量级锁 | 竞争的线程不会阻塞, 提高了程序的响应速度                     | 如果始终得不到锁竞争的线程, 使用自旋会消耗CPU   | 追求响应时间, 同步快执行速度非常快 |
| 重量级锁 | 线程竞争不适用自旋, 不会消耗CPU                              | 线程堵塞, 响应时间缓慢                          | 追求吞吐量, 同步快执行时间速度较长 |

总结

首先要明确一点是引入这些锁是为了提高获取锁的效率, 要明白每种锁的使用场景, 比如偏向锁适合一个线程对一个锁的多次获取的情况; 轻量级锁适合锁执行体比较简单(即减少锁粒度或时间), 自旋一会儿就可以成功获取锁的情况.

### 10.谈谈 synchronized 和 ReentrantLock 的区别

两者都是可重入锁

**“可重入锁”** 指的是自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增 1，所以要等到锁的计数器下降为 0 时才能释放锁。

synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API

`synchronized` 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 `synchronized` 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。`ReentrantLock` 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。

ReentrantLock 比 synchronized 增加了一些高级功能

相比`synchronized`，`ReentrantLock`增加了一些高级功能。主要来说主要有三点：

- **等待可中断** : `ReentrantLock`提供了一种能够中断等待锁的线程的机制，通过 `lock.lockInterruptibly()` 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。
- **可实现公平锁** : `ReentrantLock`可以指定是公平锁还是非公平锁。而`synchronized`只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。`ReentrantLock`默认情况是非公平的，可以通过 `ReentrantLock`类的`ReentrantLock(boolean fair)`构造方法来制定是否是公平的。
- **可实现选择性通知（锁可以绑定多个条件）**: `synchronized`关键字与`wait()`和`notify()`/`notifyAll()`方法相结合可以实现等待/通知机制。`ReentrantLock`类当然也可以实现，但是需要借助于`Condition`接口与`newCondition()`方法。

> `Condition`是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个`Lock`对象中可以创建多个`Condition`实例（即对象监视器），**线程对象可以注册在指定的`Condition`中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用`notify()/notifyAll()`方法进行通知时，被通知的线程是由 JVM 选择的，用`ReentrantLock`类结合`Condition`实例可以实现“选择性通知”** ，这个功能非常重要，而且是 Condition 接口默认提供的。而`synchronized`关键字就相当于整个 Lock 对象中只有一个`Condition`实例，所有的线程都注册在它一个身上。如果执行`notifyAll()`方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而`Condition`实例的`signalAll()`方法 只会唤醒注册在该`Condition`实例中的所有等待线程。

**如果你想使用上述功能，那么选择 ReentrantLock 是一个不错的选择。性能已不是选择标准**。

## Lock

### 1.ReentrantLock

ReentrantLock继承了Lock接口并实现了在接口中定义的方法，是一个可重入的独占锁。ReentrantLock 通过自定义队列同步器（Abstract Queued Sychronized，AQS）来实现锁的获取与释放。 独占锁指该锁在同一时刻只能被一个线程获取，而获取锁的其他线程只能在同步队列中等待；可重入锁指该锁能够支持一个线程对同一个资源执行多次加锁操作。 ReentrantLock支持公平锁和非公平锁的实现。公平指线程竞争锁的机制是公平的，而非公平指不同的线程获取锁的机制是不公平的。 ReentrantLock不但提供了synchronized对锁的操作功能，还提供了诸如可响应中断锁、可轮询锁请求、定时锁等避免多线程死锁的方法。

ReentrantLock的用法

ReentrantLock有显式的操作过程，何时加锁、何时释放锁都在程序员的控制之下。具体的使用流程是定义一个ReentrantLock，在需要加锁的地方通过lock方法加锁，等资源使用完成后再通过unlock方法释放锁。

```html
public class ReentrantLockDemo implements Runnable{
    public static ReentrantLock lock=new ReentrantLock();
    //加锁
    lock.lock();
    //释放锁
    lock.unlock();
}
```

ReentrantLock之所以被称为可重入锁，是因为ReentrantLock锁可以反复进入。即允许连续两次获得同一把锁，两次释放同一把锁。注意，获取锁和释放锁的次数要相同，如果释放锁的次数多于获取锁的次数，Java就会抛出java.lang.IllegalMonitorStateException异常；如果释放锁的次数少于获取锁的次数，该线程就会一直持有该锁，其他线程将无法获取锁资源。

ReentrantLock如何避免死锁：响应中断、可轮询锁、定时锁

（1）响应中断：在synchronized中如果有一个线程尝试获取一把锁，则其结果是要么获取锁继续执行，要么保持等待。ReentrantLock还提供了可响应中断的可能，即在等待锁的过程中，线程可以根据需要取消对锁的请求。

```html
//如果当前线程未被中断，则获取锁
lock.lockInterruptibly();
```

（2）可轮询锁：通过boolean tryLock()获取锁。如果有可用锁，则获取该锁并返回true，如果无可用锁，则立即返回false。 （3）定时锁：通过boolean tryLock(long time,TimeUnit unit)throws InterruptedException获取定时锁。如果在给定的时间内获取到了可用锁，且当前线程未被中断，则获取该锁并返回true。如果在给定的时间内获取不到可用锁，将禁用当前线程，并且在发生以下三种情况之前，该线程一直处于休眠状态。

- 当前线程获取到了可用锁并返回true。
- 当前线程在进入此方法时设置了该线程的中断状态，或者当前线程在获取锁时被中断，则将抛出InterruptedException，并清除当前线程的已中断状态。
- 当前线程获取锁的时间超过了指定的等待时间，则将返回false。如果设定的时间小于等于0，则该方法将完全不等待。

公平锁与非公平锁

ReentrantLock支持公平锁和非公平锁两种方式。

公平锁指锁的分配和竞争机制是公平的，即遵循先到先得原则。通常先对锁提出获取请求的线程会先被分配到锁，ReentrantLock在构造函数中提供了是否公平锁的初始化方式来定义公平锁。

非公平锁指JVM遵循随机、就近原则分配锁的机制。

ReentrantLock通过在构造函数ReentrantLock(boolean fair)中传递不同的参数来定义不同类型的锁，默认的实现是非公平锁。这是因为，非公平锁虽然放弃了锁的公平性，但是执行效率明显高于公平锁。如果系统没有特殊的要求，一般情况下建议使用非公平锁。

### 2.Lock接口的主要方法

Lock接口的主要方法如下。

- void lock()：给对象加锁，如果锁未被其他线程使用，则当前线程将获取该锁；如果锁正在被其他线程持有，则将禁用当前线程，直到当前线程获取锁。
- boolean tryLock()：试图给对象加锁，如果锁未被其他线程使用， 则将获取该锁并返回true ， 否则返回false 。tryLock() 和lock()的区别在于tryLock()只是“试图”获取锁，如果没有可用锁，就会立即返回。lock()在锁不可用时会一直等待，直到获取到可用锁。
- tryLock()：尝试获取锁，仅在调用时锁未被线程占用，获取锁。
- tryLock(long timeout TimeUnit unit)：创建定时锁，如果在给定的等待时间内有可用锁，则获取该锁。
- void unlock()：释放当前线程所持有的锁。锁只能由持有者释放，如果当前线程并不持有该锁却执行该方法，则抛出异常。
- Condition newCondition()：创建条件对象，获取等待通知组件。该组件和当前锁绑定，当前线程只有获取了锁才能调用该组件的await()，在调用后当前线程将释放锁。
- getHoldCount()：查询当前线程保持此锁的次数，也就是此线程执行lock方法的次数。
- getQueueLength()：返回等待获取此锁的线程估计数，比如启动 5 个线程，1 个线程获得锁，此时返回4。
- getWaitQueueLength(Condition condition) ： 返回在Condition条件下等待该锁的线程数量。比如有 5 个线程用同一个condition对象，并且这 5 个线程都执行了condition对象的await方法，那么执行此方法将返回5。
- hasWaiters(Condition condition)：查询是否有线程正在等待与给定条件有关的锁，即对于指定的contidion对象，有多少线程执行了condition.await方法。
- hasQueuedThread(Thread thread)：查询给定的线程是否等待获取该锁。
- hasQueuedThreads()：查询是否有线程等待该锁。
- isFair()：查询该锁是否为公平锁。
- isHeldByCurrentThread()：查询当前线程是否持有该锁，线程执行lock方法的前后状态分别是false和true。
- isLock()：判断此锁是否被线程占用。
- lockInterruptibly()：如果当前线程未被中断，则获取该锁。

### 3.synchronized和ReentrantLock的比较

- 与synchronized相比，ReentrantLock提供了更多，更加全面的功能，具备更强的扩展性。例如：时间锁等候，可中断锁等候，锁投票。
- ReentrantLock还提供了条件Condition，对线程的等待、唤醒操作更加详细和灵活，所以在多个条件变量和高度竞争锁的地方，ReentrantLock更加适合。
- ReentrantLock提供了可轮询的锁请求。它会尝试着去获取锁，如果成功则继续，否则可以等到下次运行时处理，而synchronized则一旦进入锁请求要么成功要么阻塞，所以相比synchronized而言，ReentrantLock会不容易产生死锁些。
- ReentrantLock支持更加灵活的同步代码块，但是使用synchronized时，只能在同一个synchronized块结构中获取和释放。注：ReentrantLock的锁释放一定要在finally中处理，否则可能会产生严重的后果。

1. ReentrantLock支持中断处理，且性能较synchronized会好些。

### 4.ReentrantReadWriteLock

在没有任何读写锁的时候，才可以取得写锁。如果一直有读锁存在，则无法执行写锁，这就会导致写锁饥饿。

### 5.StampedLock

控制锁三种模式：写、读、乐观读。

StampedLock的状态由版本和模式两个部分组成，锁获取方法返回的是一个数字作为票据，用相应的锁状态来表示并控制相关的访问，数字0表示没有写锁被授权访问。 在读锁上分为悲观锁和乐观锁，乐观读就是在读操作很多，写操作很少的情况下，可以乐观的认为写入和读取同时发生的几率很小。因此，不悲观的使用完全的读取锁定。程序可以查看读取资料之后，是否遭到写入进行了变更，再采取后续的措施，这样的改进可以大幅度提升程序的吞吐量。 总之，在读线程越来越多的场景下，StampedLock大幅度提升了程序的吞吐量。

### 6.Condition

Condition是一个多线程间协调通信的工具类，Condition除了实现wait和notify的功能以外，它的好处在于一个lock可以创建多个Condition，可以选择性的通知wait的线程。 特点： Condition 的前提是Lock，由AQS中newCondition()方法 创建Condition的对象 Condition await方法表示线程从AQS中移除，并释放线程获取的锁，并进入Condition等待队列中等待，等待被signal Condition signal方法表示唤醒对应Condition等待队列中的线程节点，并加入AQS中，准备去获取锁。

### 27.ReentrantLock

ReentrantLock，锁API，是如何基于AQS来实现的，读写锁，可以加读锁，也可以加写锁

但是，读锁和写锁是互斥的，也就是说，你加了读锁之后，就不能加写锁；如果加了写锁，就不能加读锁

但是如果有人加了读锁之后，别人可以同时加读锁

如果你有一份数据，有人读，有人写，如果你全部都是用synchronized的话，会导致如果多个人读，也是要串行化，一个接一个的读

我们希望的效果是多个人可以同时来读，如果使用读锁和写锁分开的方式，就可以让多个人来读数据，多个人可以同时加读锁

如果有人在读数据，就不能有人写数据，读锁 -> 写锁 -> 互斥

如果有人在写数据，别人不能写数据，写锁 -> 写锁 -> 互斥；如果有人在写数据，别人也不能读数据，写锁 -> 读锁 > 互斥

基于AQS的state二进制高低16位判断实现写锁的可重入加锁

写锁的可重入加锁，state的二进制高低16位的判断了

如果是int类型的state不是0的话，那么他的二进制数值，32位，低16位一定不是0，如果低16位不是0的话，就代表他是加过写锁的

c != 0，w == 0，c肯定不是0，但是低16位是0，说明有人加了读锁，没有人加写锁，此时你要加写锁，而且你还不是之前加锁的那个线程

c != 0，w != 0，有人加过锁，之前加的是写锁，但是当前线程不是之前加锁的线程，此时也不让你加写锁，同一个时间，只能有一个线程加写锁，如果线程1比如加了写锁，线程2也要加写锁，是不行的

c != 0，w != 0，之前有人加过写锁，而且加写锁的是你自己

如果加写锁的人是你自己，说明你就是在可重入的加写锁，将state += 1

c =1

w = 1，代表了c，int，32位，低16位的值，代表了写锁的可重入加锁次数

读写锁互斥：基于AQS的state二进制高低16位完成互斥判断

exclusiveCount(c)：获取的是state的低16位，代表写锁的状态值，如果不等于0，说明有人加了写锁，而且还不是你加的那个写锁，此时是不能加读锁的，在这里就形成了一个基于state的二进制高低16

写锁 -> 读锁的互斥

基于CAS实现多线程并发同时只有一个可以加读锁

int r = sharedCount(c);：拿到state的高16位的值，代表了读锁的加锁次数

### 28.锁优化策略

标志位修改等可见性场景优先使用volatile

如果仅仅只是有一些线程会来写一个变量，标志位，另外一个线程是来读取这个标志位的值，那么此时优先使用volatile

能不用锁尽量别用锁，就会比较麻烦，可能会导致锁争用和冲突，如果没弄好的，锁的问题，可能会导致系统的吞吐量、性能会大幅度的降低

数值递增场景优先使用Atomic原子类

多线程并发访问一些共享数据，先分析和判断一下，那个变量是不是有人读，有人写，直接就是volatile就可以了。如果大家都要写，再判断一下，仅仅只是简单的数值累加或者变更，数值的一些操作

建议大家可以用 Atomic原子类，CAS机制，无锁化，并发性要比synchronized要好不少的，相当于简单的更新数值，都要一个一个线程依次加锁进入，修改，再释放锁，这个过程其实是并发性还是要差一些的

数据允许多副本场景优先使用ThreadLocal

如果你不需要多个线程共享读写一个数据的话，可以让每个线程保持一个本地变量的副本的话，那么你其实可以搞一个ThreadLocal，让每个线程都维护一个变量的副本，每个线程就操作自己本地的副本就可以了

txid，requestId，每个线程就把自己的那个值放到自己本地副本里，后续自己来修改和读取就可以了，跟其他的线程之间就没有什么冲突了

读多写少需要加锁的场景优先使用读写锁

多线程并发访问一块共享数据，就需要加锁了，优先考虑读写锁，synchronized重量级

读多写少的场景，读写锁分离，读锁 -> 大量的线程并发的读，写锁 -> 写数据其他人不能写同时来写，也不能有人来读

服务注册表的读写锁分离

读多，写少，每隔30秒拉拉取增量注册表，发送心跳，大多数的操作都是走读锁，并发的来实现读，synchronized的话，大量的读都是串行化，一个一个的读，导致服务注册中心的并发的性能很差

尽可能减少线程对锁占用的时间

读写锁用不了，synchronized锁，读写锁，加锁，有一个核心点，尽量保证你加锁的时间是很短的，不要在加锁之后，执行一些磁盘文件读写、网络IO读写，导致锁占用的时间过于长

一般 来说，我们建议，加锁，尽量就是操作一下内存里的数据就可以了，不要在锁里面去执行一些耗时的一些操作，比如说执行数据库操作，SQL，或者是别的一些东西，可能会导致占用锁的时间

就会导致线程并发的吞吐量大幅度的下降，并发能力就很弱，性能很差

尽可能减少线程对数据加锁的粒度

（1）分段加锁，实践，尽可能的减少一个线程占用锁的时间

（2）尽可能减少你对数据加锁的粒度

比方说，你手上有一份数据，里面包含了多个子数据，你加锁，可以对一整块完整的大数据来加锁，别人只要访问这一大块数据，都会有锁的争用的问题。你也可以选择降低加锁的粒度，你仅仅对大块数据里的部分子数据加锁

如果别的线程去请求这个大块数据里，其他的子数据的话，就不会跟你的锁产生冲突

更少的数据，或者是对更少的代码进行加锁

尽可能对不同功能分离锁的使用

就是如果可能的话，比如说你有一个锁，你看看能不能按照这个功能的不同，拆分为两把锁，在使用不同的功能的时候，可以用不同的锁，这样降低线程竞争锁的冲突。阻塞队列，人家在实现源码的时候，就使用了两把锁

队头是一把锁，队列尾巴是一把锁，你从队列尾巴插进去是加的一把锁，从队头消费数据使用的是另外一把锁，入队和出队的操作，就不会因为锁产生冲突了

避免在循环中频繁的加锁以及释放锁

如果是正常的代码流程，尽量避免在for循环里频繁的加锁释放锁

尽量减少高并发场景中线程对锁的争用

读写锁主要是解决了大量的读请求不会串行化，读请求可以并发起来

降低锁竞争的频率

系统刚启动的时候，会有一个线程来填充各级缓存的数据

此后的30秒内，大家全部都是读缓存数据的，不会涉及到任何加锁的行为

在这个过程中，如果有人更新注册表的数据的话，一方面会对注册表本身加写锁，另外一方面对缓存加一个锁，那么会过期掉readWriteMap的缓存。此时所有加的锁，是不会对高频的读请求有任何的锁的冲突和影响的

在写数据的期间，读数据不涉及任何读写锁的冲突，直接读的cache数据

有一个后台的线程，可能会过30秒之后，对缓存加个锁，然后同步两个缓存的数据，在这个过程中，实际上来说也是不会对高频的读操作施加任何的影响的

只有在此时，会有线程感知到缓存数据是null，重新填充数据，重新填充数据的时候，会涉及到重新从服务注册表查数据，然后加读锁，此时就是一个线程加了一个读锁，而且是很快的行为，大量的降低了频繁的读操作，可能频繁的跟写操作，读写锁冲突的问题

大幅度的降低了读写锁的互斥冲突的问题。避免了说高频的读操作，对服务注册表频繁的读取，频繁的加读锁，导致跟服务注册表的写锁频繁的互斥和冲突

各种锁的运用，加了两级缓存，尽可能的把读写锁的互斥和冲突，降低到了最低

为什么是两级缓存？不是一级缓存呢？

### 29.Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁

从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化

synchronized说是锁，但是他的底层加锁的方式 可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁

这些东西本身你只要了解一个概念就可以了，JDK 1.6开始对synchronized关键字做过哪些优化，有哪些加锁的方式，效果是什么，作用是什么，在实际的开发和使用中，根本就不需要你去过多的care一些东西

synchronized(this) {

}

（1）锁消除

锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令

这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁

（2）锁粗化

synchronized(this) {

}

synchronized(this) {

}

synchronized(this) {

}

这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁

（3）偏向锁

这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS

性能会提升很多

但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好

可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小

如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好

（4）轻量级锁

如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁

如果是自己加的锁，那就执行代码就好了

如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁

（5）适应性锁

这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒

也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大

所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了

一直追问我，什么自旋锁，不是什么事儿，当然，如果要站在jvm的底层层面，去说清楚的话，确实是比较复杂的，但是我觉得起码目前为止，暂时也没必要，各种锁底层是如何来实现的，完全可以等到以后jvm那块都讲过之后

再回过头来深入jvm底层的原理来剖析：偏向锁、自旋锁、轻量级锁，jvm层面的概念，栈侦，Load Record，不一定能听懂，基础的知识没有铺垫好，需要通过调节jvm的一些参数来优化底层synchronized里的各种加锁方式的使用

这样可以大幅度减少线程上下文的切换，而这种自旋等待获取锁的方式，就是所谓自旋锁，就是不断的自旋尝试获取锁

如果一个线程持有锁的时间很长，那么其他线程获取不到锁，就会暂停，发生上下文切换，让其他线程来执行，这种自己暂停获取锁的方式，就是所谓的重量级锁

这个根据不同情况自动调整的过程，就是适应锁的意思

## 并发包

### 1.CountDownLatch（线程计数器）

CountDownLatch类位于java.util.concurrent包下，是一个同步工具类，允许一个或多个线程一直等待其他线程的操作执行完后再执行相关操作。

CountDownLatch基于线程计数器来实现并发访问控制，主要用于主线程等待其他子线程都执行完毕后执行相关操作。其使用过程为：在主线程中定义CountDownLatch，并将线程计数器的初始值设置为子线程的个数，多个子线程并发执行，每个子线程在执行完毕后都会调用countDown函数将计数器的值减1，直到线程计数器为0，表示所有的子线程任务都已执行完毕，此时在CountDownLatch上等待的主线程将被唤醒并继续执行。

我们利用CountDownLatch可以实现类似计数器的功能。比如有一个主任务，它要等待其他两个任务都执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能。

```html
//定义大小为1的CountDownLatch
final CountDownLatch latch=new CountDownLatch(1);
new Thread(){
    public void run(){
        try{
            latch.countDown();//在子线程1执行完毕后调用countDown方法
        }catch(Exception e){
        }
    }
}.start();
try{
    latch.await();//在CountDownLatch上等待子程序执行完毕
    //子程序执行完毕，开始执行主程序
}
```

以上代码片段先定义了一个大小为1的CountDownLatch，然后定义了一个子线程并启动该子线程，子线程执行完业务代码后在执行latch.countDown()时减少一个信号量，表示自己已经执行完成。主线程调用latch.await()阻塞等待，在所有线程都执行完成并调用了countDown函数时，表示所有线程均执行完成，这时程序会主动唤醒主线程并开始执行主线程的业务逻辑。

### 2.CyclicBarrier（回环栅栏-等待至barrier状态再全部同时执行）

CyclicBarrier（循环屏障）是一个同步工具，可以实现让一组线程等待至某个状态之后再全部同时执行。在所有等待线程都被释放之后，CyclicBarrier可以被重用。CyclicBarrier的运行状态叫作Barrier状态，在调用await方法后，线程就处于Barrier状态。

CyclicBarrier中最重要的方法是await方法，它有两种实现。

- public int await()：挂起当前线程直到所有线程都为Barrier状态再同时执行后续的任务。
- public int await(long timeout, TimeUnit unit)：设置一个超时时间，在超时时间过后，如果还有线程未达到Barrier状态，则不再等待，让达到Barrier状态的线程继续执行后续的任务。

定义一个CyclicBarrier，然后循环启动了多个线程，每个线程都通过构造函数将CyclicBarrier传入线程中，在线程内部开始执行第1阶段的工作，比如查询数据等；等第1阶段的工作处理完成后，再调用cyclicBarrier.await方法等待其他线程也完成第1阶段的工作（CyclicBarrier让一组线程等待到达某个状态再一起执行）；等其他线程也执行完第1阶段的工作，便可执行并发操作的下一项任务，比如数据分发等。

### 3.Semaphore（信号量-控制同时访问的线程个数）

Semaphore指信号量，用于控制同时访问某些资源的线程个数，具体做法为通过调用acquire()获取一个许可，如果没有许可，则等待，在许可使用完毕后通过release()释放该许可，以便其他线程使用。

Semaphore常被用于多个线程需要共享有限资源的情况，比如办公室有两台打印机，但是有5个员工需要使用，一台打印机同时只能被一个员工使用，其他员工排队等候，且只有该打印机被使用完毕并释放后其他员工方可使用，这时就可以通过Semaphore来实现。

定义了一个数量为 2的Semaphore，然后定义了一个工作线程Worker并通过构造函数将Semaphore传入线程内部。在线程调用semaphore.acquire()时开始申请许可并执行业务逻辑，在线程业务逻辑执行完成后调用semaphore.release()释放许可以便其他线程使用。

在Semaphore类中有以下几个比较重要的方法。

- public void acquire()：以阻塞的方式获取一个许可，在有可用许可时返回该许可，在没有可用许可时阻塞等待，直到获得许可。
- public void acquire(int permits)：同时获取permits个许可。
- public void release()：释放某个许可。
- public void release(int permits)：释放permits个许可。
- public boolean tryAcquire()：以非阻塞方式获取一个许可，在有可用许可时获取该许可并返回true，否则返回false，不会等待。
- public boolean tryAcquire(long timeout,TimeUnit unit)：如果在指定的时间内获取到可用许可，则返回true，否则返回false。
- public boolean tryAcquire(int permits)：如果成功获取permits个许可，则返回true，否则立即返回false。
- public boolean tryAcquire(int permits,longtimeout,TimeUnit unit)：如果在指定的时间内成功获取permits个许可，则返回true，否则返回false。
- availablePermits()：查询可用的许可数量。

### 4.Semaphore与ReentrantLock

Semaphore基本能完成ReentrantLock的所有工作，使用方法也与之类似，通过acquire方法和release方法来获取和释放许可信号资源。Semaphone.acquire方法默认和ReentrantLock. lockInterruptibly方法的效果一样，为可响应中断锁，也就是说在等待许可信号资源的过程中可以被Thread.interrupt方法中断而取消对许可信号的申请。

此外，Semaphore也实现了可轮询的锁请求、定时锁的功能，以及公平锁与非公平锁的机制。对公平与非公平锁的定义在构造函数中设定。

Semaphore的锁释放操作也需要手动执行，因此，为了避免线程因执行异常而无法正常释放锁，释放锁的操作必须在finally代码块中完成。

Semaphore也可以用于实现一些对象池、资源池的构建，比如静态全局对象池、数据库连接池等。此外，我们也可以创建计数为1的Semaphore，将其作为一种互斥锁的机制（也叫二元信号量，表示两种互斥状态），同一时刻只能有一个线程获取该锁。

### 5.CountDownLatch、CyclicBarrier、Semaphore的区别

- CountDownLatch和CyclicBarrier都用于实现多线程之间的相互等待，但二者的关注点不同。CountDownLatch主要用于主线程等待其他子线程任务均执行完毕后再执行接下来的业务逻辑单元，而CyclicBarrier主要用于一组线程互相等待大家都达到某个状态后，再同时执行接下来的业务逻辑单元。此外，CountDownLatch是不可以重用的，而CyclicBarrier是可以重用的。
- Semaphore和Java中的锁功能类似，主要用于控制资源的并发访问。

### 6.在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？

CyclicBarrier 可以重复使用，而 CountdownLatch 不能重复使用。 Java 的 concurrent 包里面的 CountDownLatch 其实可以把它看作一个计数器，只不过这个计数器的操作是原子操作，同时只能有一个线程去操作这个计数器，也就是同时只能有一个线程去减这个计数器里面的值。你可以向 CountDownLatch 对象设置一个初始的数字作为计数值，任何调用这个对象上的 await()方法都会阻塞，直到这个计数器的计数值被其他的线程减为 0 为止。 所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 CountDownLatch 的一个非常典型的应用场景是：有一个任务想要往下执行，但必须要等到其他的任务执行完毕后才可以继续往下执行。 假如我们这个想要继续往下执行的任务调用一个 CountDownLatch 对象的 await()方法，其他的任务执行完自己的任务后调用同一个CountDownLatch 对象上的 countDown()方法，这个调用 await()方法的任务将一直阻塞等待，直到这个 CountDownLatch 对象的计数值减到 0 为止。 CyclicBarrier 一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrie在释放等待线程后可以重用，所以称它为循环 的barrier。

3.什么是并发容器的实现？

同步容器：可以简单地理解为通过synchronized 来实现同步的容器，如果有多个线程调用同步容器的方法，它们将会串行执行。比如Vector，Hashtable，以

及Collections.synchronizedSet，synchronizedList 等方法返回的容器。可以通过查看Vector， Hashtable 等这些同步容器的实现代码，可以看到这些容器实现

线程安全的方式就是将它们的状态封装起来，并在需要同步的方法上加上关键字synchronized。

并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在ConcurrentHashMap 中采用了一种粒度更细的加锁机制， 可以称为分段

锁， 在这种锁机制下，允许任意数量的读线程并发地访问map，并且执行读操作的线程和写操作的线程也可以并发的访问map，同时允许一定数量的写操作线程

并发地修改map，所以它可以在并发环境下实现更高的吞吐量。

16.多线程同步和互斥有几种实现方法，都是什么？

线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息， 当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。

线程互斥是指对于共享的进程系统资源， 在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时， 任何时刻最多只允许一个线程去使用，其它

要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。

线程间的同步方法大体可分为两类： 用户模式和内核模式。

顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式不需要切换到内核态，只在用户态完成操作。

用户模式下的方法有：原子操作（例如一个单一的全局变量）， 临界区。内核模式下的方法有：事件，信号量，互斥量。

6.SynchronizedMap 和ConcurrentHashMap 有什么区别？

SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为map。

ConcurrentHashMap 使用分段锁来保证在多线程下的性能。ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将hash 表分为16 个桶，

诸如get,put,remove 等常用操作只锁当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有16 个写线程执行，并发性能的提升是显而易见的。

另外ConcurrentHashMap 使用了一种不同的迭代方式。 当iterator 被创建后集合再发生改变就不再是抛出ConcurrentModificationException，取而代之的是在

改变时new 新的数据从而不影响原有的数据，iterator 完成后再将头指针替换为新的数据，这样iterator线程可以使用原来老的数据，而写线程也可以并发的完成

改变。

7.CopyOnWriteArrayList 可以用于什么应用场景？

CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出ConcurrentModificationException。CopyOnWriteArrayList

中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时， 读取操作可以安全地执行。

1、由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下， 可能导致young gc 或者full gc；

2、不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个set操作后，读取到数据可能还是旧的,虽然CopyOnWriteArrayList 能做到最终一致

性,但是还是没法满足实时性要求；

CopyOnWriteArrayList 透露的思想

1、读写分离，读和写分开

2、最终一致性

3、使用另外开辟空间的思路，来解决并发冲突

### 15、什么是并发容器的实现？

何为同步容器：可以简单地理解为通过synchronized来实现同步的容器，如果有多个线程调⽤同步容器的⽅法，它们将会串⾏执 ⾏。⽐如Vector，Hashtable，以及Collections.synchronizedSet，synchronizedList等⽅法返回的容器。 可以通过查看Vector，Hashtable等这些同步容器的实现代码，可以看到这些容器实现线程安全的⽅式就是将它们的状态封装起 来，并在需要同步的⽅法上加上关键字synchronized。 并发容器使⽤了与同步容器完全不同的加锁策略来提供更⾼的并发性和伸缩性，例如在ConcurrentHashMap中采⽤了⼀种粒度 更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问map，并且执⾏读操作的线程和写操作 的线程也可以并发的访问map，同时允许⼀定数量的写操作线程并发地修改map，所以它可以在并发环境下实现更⾼的吞吐量。

## 原子类

### 1.AtomicInteger

我们知道，在多线程程序中，诸如++i或i++等运算不具有原子性，因此不是安全的线程操作。我们可以通过synchronized或ReentrantLock将该操作变成一个原子操作，但是synchronized和ReentrantLock均属于重量级锁。因此JVM为此类原子操作提供了一些原子操作同步类，使得同步操作（线程安全操作）更加方便、高效，它便是AtomicInteger。

AtomicInteger为提供原子操作的Integer的类，常见的原子操作类还有AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference等，它们的实现原理相同，区别在于运算对象的类型不同。还可以通过AtomicReference将一个对象的所有操作都转化成原子操作。AtomicInteger的性能通常是synchronized和ReentrantLock的好几倍。具体用法如下：

```html
class AtomicIntegerDemo implements Runnable{
    //定义一个原子操作数
    static AtomicInteger safeCounter=new AtomicInteger(0);
    public void run(){
        for(int m=0;m<100000;m++){
            safeCounter.getAndIncrement();//对原子操作数执行自增操作
        }
    }
}
```

### 2.说一下 atomic 的原理？

atomic 主要利用 CAS (Compare And Wwap) 和 volatile 和 native 方法来保证原子操作，从而避免synchronized 的高开销，执行效率大为提升。

### 50.**并发容器总结**

JDK 提供的并发容器

- **ConcurrentHashMap:** 线程安全的 HashMap
- **CopyOnWriteArrayList:** 线程安全的 List，在读多写少的场合性能非常好，远远好于 Vector.
- **ConcurrentLinkedQueue:** 高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。
- **BlockingQueue:** 这是一个接口，JDK 内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。
- **ConcurrentSkipListMap:** 跳表的实现。这是一个 Map，使用跳表的数据结构进行快速查找。

ConcurrentHashMap

HashMap 不是线程安全的，在并发场景下如果要保证一种可行的方式是使用 `Collections.synchronizedMap()` 方法来包装我们的 HashMap。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。

所以就有了 HashMap 的线程安全版本—— ConcurrentHashMap 的诞生。在 ConcurrentHashMap 中，无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。

CopyOnWriteArrayList

CopyOnWriteArrayList 简介

```html
public class CopyOnWriteArrayList<E>
extends Object
implements List<E>, RandomAccess, Cloneable, Serializable
```

CopyOnWriteArrayList 是如何做到的？

`CopyOnWriteArrayList` 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。

从 `CopyOnWriteArrayList` 的名字就能看出`CopyOnWriteArrayList` 是满足`CopyOnWrite` 的 ArrayList，所谓`CopyOnWrite` 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。

CopyOnWriteArrayList 读取操作的实现

读取操作没有任何同步控制和锁操作，理由就是内部数组 array 不会发生修改，只会被另外一个 array 替换，因此可以保证数据安全。

CopyOnWriteArrayList 写入操作的实现

CopyOnWriteArrayList 写入操作 add() 方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会 copy 出多个副本出来。

ConcurrentLinkedQueue

Java 提供的线程安全的 Queue 可以分为**阻塞队列**和**非阻塞队列**，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 **阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。**

从名字可以看出，`ConcurrentLinkedQueue`这个队列使用链表作为其数据结构．ConcurrentLinkedQueue 应该算是在高并发环境中性能最好的队列了。它之所有能有很好的性能，是因为其内部复杂的实现。

ConcurrentLinkedQueue 内部代码我们就不分析了，大家知道 ConcurrentLinkedQueue 主要使用 CAS 非阻塞算法来实现线程安全就好了。

ConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue 来替代。

BlockingQueue

BlockingQueue 简单介绍

阻塞队列——BlockingQueue。阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。

BlockingQueue 是一个接口，继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。

ArrayBlockingQueue

**ArrayBlockingQueue** 是 BlockingQueue 接口的有界队列实现类，底层采用**数组**来实现。ArrayBlockingQueue 一旦创建，容量不能改变。其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。

ArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，当 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 ArrayBlockingQueue，可采用如下代码：

```html
private static ArrayBlockingQueue<Integer> blockingQueue = new ArrayBlockingQueue<Integer>(10,true);
```

LinkedBlockingQueue

**LinkedBlockingQueue** 底层基于**单向链表**实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用，同样满足 FIFO 的特性，与 ArrayBlockingQueue 相比起来具有更高的吞吐量，为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存。通常在创建 LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于 Integer.MAX_VALUE。

PriorityBlockingQueue

**PriorityBlockingQueue** 是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现 `compareTo()` 方法来指定元素排序规则，或者初始化时通过构造器参数 `Comparator` 来指定排序规则。

PriorityBlockingQueue 并发控制采用的是 **ReentrantLock**，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，**如果空间不够的话会自动扩容**）。

简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。

ConcurrentSkipListMap

对于一个单链表，即使链表是有序的，如果我们想要在其中查找某个数据，也只能从头到尾遍历链表，这样效率自然就会很低，跳表就不一样了。跳表是一种可以用来快速查找的数据结构，有点类似于平衡树。它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整。而对跳表的插入和删除只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，你会需要一个全局锁来保证整个平衡树的线程安全。而对于跳表，你只需要部分锁即可。这样，在高并发环境下，你就可以拥有更好的性能。而就查询的性能而言，跳表的时间复杂度也是 **O(logn)** 所以在并发数据结构中，JDK 使用跳表来实现一个 Map。

跳表的本质是同时维护了多个链表，并且链表是分层的，

最低层的链表维护了跳表内所有的元素，每上面一层链表都是下面一层的子集。

跳表内的所有链表的元素都是排序的。查找时，可以从顶级链表开始找。一旦发现被查找的元素大于当前链表中的取值，就会转入下一层链表继续找。这也就是说在查找过程中，搜索是跳跃式的。

**跳表是一种利用空间换时间的算法。**

使用跳表实现 Map 和使用哈希算法实现 Map 的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。JDK 中实现这一数据结构的类是 ConcurrentSkipListMap。

### 51.java 并发队列 BlockingQueue

## 阻塞队列

### 1.阻塞队列

队列是一种只允许在表的前端进行删除操作，而在表的后端进行插入操作的线性表。阻塞队列和一般队列的不同之处在于阻塞队列是“阻塞”的，这里的阻塞指的是操作队列的线程的一种状态。在阻塞队列中，线程阻塞有如下两种情况。

- 消费者阻塞：在队列为空时，消费者端的线程都会被自动阻塞（挂起），直到有数据放入队列，消费者线程会被自动唤醒并消费数据
- 生产者阻塞：在队列已满且没有可用空间时，生产者端的线程都会被自动阻塞（挂起），直到队列中有空的位置腾出，线程会被自动唤醒并生产数据。

### 2.阻塞队列的主要操作

阻塞队列的主要操作有插入操作和移除操作。

| 方法类型 | 抛出异常  | 特殊值   | 阻塞   | 超时               |
| -------- | --------- | -------- | ------ | ------------------ |
| 插入     | add(e)    | offer(e) | put(e) | offer(e,time,unit) |
| 移除     | remove()  | poll()   | take() | poll(time,unit)    |
| 检查     | element() | peek()   | 不可用 | 不可用             |

- 抛出异常：抛出一个异常；
- 特殊值：返回一个特殊值（null或false,视情况而定）
- 则塞：在成功操作之前，一直阻塞线程
- 超时：放弃前只在最大的时间内阻塞

插入操作

（1）public abstract boolean add(E paramE)：将指定的元素插入队列中，在成功时返回true，如果当前没有可用的空间，则抛出IllegalStateException。如果该元素是null，则抛出NullPointerException异常。

（2）public abstract boolean offer(E paramE)：将指定的元素插入队列中，在成功时返回true，如果当前没有可用的空间，则返回false。

（3）offer(E o, long timeout, TimeUnit unit)：将指定的元素插入队列中，可以设定等待的时间，如果在设定的等待时间内仍不能向队列中加入元素，则返回false。

（4）public abstract void put(E paramE) throws InterruptedException：将指定的元素插入队列中，如果队列已经满了，则阻塞、等待可用的队列空间的释放，直到有可用的队列空间释放且插入成功为止。

获取数据操作

（1）poll(time)：取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null;

（2）poll(long timeout, TimeUnit unit)：取走队列队首的对象，如果在指定的时间内队列有数据可取，则返回队列中的数据，否则等待一定时间，在等待超时并且没有数据可取时，返回null。

（3）take()：取走队列队首的对象，如果队列为空，则进入阻塞状态等待，直到队列有新的数据被加入，再及时取出新加入的数据。

（4）drainTo(Collection collection)：一次性从队列中批量获取所有可用的数据对象，同时可以指定获取数据的个数，通过该方法可以提升获取数据的效率，避免多次频繁操作引起的队列锁定。

### 3.Java中的阻塞队列实现

Java中的阻塞队列有：ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、DelayQueue、SynchronousQueue、LinkedTransferQueue、LinkedBlockingDeque。

- ArrayBlockingQueue：基于数组结构实现的有界阻塞队列
- LinkedBlockingQueue：基于链表结构实现的有界阻塞队列
- PriorityBlockingQueue：基于优先级排序的无界阻塞队列
- DelayQueue：基于优先级队列实现的无界阻塞队列
- SynchronousQueue：基于控制互斥操作的阻塞队列
- LinkedTransferQueue：基于链表结构实现的无界阻塞队列
- LinkedBlockingDeque：基于链表结构实现的双向阻塞队列

ArrayBlockingQueue（公平、非公平）

ArrayBlockingQueue是基于数组实现的有界阻塞队列。ArrayBlockingQueue队列按照先进先出原则对元素进行排序，在默认情况下不保证元素操作的公平性。

队列操作的公平性指在生产者线程或消费者线程发生阻塞后再次被唤醒时，按照阻塞的先后顺序操作队列，即先阻塞的生产者线程优先向队列中插入元素，先阻塞的消费者线程优先从队列中获取元素。因为保证公平性会降低吞吐量，所以如果要处理的数据没有先后顺序，则对其可以使用非公平处理的方式。我们可以通过以下代码创建一个公平或者非公平的阻塞队列：

```html
//大小为1000的公平队列
final ArrayBlockingQueue fairQueue=new ArrayBlockingQueue(1000,true);
//大小为1000的非公平队列
final ArrayBlockingQueue fairQueue=new ArrayBlockingQueue(1000,false);
```

LinkedBlockingQueue（两个独立锁提高并发）

LinkedBlockingQueue是基于链表实现的阻塞队列，同ArrayListBlockingQueue类似，此队列按照先进先出原则对元素进行排序；LinkedBlockingQueue对生产者端和消费者端分别采用了两个独立的锁来控制数据同步，我们可以将队列头部的锁理解为写锁，将队列尾部的锁理解为读锁，因此生产者和消费者可以基于各自独立的锁并行地操作队列中的数据，队列的并发性能较高。

LinkedBlockingQueue会默认一个类似无限大小的容量（Integer.MAX_VALUE）。

具体用法如下：

```html
final LinkedBlockingQueue linkQueue=new LinkedBlockingQueue(100);
```

PriorityBlockingQueue（compareTo排序实现优先）

PriorityBlockingQueue是一个支持优先级的无界队列。元素在默认情况下采用自然顺序升序排列。可以自定义实现compareTo方法来指定元素进行排序规则，或者在初始化PriorityBlockingQueue时指定构造参数Comparator来实现对元素的排序。注意：如果两个元素的优先级相同，则不能保证该元素的存储和访问顺序。

DelayQueue（缓存失效、定时任务）

DelayQueue是一个支持延时获取元素的无界阻塞队列，在队列底层使用PriorityQueue实现。DelayQueue队列中的元素必须实现Delayed接口，该接口定义了在创建元素时该元素的延迟时间，在内部通过为每个元素的操作加锁来保障数据的一致性。只有在延迟时间到后才能从队列中提取元素。我们可以将DelayQueue运用于以下场景中。

- 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素，则表示缓存的有效期到了。
- 定时任务调度：使用DelayQueue保存即将执行的任务和执行时间，一旦从DelayQueue中获取元素，就表示任务开始执行，Java中的TimerQueue就是使用DelayQueue实现的。

在具体使用时，延迟对象必须先实现Delayed类并实现其getDelay方法和compareTo方法，才可以在延迟队列中使用。

SynchronousQueue（不存储数据、可用于传递数据）

SynchronousQueue是一个不存储元素的阻塞队列。SynchronousQueue中的每个put操作都必须等待一个take操作完成，否则不能继续添加元素。

我们可以将SynchronousQueue看作一个“快递员”，它负责把生产者线程的数据直接传递给消费者线程，非常适用于传递型场景，比如将在一个线程中使用的数据传递给另一个线程使用。SynchronousQueue的吞吐量高于LinkedBlockingQueue和ArrayBlockingQueue。

LinkedTransferQueue

LinkedTransferQueue是基于链表结构实现的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了transfer、tryTransfer和tryTransfer(E e, long timeout, TimeUnit unit)方法。

- transfer方法：如果当前有消费者正在等待接收元素，transfer方法就会直接把生产者传入的元素投递给消费者并返回true。如果没有消费者在等待接收元素，transfer方法就会将元素存放在队列的尾部（tail）节点，直到该元素被消费后才返回。
- tryTransfer方法：首先尝试能否将生产者传入的元素直接传给消费者，如果没有消费者等待接收元素，则返回false。和transfer方法的区别是，无论消费者是否接收元素，tryTransfer方法都立即返回，而transfer方法必须等到元素被消费后才返回。
- tryTransfer(E e, long timeout, TimeUnit unit)方法：首先尝试把生产者传入的元素直接传给消费者，如果没有消费者，则等待指定的时间，在超时后如果元素还没有被消费，则返回false，否则返回true。

LinkedBlockingDeque

LinkedBlockingDeque是基于链表结构实现的双向阻塞队列，可以在队列的两端分别执行插入和移出元素操作。这样，在多线程同时操作队列时，可以减少一半的锁资源竞争，提高队列的操作效率。

LinkedBlockingDeque相比其他阻塞队列，多了addFirst、addLast、offerFirst、offerLast、peekFirst、peekLast等方法。以First结尾的方法表示在队列头部执行插入（add）、获取（peek）、移除（offer）操作；以Last结尾的方法表示在队列的尾部执行插入、获取、移除操作。

在初始化LinkedBlockingDeque时，可以设置队列的大小以防止内存溢出，双向阻塞队列也常被用于工作窃取模式。

### 4.什么是双端队列？

在上面，我们看到的 LinkedBlockingQueue、ArrayBlockingQueue、PriorityBlockingQueue、SynchronousQueue 等，都是阻塞队列。

而 ArrayDeque、LinkedBlockingDeque 就是双端队列，类名以 Deque 结尾。

- 正如阻塞队列适用于生产者消费者模式，双端队列同样适用与另一种模式，即

  工作密取

  。在生产者-消费者设计中，所有消费者共享一个工作队列，而在工作密取中，每个消费者都有各自的双端队列。

  - 如果一个消费者完成了自己双端队列中的全部工作，那么他就可以从其他消费者的双端队列末尾秘密的获取工作。具有更好的可伸缩性，这是因为工作者线程不会在单个共享的任务队列上发生竞争。
  - 在大多数时候，他们都只是访问自己的双端队列，从而极大的减少了竞争。当工作者线程需要访问另一个队列时，它会从队列的尾部而不是头部获取工作，因此进一步降低了队列上的竞争。

- 适用于：网页爬虫等任务中

### 5.简述 ConcurrentLinkedQueue 和 LinkedBlockingQueue 的用处和不同之处？

在 Java 多线程应用中，队列的使用率很高，多数生产消费模型的首选数据结构就是队列(先进先出)。

Java 提供的线程安全的 Queue 可以分为

- 阻塞队列，典型例子是 LinkedBlockingQueue 。适用阻塞队列的好处：多线程操作共同的队列时不需要额外的同步，另外就是队列会自动平衡负载，即那边（生产与消费两边）处理快了就会被阻塞掉，从而减少两边的处理速度差距。
- 非阻塞队列，典型例子是 ConcurrentLinkedQueue 。当许多线程共享访问一个公共集合时，`ConcurrentLinkedQueue` 是一个恰当的选择。

具体的选择，如下：

- LinkedBlockingQueue 多用于任务队列。
  - 单生产者，单消费者
  - 多生产者，单消费者
- ConcurrentLinkedQueue 多用于消息队列。
  - 单生产者，多消费者
  - 多生产者，多消费者

### 35.阻塞队列的原理

阻塞队列的原理很简单，利用了Lock锁的多条件（Condition）阻塞控制。

首先是构造器，除了初始化队列的大小和是否是公平锁之外，还对同一个锁（lock）初始化了两个监视器，分别是notEmpty和notFull。这两个监视器的作用目前可以简单理解为标记分组，当该线程是put操作时，给他加上监视器notFull,标记这个线程是一个生产者；当线程是take操作时，给他加上监视器notEmpty，标记这个线程是消费者。

put操作

总结put的流程：

1. 所有执行put操作的线程竞争lock锁，拿到了lock锁的线程进入下一步，没有拿到lock锁的线程自旋竞争锁。
2. 判断阻塞队列是否满了，如果满了，则调用await方法阻塞这个线程，并标记为notFull（生产者）线程，同时释放lock锁,等待被消费者线程唤醒。
3. 如果没有满，则调用enqueue方法将元素put进阻塞队列。注意这一步的线程还有一种情况是第二步中阻塞的线程被唤醒且又拿到了lock锁的线程。
4. 唤醒一个标记为notEmpty（消费者）的线程。

take操作

take操作和put操作的流程是类似的，总结一下take操作的流程：

1. 所有执行take操作的线程竞争lock锁，拿到了lock锁的线程进入下一步，没有拿到lock锁的线程自旋竞争锁。
2. 判断阻塞队列是否为空，如果是空，则调用await方法阻塞这个线程，并标记为notEmpty（消费者）线程，同时释放lock锁,等待被生产者线程唤醒。
3. 如果没有空，则调用dequeue方法。注意这一步的线程还有一种情况是第二步中阻塞的线程被唤醒且又拿到了lock锁的线程。
4. 唤醒一个标记为notFull（生产者）的线程。

**注意**

1. put和tack操作都需要**先获取锁**，没有获取到锁的线程会被挡在第一道大门之外自旋拿锁，直到获取到锁。
2. 就算拿到锁了之后，也**不一定**会顺利进行put/take操作，需要判断**队列是否可用**（是否满/空），如果不可用，则会被阻塞，**并释放锁**。
3. 在第2点被阻塞的线程会被唤醒，但是在唤醒之后，**依然需要拿到锁**才能继续往下执行，否则，自旋拿锁，拿到锁了再while判断队列是否可用（这也是为什么不用if判断，而使用while判断的原因）。

## volatile

### 1.volatile关键字的作用（变量可见性、禁止重排序）

Java除了使用了synchronized保证变量的同步，还使用了稍弱的同步机制，即volatile变量。volatile也用于确保将变量的更新操作通知到其他线程。

volatile变量具备两种特性：一种是保证该变量对所有线程可见，在一个线程修改了变量的值后，新的值对于其他线程是可以立即获取的；一种是volatile禁止指令重排，即volatile变量不会被缓存在寄存器中或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。

因为在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatile变量是一种比synchronized关键字更轻量级的同步机制。volatile主要适用于一个变量被多个线程共享，多个线程均可针对这个变量执行赋值或者读取的操作。

在有多个线程对普通变量进行读写时，每个线程都首先需要将数据从内存中复制变量到CPU缓存中，如果计算机有多个CPU，则线程可能都在不同的CPU中被处理，这意味着每个线程都需要将同一个数据复制到不同的CPU Cache中，这样在每个线程都针对同一个变量的数据做了不同的处理后就可能存在数据不一致的情况。

如果将变量声明为volatile，JVM就能保证每次读取变量时都直接从内存中读取，跳过CPU Cache这一步，有效解决了多线程数据同步的问题。

需要说明的是，volatile关键字可以严格保障变量的单次读、写操作的原子性，但并不能保证像i++这种操作的原子性，因为i++在本质上是读、写两次操作。volatile在某些场景下可以代替synchronized，但是volatile不能完全取代synchronized的位置，只有在一些特殊场景下才适合使用volatile。比如，必须同时满足下面两个条件才能保证并发环境的线程安全。

- 对变量的写操作不依赖于当前值（比如i++），或者说是单纯的变量赋值（boolean flag=true）。
- 该变量没有被包含在具有其他变量的不变式中，也就是说在不同的volatile变量之间不能互相依赖，只有在状态真正独立于程序内的其他内容时才能使用volatile。

volatile关键字的使用方法比较简单，直接在定义变量时加上volatile关键字即可：

```html
volatile boolean flag=true;
```

### 2.synchronized和 volatile关键字对比

volatile本质是在告诉 jvm 当前变量在寄存器（工作内存）中的值是不确定的，需 要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 1.volatile 仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的 2.volatile 仅能实现变量的修改可见性，并不能保证原子性；synchronized则可以保证变量的修改可见性和原子性 3.volatile 不会造成线程的阻塞；synchronized可能会造成线程的阻塞。 4.volatile 标记的变量不会被编译器优化；synchronized标记的变量可以被编译 器优化

### 3.volatile

volatile主要是用来解决多线程场景下变量的可见性以及顺序性。 如何实现可见性呢？如果在一个线程中修改了被volatile修饰的变量值，那么会让其他线程的工作内存中的变量值失效，在下一次需要用到该变量的时候，重新去主存中去加载最新的变量值。

比如有一个data变量被volatile修饰，线程1将data的值改变，并将data修改后的值刷回主内存。它就会去将其他线程的工作内存中原本的加载的data旧值全部失效，在其他线程想使用自身工作内存中的内存值时，会发现已经失效，必须从主内存中去读取data值，这时读取的就是data修改后的值了。

### 4.被volatile修饰之后的变量就不会被缓存到CUP级别的缓存中了？

都会缓存到cpu缓存中的, 被volatile修饰后, 其他变量的的修改, 通过mesi协议和内存屏障作用, 当前缓存的变量会被置为失效, 这样cpu核读取变量发现是已失效, 会重新从内存中获取

### 5.volatile底层是如何保证可见性的？

lock指令：volatile保证可见性

对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改 。

如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了 。

### 6.happens-before

我们无法就所有场景来规定某个线程修改的变量何时对其他线程可见，但是我们可以指定某些规则，这规则就是happens-before，从JDK 5 开始，JMM就使用happens-before的概念来阐述多线程之间的内存可见性。

如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。

happens-before原则定义如下：

1. 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。
2. 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。

happens-before原则规则：

1. 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作；
2. 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作；
3. volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作；
4. 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C； i = 1; //线程A执行 j = i ; //线程B执行
5. 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作；
6. 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；
7. 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；
8. 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始；

### 7.volatile的内存语义

当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值立即刷新到主内存中。 当读一个volatile变量时，JMM会把该线程对应的本地内存设置为无效，直接从主内存中读取共享变量。

所以volatile的写内存语义是直接刷新到主内存中，读的内存语义是直接从主内存中读取。

重排序规则：

1. 如果第一个操作为volatile读，则不管第二个操作是啥，都不能重排序。这个操作确保volatile读之后的操作不会被编译器重排序到volatile读之前；
2. 当第二个操作为volatile写，则不管第一个操作是啥，都不能重排序。这个操作确保volatile写之前的操作不会被编译器重排序到volatile写之后；
3. 当第一个操作volatile写，第二操作为volatile读时，不能重排序。

volatile的底层实现是通过插入内存屏障，但是对于编译器来说，发现一个最优布置来最小化插入内存屏障的总数几乎是不可能的，所以，JMM采用了保守策略。如下： 在每一个volatile写操作前面插入一个StoreStore屏障 在每一个volatile写操作后面插入一个StoreLoad屏障 在每一个volatile读操作后面插入一个LoadLoad屏障 在每一个volatile读操作后面插入一个LoadStore屏障

StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作都已经刷新到主内存中。 StoreLoad屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。 LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。 LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。

### 8.**说一下volatile关键字对原子性、可见性以及有序性的保证？**

在volatile变量写操作的前面会加入一个`Release`屏障，然后在之后会加入一个`Store`屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作。

在volatile变量读操作的前面会加入一个`Load`屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他 处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；在之后会加入一个`Acquire`屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序。 与volatie读写内存屏障对比一下，是类似的意思。 `Acquire屏障`其实就是`LoadLoad屏障 + LoadStore屏障`， `Release屏障`其实就是`StoreLoad屏障 + StoreStore屏障`

### 9.操作系统语义

计算机在运行程序时，每条指令都是在 CPU 中执行的，在执行过程中势必会涉及到数据的读写。我们知道程序运行的数据是存储在主存中，这时就会有一个问题，读写主存中的数据没有 CPU 中执行指令的速度快，如果任何的交互都需要与主存打交道则会大大影响效率，所以就有了 **CPU 高速缓存**。CPU高速缓存为某个CPU独有，只与在该CPU运行的线程有关。

有了 CPU 高速缓存虽然解决了效率问题，但是它会带来一个新的问题：**数据一致性**。在程序运行中，会将运行所需要的数据复制一份到 CPU 高速缓存中，在进行运算时 CPU 不再也主存打交道，而是直接从高速缓存中读写数据，只有当运行结束后，才会将数据刷新到主存中。

```html
`i = i + 1;`
```

当线程运行这段代码时，首先会从主存中读取 `i` 的值( 假设此时 `i = 1` )，然后复制一份到 CPU 高速缓存中，然后 CPU 执行 `+ 1` 的操作（此时 `i = 2`），然后将数据 `i = 2` 写入到告诉缓存中，最后刷新到主存中。

两个线程从主存中读取 `i` 的值( 假设此时 `i = 1` )，到各自的高速缓存中，然后线程 A 执行 `+1` 操作并将结果写入高速缓存中，最后写入主存中，此时主存 `i = 2` 。线程B做同样的操作，主存中的 `i` 仍然 `=2` 。所以最终结果为 2 并不是 3 。这种现象就是**缓存一致性问题**。

**解决缓存一致性方案有两种**：

1. 通过在总线加 LOCK# 锁的方式
2. 通过缓存一致性协议

第一种方案， 存在一个问题，它是采用一种独占的方式来实现的，即总线加 LOCK# 锁的话，只能有一个 CPU 能够运行，其他 CPU 都得阻塞，效率较为低下。

第二种方案，缓存一致性协议（MESI 协议），它确保每个缓存中使用的共享变量的副本是一致的。其核心思想如下：当某个 CPU 在写数据时，如果发现操作的变量是共享变量，则会通知其他 CPU 告知该变量的缓存行是无效的，因此其他 CPU 在读取该变量时，发现其无效会重新从主存中加载数据。

### 10.Java内存模型

原子性

原子性就像数据库里面的事务一样，他们是一个团队，同生共死。

```html
i = 0;  // <1>
j = i ;  // <2>
i++;  // <3>
i = j + 1; // <4>
```

**其实只有 1 才是原子操作，其余均不是**。

- `<1>`：在 Java 中，对基本数据类型的变量和赋值操作都是原子性操作。
- `<2>`：包含了两个操作：读取 `i`，将 `i` 值赋值给 `j` 。
- `<3>`：包含了三个操作：读取 `i` 值、`i + 1` 、将 `+1` 结果赋值给 `i` 。
- `<4>`：同 `<3>` 一样

在单线程环境下我们可以认为整个步骤都是原子性操作，但是在多线程环境下则不同，Java 只保证了基本数据类型的变量和赋值操作才是原子性的（**注：在 32 位的 JDK 环境下，对 64 位数据的读取不是原子性操作，例如：long、double**）。要想在多线程环境下保证原子性，则可以通过锁、`synchronized` 来确保。

另外，`volatile` 是**无法保证**复合操作的原子性。

可见性

可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。

在多线程环境下，一个线程对共享变量的操作对其他线程是不可见的。

**Java提供了 volatile 来保证可见性。**

当一个变量被 `volatile` 修饰后，表示着线程本地内存无效。当一个线程修改共享变量后他会**立即**被更新到主内存中；当其他线程读取共享变量时，它会直接从主内存中读取。

当然，`synchronize` 和锁都可以保证可见性。

有序性

有序性：即程序执行的顺序按照代码的先后顺序执行。

**Java 提供 volatile 来保证一定的有序性**。

### 11.volatile 原理

`volatile` 可以保证线程可见性且提供了一定的有序性，但是无法保证原子性。在 JVM 底层，`volatile` 是采用“内存屏障”来实现的。

在执行程序时为了提高性能，编译器和处理器通常会对指令做重排序：

1. 编译器重排序。编译器在**不改变单线程**程序语义的前提下，可以重新安排语句的执行顺序。
2. 处理器重排序。如果**不存在数据依赖性**，处理器可以改变语句对应机器指令的执行顺序。

指令重排序对单线程没有什么影响，他不会影响程序的运行结果，但是会影响多线程的正确性。

**happens-before**：**该原则保证了程序的“有序性”，它规定如果两个操作的执行顺序无法从 happens-before 原则中推到出来，那么他们就不能保证有序性，可以随意进行重排序**。其定义如下：

- 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作，happens-before 于书写在后面的操作。
- 锁定规则：一个 unLock 操作，happens-before 于后面对同一个锁的 lock 操作。
- volatile 变量规则：对一个变量的写操作，happens-before 于后面对这个变量的读操作。
- 传递规则：如果操作 A happens-before 操作 B，而操作 B happens-before 操作C，则可以得出，操作 A happens-before 操作C
- 线程启动规则：Thread 对象的 start 方法，happens-before 此线程的每个一个动作。
- 线程中断规则：对线程 interrupt 方法的调用，happens-before 被中断线程的代码检测到中断事件的发生。
- 线程终结规则：线程中所有的操作，都 happens-before 线程的终止检测，我们可以通过Thread.join() 方法结束、Thread.isAlive() 的返回值手段，检测到线程已经终止执行。
- 对象终结规则：一个对象的初始化完成，happens-before 它的 finalize() 方法的开始

### 12.JVM 是如何禁止重排序的？

**加入volatile 关键字时，会多出一个 lock 前缀指令**。lock 前缀指令，其实就相当于一个内存屏障。内存屏障是一组处理指令，用来实现对内存操作的顺序限制。`volatile` 的底层就是通过内存屏障来实现的。

### 13.**可以创建 volatile 数组吗?**

Java 中可以创建 `volatile` 类型数组，不过只是一个指向数组的引用，而不是整个数组。如果改变引用指向的数组，将会受到 `volatile` 的保护，但是如果多个线程同时改变数组的元素，`volatile` 标示符就不能起到之前的保护作用了。

同理，对于 Java POJO 类，使用 `volatile` 修饰，只能保证这个引用的可见性，不能保证其内部的属性。

### 14.**volatile 变量和 atomic 变量有什么不同？**

- `volatile` 变量，可以确保先行关系，即写操作会发生在后续的读操作之前，但它并不能保证原子性。例如用 `volatile` 修饰 `count` 变量，那么 `count++` 操作就不是原子性的。
- AtomicInteger 类提供的 atomic 方法，可以让这种操作具有原子性。例如 `#getAndIncrement()` 方法，会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。

### 15.**volatile 能使得一个非原子操作变成原子操作吗?**

一个典型的例子是在类中有一个 `long` 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为 `volatile` 。为什么？因为 Java 中读取 `long` 类型变量不是原子的，需要分成两步，如果一个线程正在修改该 `long` 变量的值，另一个线程可能只能看到该值的一半（前 32 位）。但是对一个 `volatile` 型的 `long` 或 `double` 变量的读写是原子。

一种实践是用 `volatile` 修饰 `long` 和 `double` 变量，使其能按原子类型来读写。`double` 和 `long` 都是64位宽，因此对这两种类型的读是分为两部分的，第一次读取第一个 32 位，然后再读剩下的 32 位，这个过程不是原子的，但 Java 中 `volatile` 型的 `long` 或 `double` 变量的读写是原子的。

### 16.**什么场景下可以使用 volatile 替换 synchronized ？**

1. 只需要保证共享资源的可见性的时候可以使用 `volatile` 替代，`synchronized` 保证可操作的原子性一致性和可见性。
2. `volatile` 适用于新值不依赖于旧值的情形。
3. 1 写 N 读。
4. 不与其他变量构成不变性条件时候使用 `volatile` 。

### 17.**volatile 修饰符的有过什么实践？**

1. 一种实践是用 volatile 修饰 long 和 double 变量，使其能按原子类型来读写。double 和 long 都是64位宽，因此对这两种类型的读是分为两部分的，第一次读取第一个 32 位，然后再读剩下的 32 位，这个过程不是原子的，但 Java 中 volatile 型的 long 或 double 变量的读写是原子的。
2. volatile 修复符的另一个作用是提供内存屏障（memory barrier），例如在分布式框架中的应用。简单的说，就是当你写一个 volatile 变量之前，Java 内存模型会插入一个写屏障（write barrier），读一个 volatile 变量之前，会插入一个读屏障（read barrier）。意思就是说，在你写一个 volatile 域时，能保证任何线程都能看到你写的值，同时，在写之前，也能保证任何数值的更新对所有线程是可见的，因为内存屏障会将其他所有写的值更新到缓存。

### 18.重排序

在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类：

1、**编译器优化的重排序**。编译器在不改变**单线程**程序语义的前提下，可以重新安排语句的执行顺序。

2、**指令级并行的重排序**。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在**数据依赖性**，处理器可以改变语句对应机器指令的执行顺序。

3、**内存系统的重排序**。由于处理器使用缓存和读／写缓冲区，这使得**加载和存储**操作看上去可能是在乱序执行。

上面的这些重排序都可能导致多线程程序出现内存可见性问题。

- 对于编译器，JMM 的编译器重排序规则会**禁止特定类型的编译器重排序**（不是所有的编译器重排序都要禁止）。
- 对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，**插入特定类型的内存屏障指令**，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。

JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。

### 19.处理器重排序

现代的处理器使用**写缓冲区**来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！

由于**写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致**。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。

### 20.什么是内存屏障？

内存屏障，又称内存栅栏，是一组处理器指令，用于实现对内存操作的顺序限制。

**内存屏障为何重要？**

对主存的一次访问一般花费硬件的数百次时钟周期。处理器通过缓存（caching）能够从数量级上降低内存延迟的成本这些缓存为了性能重新排列待定内存操作的顺序。也就是说，程序的读写操作不一定会按照它要求处理器的顺序执行。当数据是不可变的，同时/或者数据限制在线程范围内，这些优化是无害的。如果把这些优化与对称多处理（symmetric multi-processing）和共享可变状态（shared mutable state）结合，那么就是一场噩梦。

当基于共享可变状态的内存操作被重新排序时，程序可能行为不定。一个线程写入的数据可能被其他线程可见，原因是数据写入的顺序不一致。适当的放置内存屏障，通过强制处理器顺序执行待定的内存操作来避免这个问题。

### 21.volatile关键字的作用？

一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语 义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他 线程来说是立即可见的。 禁止进行指令重排序。 volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读 取；synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的。 volatile仅能实现变量的修改可见性，并不能保证原子性；synchronized则可以保证变量的修改可 见性和原子性。 volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化。

### 22.java的内存模型是什么？能结合内存模型说一下volatile的工作原理吗？指令重排序，内存栅栏，happen-before等概念是指的什么意思？

1.**操作系统内存模型**

先聊下内存模型这个事儿。

操作系统这个层面上，所有指令都是cpu里执行的，但是执行指令的时候肯定是要读写数据的，数据一般都放主存里，但是直接读写内存速度不够快，所以一般会在cpu里放高速缓存，就是说主存的数据会放一份在cpu内部的高速缓存中，这样cpu指令直接对cpu内部的高速缓存里的数据读写就可以了。

举个例子，i++

但是这样就会带来数据一致性的问题，因为每次cpu都是从内存读取数据到cpu高速缓存，操作完以后，再写回主存里。比如i++，这个操作吧，一般就是从主存读取i（i = 1）到cpu高速缓存里，然后对这个值累加，i = 2，先写入高速缓存，接着写回主存。

但是如果是多线程，可不是这么玩儿的，多线程的时候，在多cpu场景下，可能两个线程会将主存的i = 1都读到高速缓存里，然后都累加，i = 2，接着写回自己的高速缓存，然后刷回主存，此时就会导致主存里的数据是i = 2，而不是我们期望的i = 3。

可以用总线lock锁机制，这个上一讲还提到过，但是这个机制其实很重，会导致只有一个cpu可以操作主存，别的cpu都没法操作了。所以最新的cpu里，都不会用总线锁了，一般都会用优化的缓存一致性协议（MSI），就是某个cpu写数据的时候，发现写的是共享变量，会通知其他cpu这个数据在他们内部的高速缓存是无效的，让其他cpu读这个变量的时候从写数据的那个cpu缓存里读取，保证大家的缓存是一致的。当然，也有一种实现，是说修改变量的线程将数据从自己缓存立即刷新到内存，然后其他cpu发现自己的缓存行失效，如果要读写数据的时候重新从内存里加载到缓存里来。

并发问题里，有3个至关重要的点，原子性、可见性、有序性

**（1）原子性**

这个很简单，就是说比如i++这个操作吧，根据操作系统内存模型，是需要先从主存中读取数据到cpu高速缓存中，然后给它加1后写入高速缓存，最后再从高速缓存刷入主存，这个过程是有很多个步骤的。如果这个过程能保证绝对可以执行成功，不会出现任何意外，那么就是原子性。

你可以保证并发操作的原子性，无论多少个线程怎么i++，最后都是不断累加1的，数据不会错，就是原子性的

**（2）可见性**

```html
//线程1
int i = 1;
i = 5; -> 还放在高速缓存里，还没写入主存
 
//线程2
int j = i; -> 从主存加载i的值，还是1，将i = 1的值，赋值给了j，j = 1
```

比如上面的代码，如果两个线程在不同的cpu运行，此时可能会出现说，线程1将i的值赋值为1，然后写入了cpu高速缓存，同时可能写入了主存；接着将i再次更新为5，然后写入了cpu高速缓存，但是还没来得及写入主存；此时线程2要将i的值赋值给j，那么就会从主存中读取i的值，此时还是1呢，所以就会将1这个值赋值给j

这就是不可见了，因为线程1做的i = 5的修改停留在cpu高速缓存里，还没来得及写入主存，导致线程2没看到。

**（3）有序性**

同时还有一个问题是指令重排序，编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序，就是说比如下面的代码

```html
//线程1:
prepare();   // 准备资源
flag = true;         

//线程2:
while(!flag){
  Thread.sleep();
}
execute(); // 基于准备好的资源执行操作
```

重排序之后，让flag = true先执行了，会导致线程2直接跳过while等待，执行某段代码，结果prepare()方法还没执行，资源还没准备好呢，此时就会导致代码逻辑出现异常。

2.**java内存模型**

Java内存模型，规定的东西跟操作系统是相关的，规定了所有变量的值都在主存中，每个线程都有自己的工作内存（类似前面说的cpu高速缓存），线程对变量的操作都必须在工作内存中完成，是不能直接对主存进行操作的，而且每个线程不能访问其他线程的工作内存，其实在上面都能找到对应的概念。

所以先熟悉了操作系统内存模型，再看java内存模型，就能看得懂了。

另外，你要明白一个线程对工作内存和主存的操作模型，read（从主存读取），load（将主存读取到的值写入工作内存），use（从工作内存读取数据来计算），assign（将计算好的值重新赋值到工作内存中），store（将工作内存数据写入主存），write（将store过去的变量值赋值给主存中的变量）

**（1）原子性**

举个例子，i = 1，必须线程先在自己的工作内存中将i赋值为1，然后再写入主存中，不能直接修改主存的。而且java中仅仅保证i = 1这种基本数字类型的赋值操作，是原子的，如果是什么i = y，i++这种，都需要先将某个值从主存读入工作内存，操作过后再写回主存，都不是原子的。

voaltile是不能保证原子性的，这个稍后来分析。

原子性，实际上只能靠synchronized和lock来解决，就是仅仅在一段时间内允许一个线程获取锁，然后执行某段代码，操作完之后强制将工作内存数据刷回主存，其他线程获取锁之后立马可以看到数据然后使用。

**（2）可见性**

volatile是用来保证可见性的，就是加了volatile关键字的变量，你在修改之后，会立即刷入主存，接着其他线程会感知到强制从主存读取最新的值，可以保证可见性。

**（3）有序性**

java中有一个happens-before原则：

程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作

锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作

volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作

传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C

线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作

线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生

线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行

对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始

上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。

但是如果没满足上面的规则，那么就可能会出现指令重排，就这个意思。这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。

3.**volatile的作用**

**（1）volatile对可见性的影响**

```html
volatile run = false;
// 线程1，先执行
run=true  // 已经变成1了   
// 线程2，后执行
// 读取到的还是0
while(!run) {
// 无限循环
} 
```

如果不用volatile，可能导致run变量的修改对其他线程是不可见的，所以上面的代码，不一定能保证说修改了run变量的值之后，理解让线程2跳出来

但是只要你加了volatile关键词，保证了每个线程对一个变量的修改，立即都是对其他线程是可见的，只要线程1一旦修改了run = true，然后理解就是其他线程会看到最新的值，然后while循环就会立即跳出

就是某个线程对一个变量先修改了值，然后另外一个线程去读取那个变量一定会看到最新的值，这个叫做可见性

有的时候我们会用上面的代码来让一个线程控制另外一个线程终止无限循环然后结束，但是如果直接这么搞可能就是有问题的，因为线程1读取了run到自己工作内存，然后依据工作内存中的run=true不断的循环；而线程2虽然将run的值修改为了false而且写入了主存，但是线程1可能确实就是没感知到，因为还是一直在依赖自己工作内存中的run = true在工作。

如果加了volatile关键字之后，可以保证的是，一个线程对数据的写操作，会立即同步到主存，其他线程也是直接从主存来读取的，就是忽略了cpu高速缓存的作用了。因为加了volatile之后，相当于是在cpu层面加了lock指令。这个lock指令，会基于缓存一致性协议来实现，让cpu写数据到高速缓存之后，立即同步到主存，同时其他cpu发现这个共享数据被修改之后，就会标记自己高速缓存中的那个数据失效了，那么如果要对这个失效数据进行修改的时候，会强制重新从内存里把这个数据读到自己缓存里来，再进行修改。

比如上面的场景，如果对i这个共享变量加了volatile关键字之后，线程1修改i++，会强制立即将工作内存中的值刷入主存，而且会导致线程2的工作内存中的i变量的缓存行无效，因为线程2的i变量缓存行无效了，此时就会重新从主存中读取run变量的最新值，就是i = 1。

所以说，volatile是可以保证共享变量的可见性的。

只要是对volatile变量并发操作

某个线程，先修改了值，立即刷回主存；如果另外一个线程此时再来读，会直接从主存里读到最新的值；如果值钱另外一个线程已经将这个变量的值读到自己的工作内存里去了，此时其他线程要从工作内存里读的时候，会发现那个变量的缓存行已经失效了，此时会重新从主存里加载最新的值

**（2）volatile对原子性的影响**

网上没有一篇文章对volatile关于原子性的事情说清楚了，其实坦白说，很多人完全不理解volatile的作用，所以出现了瞎用的情况，比如说下面的例子：

```html
volatile int i = 0
// 线程1
i++
// 线程2
i++
```

如果说你以为volatile是轻量级锁的人，很傻X，我个人是绝对不认可volatile是一把锁；可以保证多线程并发修改一个共享资源的话 -> JDK傻乎乎的搞了什么synchronized、ReentrantLock、Semaphore。

如果你以为加了个volatile就是可以保证万无一失，那你真的是too young too simple

如果某一时刻，两个线程都先后将这个i的值，i = 0加载到工作内存里来了，结果可能线程1都执行到use步骤了，此时已经将i = 0弄出来准备加1了；同时线程2在这个瞬间就完成了use、assign、store、write等步骤，i = 1，同时写回了主存，并且让线程1的工作内存缓存行失效了；但是然而有什么用呢？线程1此时不需要工作内存了，之前use已经从工作内存读取了数据，所以也同时将i加1变成了1，然后接着assign、store、write回了主存，所以相当于写了两次1回主存。

看明白了java内存模型，你就理解了所谓的可见性是啥，原子性是啥了。

可见性是说，你可能某一次读取是错的，但是下一次再次读取的时候，一定会发现失效的缓存行，重新从主存去read和load进行最新的值；原子性是说，你还是可能会出现某一次读取到的值是旧值，没人告诉你一定能保证100%没问题。

正是对这块的不理解，导致几乎没几个程序员能用好volatile关键字的。

**（3）volatile对有序性的影响**

volatile可以在一定程度上禁止部分指令重排序

```html
//线程1:
prepare();   // 准备资源
volatile flag = true; 
//线程2:
while(!flag){
  sleep()
}
execute(); // 基于准备好的资源执行操作
```

比如这个例子，如果用volatile来修饰flag变量，一定可以让prepare()指令在flag = true之前先执行，这就禁止了指令重排。因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。

看完了上面的东西，就可以最后说一说这几个概念了，指令重排已经说过了，编译器和指令器为了效率，有的时候会对指令重新排序，但是指令重排的时候一定会遵守happens-before这套规则，如果代码不符合happens-before规则，那么就可以随意进行指令重排，否则就要按照happens-before规则的顺序来走。

内存屏障，其实就是volatile关键字加了之后，就会加一个操作系统层面的lock指令，这就形成了一个所谓内存屏障，或者是内存栅栏吧。其实就是起到一个作用，一个是禁止指令重排，就是上面说的；另外就是说写的时候一定会强制刷主存；写完之后一定会导致其他cpu的高速缓存失效

**4. volatile的使用场景**

我觉得说到这里，大家基本上对于如何用volatile关键字就呼之欲出了吧

说白了，就看看上面那个例子，如果说，你可以忍受没有原子性，但是只要保证可见性就好，比如while(run)的时候，某一次run的值没读到最新的是ok的，但是只要下一次读一定能立马读到最新值就好了，那用volatile其实很合适。

如果你要的是i++这种复杂操作要实现线程并发安全，那要么是CAS的AtomicInteger这种，要么是synchronized或者lock加锁，因为只有加锁是能禁止别的线程执行这段操作的。用volatile是不靠谱的。

### 5.volatile 能使得一个非原子操作变成原子操作吗？

一个典型的例子是在类中有一个long 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为volatile。为什么？因

为Java 中读取long 类型变量不是原子的， 需要分成两步，如果一个线程正在修改该long 变量的值，另一个线程可能只能看到该值的一半（ 前32 位） 。但是对一

个volatile 型的long 或double 变量的读写是原子。

### 6.volatile 修饰符的有过什么实践？

一种实践是用volatile 修饰long 和double 变量，使其能按原子类型来读写。double 和long 都是64 位宽，因此对这两种类型的读是分为两部分的，第一次读取第

一个32 位，然后再读剩下的32 位，这个过程不是原子的，但Java 中volatile 型的long 或double 变量的读写是原子的。

volatile 修复符的另一个作用是提供内存屏障（ memory barrier）， 例如在分布式框架中的应用。简单的说，就是当你写一个volatile 变量之前，Java 内存模型

会插入一个写屏障（write barrier）， 读一个volatile 变量之前，会插入一个读屏障（ read barrier）。意思就是说，在你写一个volatile 域时，能保证任何线程都

能看到你写的值，同时，在写之前，也能保证任何数值的更新对所有线程是可见的， 因为内存屏障会将其他所有写的值更新到缓存。

### 7.volatile 类型变量提供什么保证？

volatile 变量提供顺序和可见性保证，例如，JVM 或者JIT 为了获得更好的性能会对语句重排序，但是volatile 类型变量即使在没有同步块的情况下赋值也不会与其

他语句重排序。volatile 提供happens-before 的保证， 确保一个线程的修改能对其他线程是可见的。某些情况下，volatile 还能提供原子性，如读64 位数据类

型，像long 和double 都不是原子的，但volatile 类型的double 和long 就是原子的。

### 18.重排序

什么是重排序？

计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排。

**为什么指令重排序可以提高性能？**

简单地说，每一个指令都会包含多个步骤，每个步骤可能使用不同的硬件。因此，**流水线技术**产生了，它的原理是指令1还没有执行完，就可以开始执行指令2，而不用等到指令1执行结束之后再执行指令2，这样就大大提高了效率。

但是，流水线技术最害怕**中断**，恢复中断的代价是比较大的，所以我们要想尽办法不让流水线中断。指令重排就是减少中断的一种技术。

我们分析一下下面这个代码的执行情况：

```html
a = b + c;
d = e - f ;
```

先加载b、c（**注意，即有可能先加载b，也有可能先加载c**），但是在执行add(b,c)的时候，需要等待b、c装载结束才能继续执行，也就是增加了停顿，那么后面的指令也会依次有停顿,这降低了计算机的执行效率。

为了减少这个停顿，我们可以先加载e和f,然后再去加载add(b,c),这样做对程序（串行）是没有影响的,但却减少了停顿。既然add(b,c)需要停顿，那还不如去做一些有意义的事情。

综上所述，**指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。**

指令重排一般分为以下三种：

- **编译器优化重排**

  编译器在**不改变单线程程序语义**的前提下，可以重新安排语句的执行顺序。

- **指令并行重排**

  现代处理器采用了指令级并行技术来将多条指令重叠执行。如果**不存在数据依赖性**(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序。

- **内存系统重排**

  由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。

**指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致**。所以在多线程下，指令重排序可能会导致一些问题。

### 19.顺序一致性模型

顺序一致性模型有两大特性：

- 一个线程中的所有操作必须按照程序的顺序（即Java代码的顺序）来执行。
- 不管程序是否同步，所有线程都只能看到一个单一的操作执行顺序。即在顺序一致性模型中，每个操作必须是**原子性的，且立刻对所有线程可见**。

对于未同步的多线程程序，JMM只提供**最小安全性**：线程读取到的值，要么是之前某个线程写入的值，要么是默认值，不会无中生有。

为了实现这个安全性，JVM在堆上分配对象时，首先会对内存空间清零，然后才会在上面分配对象（这两个操作是同步的）。

**JMM没有保证未同步程序的执行结果与该程序在顺序一致性中执行结果一致。因为如果要保证执行结果一致，那么JMM需要禁止大量的优化，对程序的执行性能会产生很大的影响。**

未同步程序在JMM和顺序一致性内存模型中的执行特性有如下差异： 1. 顺序一致性保证单线程内的操作会按程序的顺序执行；JMM不保证单线程内的操作会按程序的顺序执行。（因为重排序，但是JMM保证单线程下的重排序不影响执行结果） 2. 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。（因为JMM不保证所有操作立即可见） 3. JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读写操作都具有原子性。

### 20.happens-before

什么是happens-before?

一方面，程序员需要JMM提供一个强的内存模型来编写代码；另一方面，编译器和处理器希望JMM对它们的束缚越少越好，这样它们就可以最可能多的做优化来提高性能，希望的是一个弱的内存模型。

JMM考虑了这两种需求，并且找到了平衡点，对编译器和处理器来说，**只要不改变程序的执行结果（单线程程序和正确同步了的多线程程序），编译器和处理器怎么优化都行。**

而对于程序员，JMM提供了**happens-before规则**（JSR-133规范），满足了程序员的需求——**简单易懂，并且提供了足够强的内存可见性保证。**换言之，程序员只要遵循happens-before规则，那他写的程序就能保证在JMM中具有强的内存可见性。

JMM使用happens-before的概念来定制两个操作之间的执行顺序。这两个操作可以在一个线程以内，也可以是不同的线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证。

happens-before关系的定义如下： 1. 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 2. **两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序。**

总之，**如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的，不管它们在不在一个线程。**

天然的happens-before关系

在Java中，有以下天然的happens-before关系：

- 程序顺序规则：一个线程中的每一个操作，happens-before于该线程中的任意后续操作。
- 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
- volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
- 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。
- start规则：如果线程A执行操作ThreadB.start()启动线程B，那么A线程的ThreadB.start（）操作happens-before于线程B中的任意操作、
- join规则：如果线程A执行操作ThreadB.join（）并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。

重排序有两类，JMM对这两类重排序有不同的策略：

- 会改变程序执行结果的重排序，比如 A -> C，JMM要求编译器和处理器都不许禁止这种重排序。
- 不会改变程序执行结果的重排序，比如 A -> B，JMM对编译器和处理器不做要求，允许这种重排序。

### 21.volatitle

内存可见性

**内存可见性，指的是线程之间的可见性，当一个线程修改了共享变量时，另一个线程可以读取到这个修改后的值**。

重排序

为优化程序性能，对原有的指令执行顺序进行优化重新排序。重排序可能发生在多个阶段，比如编译重排序、CPU重排序等。

happens-before规则

是一个给程序员使用的规则，只要程序员在写代码的时候遵循happens-before规则，JVM就能保证指令在多线程之间的顺序性符合程序员的预期。

volatile的内存语义

在Java中，volatile关键字有特殊的内存语义。volatile主要有以下两个功能：

- 保证变量的**内存可见性**
- 禁止volatile变量与普通变量**重排序**（JSR133提出，Java 5 开始才有这个“增强的volatile内存语义”）

内存可见性

所谓内存可见性，指的是当一个线程对`volatile`修饰的变量进行**写操作**（比如step 2）时，JMM会立即把该线程对应的本地内存中的共享变量的值刷新到主内存；当一个线程对`volatile`修饰的变量进行**读操作**（比如step 3）时，JMM会把立即该线程对应的本地内存置为无效，从主内存中读取共享变量的值。

> 在这一点上，volatile与锁具有相同的内存效果，volatile变量的写和锁的释放具有相同的内存语义，volatile变量的读和锁的获取具有相同的内存语义。

禁止重排序

如果volatile变量与普通变量发生了重排序，虽然volatile变量能保证内存可见性，也可能导致普通变量读取错误。

所以在旧的内存模型中，volatile的写-读就不能与锁的释放-获取具有相同的内存语义了。为了提供一种比锁更轻量级的**线程间的通信机制**，**JSR-133**专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序。

编译器还好说，JVM是怎么还能限制处理器的重排序的呢？它是通过**内存屏障**来实现的。

什么是内存屏障？硬件层面，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。内存屏障有两个作用：

1. 阻止屏障两侧的指令重排序；
2. 强制把写缓冲区/高速缓存中的脏数据等写回主内存，或者让缓存中相应的数据失效。

> 注意这里的缓存主要指的是CPU缓存，如L1，L2等

编译器在**生成字节码时**，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。编译器选择了一个**比较保守的JMM内存屏障插入策略**，这样可以保证在任何处理器平台，任何程序中都能得到正确的volatile内存语义。这个策略是：

- 在每个volatile写操作前插入一个StoreStore屏障；
- 在每个volatile写操作后插入一个StoreLoad屏障；
- 在每个volatile读操作后插入一个LoadLoad屏障；
- 在每个volatile读操作后再插入一个LoadStore屏障。

> 再逐个解释一下这几个屏障。注：下述Load代表读操作，Store代表写操作
>
> **LoadLoad屏障**：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 **StoreStore屏障**：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。
>
> **LoadStore屏障**：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。
>
> **StoreLoad屏障**：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的（冲刷写缓冲器，清空无效化队列）。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能

对于连续多个volatile变量读或者连续多个volatile变量写，编译器做了一定的优化来提高性能，比如：

> 第一个volatile读;
>
> LoadLoad屏障；
>
> 第二个volatile读；
>
> LoadStore屏障

再介绍一下volatile与普通变量的重排序规则:

1. 如果第一个操作是volatile读，那无论第二个操作是什么，都不能重排序；
2. 如果第二个操作是volatile写，那无论第一个操作是什么，都不能重排序；
3. 如果第一个操作是volatile写，第二个操作是volatile读，那不能重排序

volatile的用途

从volatile的内存语义上来看，volatile可以保证内存可见性且禁止重排序。

在保证内存可见性这一点上，volatile有着与锁相同的内存语义，所以可以作为一个“轻量级”的锁来使用。但由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁可以保证整个**临界区代码**的执行具有原子性。所以**在功能上，锁比volatile更强大；在性能上，volatile更有优势**。

在禁止重排序这一点上，volatile也是非常有用的。比如我们熟悉的单例模式，其中有一种实现方式是“双重锁检查”，比如这样的代码：



```html
public class Singleton {
    private static Singleton instance; // 不使用volatile关键字
    // 双重锁检验    
    public static Singleton getInstance() {        
        if (instance == null) { // 第7行            
            synchronized (Singleton.class) {                
                if (instance == null) {                    
                    instance = new Singleton(); // 第10行                
                }            
            }        
        }        
        return instance;    
    }
}
```

如果这里的变量声明不使用volatile关键字，是可能会发生错误的。它可能会被重排序：

## ThreadLocal

### 1.ThreadLocal作用（线程本地存储）

ThreadLocal，很多地方叫做线程本地变量，也有些地方叫做线程本地存储，ThreadLocal的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。

ThreadLocalMap（线程的一个属性）

1.每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。

2.将一个共用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后在线程执行的各处通过这个静态ThreadLocal实例的get()方法取得自己线程保存的那个对象，避免了将这个对象作为参数传递的麻烦。

3.ThreadLocalMap其实就是线程里面的一个属性，它在Thread类中定义。

```html
ThreadLocal.ThreadLocalMap threadLocals = null;
```

最常见的ThreadLocal使用场景为用来解决数据库连接、Session管理等。

每个线程调用全局 ThreadLocal 对象的 set 方法，在 set 方法中，首先根据当前线程获取当前线程的ThreadLocalMap 对象，然后往这个 map 中插入一条记录， key 其实是 ThreadLocal 对象， value 是各自的 set方法传进去的值。也就是每个线程其实都有一份自己独享的 ThreadLocalMap 对象，该对象的 Key 是 ThreadLocal对象，值是用户设置的具体值。在线程结束时可以调用 ThreadLocal.remove() 方法，这样会更快释放内存，不调用也可以，因为线程结束后也可以自动释放 相关的 ThreadLocal 变量。

### 2.ThreadLocal源码分析

ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用（相同线程数据共享），也就是变 量在线程间隔离（不同的线程数据隔离）而在方法或类间共享的场景。

对象实例与 ThreadLocal 变量的映射关系是由线程 Thread 来维护的，对象实例与 ThreadLocal 变量的映射关系是由线程 Thread 来维护的，对象实例与 ThreadLocal 变量的映射关系是由线程 Thread 来维护的。换句话说就是对象实例与 ThreadLocal 变量的映射关系是存放的一个 Map 里面（这个 Map 是个抽象的Map 并不是 java.util 中的 Map ），而这个 Map 是 Thread 类的一个字段！而真正存放映射关系的 Map 就是ThreadLocalMap 。

在 set 方法中首先获取当前线程，然后通过 getMap 获取到当前线程的 ThreadLocalMap 类型的变量 threadLocals ，如果存在 则直接赋值，如果不存在则给该线程创建 ThreadLocalMap 变量并赋值。赋值的时候这里的 this 就是调用变量的对象实例本身。

get 方法也比较简单，同样也是先获取当前线程的 ThreadLocalMap 变量，如果存在则返回值，不存在则创建并返回初始值。

ThreadLocalMap 中使用 Entry[] 数组来存放对象实例与变量的关系，并且实例对象作为 key，变量作为 value 实现对应关系。并 且这里的 key 采用的是对实例对象的弱引用

### 3.ThreadLocal（线程局部变量）关键字：

当使用 ThreadLocal 维护变量时，其为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立的改变自己的副本，而不会影响其他线程对应的副本。

ThreadLocal 内部实现机制：

1. 每个线程内部都会维护一个类似 HashMap 的对象，称为 ThreadLocalMap，里边会包含若干了 Entry（K-V 键值对），相应的线程被称为这些 Entry 的属主线程；
2. Entry 的 Key 是一个 ThreadLocal 实例，Value 是一个线程特有对象。Entry 的作用即是：为其属主线程建立起一个 ThreadLocal 实例与一个线程特有对象之间的对应关系；
3. Entry 对 Key 的引用是弱引用；Entry 对 Value 的引用是强引用。

### 15.ThreadLocal 原理

从 `Thread`类源代码入手。

```html
public class Thread implements Runnable {
 ......
//与此线程有关的ThreadLocal值。由ThreadLocal类维护
ThreadLocal.ThreadLocalMap threadLocals = null;

//与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护
ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;
 ......
}
```

从上面`Thread`类 源代码可以看出`Thread` 类中有一个 `threadLocals` 和 一个 `inheritableThreadLocals` 变量，它们都是 `ThreadLocalMap` 类型的变量,我们可以把 `ThreadLocalMap` 理解为`ThreadLocal` 类实现的定制化的 `HashMap`。默认情况下这两个变量都是 null，只有当前线程调用 `ThreadLocal` 类的 `set`或`get`方法时才创建它们，实际上调用这两个方法的时候，我们调用的是`ThreadLocalMap`类对应的 `get()`、`set()`方法。

**最终的变量是放在了当前线程的 `ThreadLocalMap` 中，并不是存在 `ThreadLocal` 上，`ThreadLocal` 可以理解为只是`ThreadLocalMap`的封装，传递了变量值。** `ThrealLocal` 类中可以通过`Thread.currentThread()`获取到当前线程对象后，直接通过`getMap(Thread t)`可以访问到该线程的`ThreadLocalMap`对象。

**每个`Thread`中都具备一个`ThreadLocalMap`，而`ThreadLocalMap`可以存储以`ThreadLocal`为 key ，Object 对象为 value 的键值对。**

```html
ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {
 ......
}
```

比如我们在同一个线程中声明了两个 `ThreadLocal` 对象的话，会使用 `Thread`内部都是使用仅有那个`ThreadLocalMap` 存放数据的，`ThreadLocalMap`的 key 就是 `ThreadLocal`对象，value 就是 `ThreadLocal` 对象调用`set`方法设置的值。

`ThreadLocalMap`是`ThreadLocal`的静态内部类。

### 16.ThreadLocal 内存泄露问题

`ThreadLocalMap` 中使用的 key 为 `ThreadLocal` 的弱引用,而 value 是强引用。所以，如果 `ThreadLocal` 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，`ThreadLocalMap` 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 `set()`、`get()`、`remove()` 方法的时候，会清理掉 key 为 null 的记录。使用完 `ThreadLocal`方法后 最好手动调用`remove()`方法

```html
      static class Entry extends WeakReference<ThreadLocal<?>> {
            /** The value associated with this ThreadLocal. */
            Object value;

            Entry(ThreadLocal<?> k, Object v) {
                super(k);
                value = v;
            }
        }
```

**弱引用介绍：**

> 如果一个对象只具有弱引用，那就类似于**可有可无的生活用品**。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。
>
> 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。

## ConcurrentHashMap

### 1.ConcurrentHashMap：分段锁实现，线程安全

ConcurrentHashMap和HashMap的实现方式类似，不同的是它采用分段锁的思想支持并发操作，所以是线程安全的。

减小锁粒度

减小锁粒度指通过缩小锁定对象的范围来减少锁冲突的可能性，最终提高系统的并发能力。减小锁粒度是一种削弱多线程锁竞争的有效方法，ConcurrentHashMap并发下的安全机制就是基于该方法实现的。

ConcurrentHashMap是线程安全的Map，对于HashMap而言，最重要的方法是get和set方法，如果为了线程安全对整个HashMap加锁，则可以得到线程安全的对象，但是加锁粒度太大，意味着同时只能有一个线程操作HashMap，在效率上就会大打折扣；而ConcurrentHashMap在内部使用多个Segment，在操作数据时会给每个Segment都加锁，这样就通过减小锁粒度提高了并发度。

ConcurrentHashMap分段锁

ConcurrentHashMap在内部细分为若干个小的HashMap，叫作数据段（Segment）。在默认情况下，一个ConcurrentHashMap被细分为 16个数据段，对每个数据段的数据都单独进行加锁操作。Segment的个数为锁的并发度。

ConcurrentHashMap是由Segment数组和HashEntry数组组成的。Segment继承了可重入锁（ReentrantLock），它在ConcurrentHashMap里扮演锁的角色。HashEntry则用于存储键值对数据。

在每一个ConcurrentHashMap里都包含一个Segment数组，Segment的结构和HashMap类似，是数组和链表结构。在每个Segment里都包含一个HashEntry数组，每个HashEntry都是一个链表结构的数据，每个Segment都守护一个HashEntry数组里的元素，在对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。

在操作ConcurrentHashMap时，如果需要在其中添加一个新的数据，则并不是将整个HashMap加锁，而是先根据HashCode查询该数据应该被存放在哪个段，然后对该段加锁并完成put操作。在多线程环境下，如果多个线程同时进行put操作，则只要加入的数据被存放在不同的段中，在线程间就可以做到并行的线程安全。

Java 8在ConcurrentHashMap中引入了红黑树。

### 2.SynchronizedMap和ConcurrentHashMap有什么区别？

SynchronizedMap()和Hashtable一样，实现上在调用map所有方法时，都对整个map进行同步。而ConcurrentHashMap的实现却更加精细，它对map中的所有桶加了锁。所以，只要有一个线程访问map，其他线程就无法进入map，而如果一个线程在访问ConcurrentHashMap某个桶时，其他线程，仍然可以对map执行某些操作。 所以，ConcurrentHashMap在性能以及安全性方面，明显比Collections.synchronizedMap()更加有优势。同时，同步操作精确控制到桶，这样，即使在遍历map时，如果其他线程试图对map进行数据修改，也不会抛出ConcurrentModificationException 。

## CAS

### 1.什么是CAS

CAS的概念：比较并交换

CAS（Compare And Swap）指比较并交换。CAS算法CAS(V,E,N)包含3个参数，V表示要更新的变量，E表示预期的值，N表示新值。在且仅在 V值等于 E值时，才会将V值设为 N，如果 V值和 E值不同，则说明已经有其他线程做了更新，当前线程什么都不做。最后，CAS返回当前V的真实值。

CAS的特性：乐观锁

CAS操作采用了乐观锁的思想，总是认为自己可以成功完成操作。在有多个线程同时使用CAS操作一个变量时，只有一个会胜出并成功更新，其余均会失败。失败的线程不会被挂起，仅被告知失败，并且允许再次尝试，当然，也允许失败的线程放弃操作。基于这样的原理，CAS操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。

CAS自旋等待

在JDK的原子包java.util.concurrent.atomic里面提供了一组原子类，这些原子类的基本特性就是在多线程环境下，在有多个线程同时执行这些类的实例包含的方法时，会有排他性。其内部便是基于CAS算法实现的，即在某个线程进入方法中执行其中的指令时，不会被其他线程打断；而别的线程就像自旋锁一样，一直等到该方法执行完成才由JVM从等待的队列中选择另一个线程进入。

相对于synchronized阻塞算法，CAS是非阻塞算法的一种常见实现。由于CPU的切换比CPU指令集的操作更加耗时，所以CAS的自旋操作在性能上有了很大的提升。

### 2.ABA问题

对CAS算法的实现有一个重要的前提：需要取出内存中某时刻的数据，然后在下一时刻进行比较、替换，在这个时间差内可能数据已经发生了变化，导致产生ABA问题。

ABA问题指第1个线程从内存的V位置取出A，这时第2个线程也从内存中取出A，并将V位置的数据首先修改为B，接着又将V位置的数据修改为A，这时第1个线程在进行CAS操作时会发现在内存中仍然是A，然后第1个线程操作成功。尽管从第1个线程的角度来说，CAS操作是成功的，但在该过程中其实V位置的数据发生了变化，只是第1个线程没有感知到罢了，这在某些应用场景下可能出现过程数据不一致的问题。

部分乐观锁是通过版本号（version）来解决ABA问题的，具体的操作是乐观锁每次在执行数据的修改操作时都会带上一个版本号，在预期的版本号和数据的版本号一致时就可以执行修改操作，并对版本号执行加1操作，否则执行失败。因为每次操作的版本号都会随之增加，所以不会出现ABA问题，因为版本号只会增加，不会减少。

### 26.Java实现CAS的原理 - Unsafe类

前面提到，CAS是一种原子操作。那么Java是怎样来使用CAS的呢？我们知道，在Java中，如果一个方法是native的，那Java就不负责具体实现它，而是交给底层的JVM使用c或者c++去实现。

在Java中，有一个`Unsafe`类，它在`sun.misc`包中。它里面是一些`native`方法，其中就有几个关于CAS的：

```html
boolean compareAndSwapObject(Object o, long offset,Object expected, Object x);
boolean compareAndSwapInt(Object o, long offset,int expected,int x);
boolean compareAndSwapLong(Object o, long offset,long expected,long x);
```

当然，他们都是`public native`的。

Unsafe中对CAS的实现是C++写的，它的具体实现和操作系统、CPU都有关系。

Linux的X86下主要是通过`cmpxchgl`这个指令在CPU级完成CAS操作的，但在多处理器情况下必须使用`lock`指令加锁来完成。当然不同的操作系统和处理器的实现会有所不同，大家可以自行了解。

当然，Unsafe类里面还有其它方法用于不同的用途。比如支持线程挂起和恢复的`park`和`unpark`， LockSupport类底层就是调用了这两个方法。还有支持反射操作的`allocateInstance()`方法。

### 28.CAS实现原子操作的三大问题

ABA问题

所谓ABA问题，就是一个值原来是A，变成了B，又变回了A。这个时候使用CAS是检查不出变化的，但实际上却被更新了两次。

ABA问题的解决思路是在变量前面追加上**版本号或者时间戳**。从JDK 1.5开始，JDK的atomic包里提供了一个类`AtomicStampedReference`类来解决ABA问题。

这个类的`compareAndSet`方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果二者都相等，才使用CAS设置为新的值和标志。

循环时间长开销大

CAS多与自旋结合。如果自旋CAS长时间不成功，会占用大量的CPU资源。

解决思路是让JVM支持处理器提供的**pause指令**。

pause指令能让自旋失败时cpu睡眠一小段时间再继续自旋，从而使得读操作的频率低很多,为解决内存顺序冲突而导致的CPU流水线重排的代价也会小很多。

只能保证一个共享变量的原子操作

这个问题你可能已经知道怎么解决了。有两种解决方案：

1. 使用JDK 1.5开始就提供的`AtomicReference`类保证对象之间的原子性，把多个变量放到一个对象里面进行CAS操作；
2. 使用锁。锁内的临界区代码可以保证只有当前线程能操作。

## AQS

### 1.AQS

什么是AQS

AQS（Abstract Queued Synchronizer）是一个抽象的队列同步器，通过维护一个共享资源状态（Volatile Int State）和一个先进先出（FIFO）的线程等待队列来实现一个多线程访问共享资源的同步框架。

它底层使用的是双向列表，是队列的一种实现 , 因此也可以将它当成一种队列。 Sync queue 是同步列表，它是双向列表 , 包括 head，tail 节点。其中 head 节点主要用来后续的调度 ; Condition queue 是单向链表 , 不是必须的 , 只有当程序中需要 Condition 的时候，才会存在这个单向链表 , 并且可能会有多个 Condition queue。

AQS的原理

AQS为每个共享资源都设置一个共享资源锁，线程在需要访问共享资源时首先需要获取共享资源锁，如果获取到了共享资源锁，便可以在当前线程中使用该共享资源，如果获取不到，则将该线程放入线程等待队列，等待下一次资源调度，许多同步类的实现都依赖于AQS，例如常用的ReentrantLock、Semaphore和CountDownLatch。

state：状态

Abstract Queued Synchronizer维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。Volatile虽然不能保证操作的原子性，但是能保证当前变量state的可见性。

state的访问方式有三种：getState()、setState()和compareAndSetState()，均是原子操作，其中，compareAndSetState的实现依赖于Unsafe的compareAndSwapInt()。

AQS共享资源的方式：独占式和共享式

AQS定义了两种资源共享方式：独占式（Exclusive）和共享式（Share）。

- 独占式：只有一个线程能执行，具体的Java实现有ReentrantLock。
- 共享式：多个线程可同时执行，具体的Java实现有Semaphore和CountDownLatch。

AQS只是一个框架，只定义了一个接口，具体资源的获取、释放都交由自定义同步器去实现。不同的自定义同步器争用共享资源的方式也不同，自定义同步器在实现时只需实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护，如获取资源失败入队、唤醒出队等，AQS已经在顶层实现好，不需要具体的同步器再做处理。

- isHeldExclusively()：查询该线程是否正在独占资源，只有用到condition才需要去实现它
- try Acquire(int)：独占方式，尝试获取资源，成功则返回true，失败则返回false
- tryRelease(int)：独占方式，尝试是否资源，成功则返回true，失败则返回false
- tryAcquireShared(int)：共享方式，尝试获取资源，负数表示失败，0表示成功，但没有剩余可用资源，正数表示成功，且有剩余资源
- tryReleaseShared(int)：共享方式，尝试释放资源，如果释放资源后允许唤醒后续等待线程，则返回true，否则返回false

同步器的实现是AQS的核心内存。ReentrantLock对AQS的独占方式实现为：ReentrantLock中的state初始值为0时表示无锁状态。在线程执行tryAcquire()获取该锁后ReentrantLock中的state+1，这时该线程独占ReentrantLock锁，其他线程在通过tryAcquire()获取锁时均会失败，直到该线程释放锁后state再次为 0，其他线程才有机会获取该锁。该线程在释放锁之前可以重复获取此锁，每获取一次便会执行一次state+1，因此ReentrantLock也属于可重入锁。但获取多少次锁就要释放多少次锁，这样才能保证state最终为 0。如果获取锁的次数多于释放锁的次数，则会出现该线程一直持有该锁的情况；如果获取锁的次数少于释放锁的次数，则运行中的程序会报锁异常。

CountDownLatch对AQS的共享方式实现为：CountDownLatch将任务分为N个子线程去执行，将state也初始化为N，N与线程的个数一致，N个子线程是并行执行的，每个子线程都在执行完成后countDown()一次，state会执行CAS操作并减1。在所有子线程都执行完成（state=0）时会unpark()主线程，然后主线程会从await()返回，继续执行后续的动作。

一般来说，自定义同步器要么采用独占方式，要么采用共享方式，实现类只需实现tryAcquire、tryRelease或tryAcquireShared、tryReleaseShared中的一组即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，例如ReentrantReadWriteLock在读取时采用了共享方式，在写入时采用了独占方式。

### 2.AQS

基本概念

AQS 是 AbstractQueuedSynchronizer 的简称，翻译成中文就是 抽象队列同步器 ，这三个单词分开来看：

- Abstract （抽象）：也就是说， AQS 是一个抽象类，只实现一些主要的逻辑，有些方法推迟到子类实现
- Queued （队列）：队列有啥特征呢？先进先出（ FIFO ）对吧？也就是说， AQS 是用先进先出队列来存储数据的
- Synchronizer （同步）：即 AQS 实现同步功能

以上概括一下， AQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单而又高效地构造出同步器。

AQS 内部实现

AQS 队列在内部维护了一个 FIFO 的双向链表，如果对数据结构比较熟的话，应该很容易就能想到，在双向链表中，每个节点都有两 个指针，分别指向直接前驱节点和直接后继节点。使用双向链表的优点之一，就是从任意一个节点开始都很容易访问它的前驱节点和 后继节点。 在 AQS 中，每个 Node 其实就是一个线程封装，当线程在竞争锁失败之后，会封装成 Node 加入到 AQS 队列中；获取锁的线程释放 锁之后，会从队列中唤醒一个阻塞的 Node （也就是线程）

AQS 使用 volatile 的变量 state 来作为资源的标识:

关于 state 状态的读取与修改，子类可以通过覆盖 getState() 和 setState() 方法来实现自己的逻辑，其中比较重要的是:

```html
// 传入期望值 expect ,想要修改的值 update ,然后通过 Unsafe 的 compareAndSwapInt() 1 即 CAS 操作来实现
protected final boolean compareAndSetState(int expect, 2 int update) {
    // See below for intrinsics setup to support this
    return unsafe.compareAndSwapInt(this, stateOffset, expect, update);
}
```

AQS 如何获取资源

在 AQS 中，获取资源的入口是 acquire(int arg) 方法，其中 arg 是获取资源的个数

在获取资源时，会首先调用 tryAcquire 方法，这个方法是在子类中具体实现的 如果通过 tryAcquire 获取资源失败，接下来会通过 addWaiter(Node.EXCLUSIVE) 方法，将这个线程插入到等待队列中，

使用的是 CAS 自旋插入，这是因为在 AQS 中会存在多个线程同时竞争资源的情况，进而一定会出现多个线程同时插 入节点的操作，这里使用 CAS 自旋插入是为了保证操作的线程安全性 现在呢，申请 acquire(int arg) 方法，然后通过调用 addWaiter 方法，将一个 Node 插入到了队列尾部。处于等待队列节点是从头结点 开始一个一个的去获取资源

在获取资源时，除了 acquire 之外，还有三个方法： acquireInterruptibly ：申请可中断的资源（独占模式） acquireShared ：申请共享模式的资源 acquireSharedInterruptibly ：申请可中断的资源（共享模式）

AQS 如何释放资源

AQS 两种资源共享模式

资源有两种共享模式: 独占模式( Exclusive )：资源是独占的，也就是一次只能被一个线程占有，比如 ReentrantLock 共享模式( Share )：同时可以被多个线程获取，具体的资源个数可以通过参数来确定，比如 Semaphore/CountDownLatch

### 3.AQS的实现原理是什么？哪些场景会用到AQS？

AQS，AbstractQueuedSynchronizer，抽象的队列式同步器，AQS其实主要定义了一套对共享资源访问的同步框架，比如ReentrantLock、Semaphore、CountDownLatch都是依赖这个AQS的。AQS是java用来构建锁和其他同步组件的一个基础框架。

内部有一个FIFO队列，如果某个线程获取同步状态失败了，那么AQS就会把这个线程构造成一个node，塞入FIFO队列的尾部，然后如果同步状态释放的时候，就会从FIFO队列的头部唤醒一个node。其实说白了，就是让线程获取一个共享资源的时候，自动进行排队。

这就是所谓java中的最核心的同步这块的底层逻辑。

AQS里是有一个volatile修饰的state的，就代表了某个共享资源的同步状态，或者更加方便的理解，可以认为是一把锁，比如多个线程访问某段被锁的代码，会去尝试获取锁，其实就是在这里去尝试获取同步状态。

一个AQS，你可以认为是代表了用于获取一把锁的这么一个控制器

AQS里面，有一个FIFO队列，用来排队的；还有一个state，代表了锁的这么一个状态

Lock.lock()

// 干活儿

Lock.unlock()

获取一把锁的时候，尝试去获取AQS里的state状态（锁的状态）

刚开始state的值是0（代表没有人获取这把锁），线程1是可以获取到锁的，然后就会将state设置为1；接着线程2来尝试获取state的值，但是此时state = 1，表示被别人锁了，所以线程2的信息构造成一个node放入FIFO队列中来排队，同时阻塞住线程2

如果线程1释放锁的时候，就会将state设置为0，此时会唤醒FIFO队列中的排在队头的那个线程2，然后线程2发现state = 0，就会获取锁，将state设置为1，同时线程2的node从FIFO队列里面出队。

其实这块我不打算在这里太过于深入的分析，因为反正是应付面试题，你能理解到这里就ok了，简单来说AQS就是一个基础并发框架，ReentractLock之类的都是基于AQS来的，比如ReentractLock里面，某个线程想要获取锁，那么就会走上面AQS定义好的基础逻辑，获取state值啥的。

包括多个线程争用一把ReentractLock锁的时候，就会出现排队，然后某个线程释放锁之后，按照FIFO的顺序后面的线程自动获取锁。就这个意思。所以AQS就是个基础框架，提供了获取锁，FIFO排队，释放锁的基础逻辑，让ReentractLock、Semaphore、CountDownLatch之类的并发包下的类来使用。

### 29.AQS

AQS简介

**AQS**是`AbstractQueuedSynchronizer`的简称，即`抽象队列同步器`，从字面意思上理解:

- 抽象：抽象类，只实现一些主要逻辑，有些方法由子类实现；
- 队列：使用先进先出（FIFO）队列存储数据；
- 同步：实现了同步的功能。

那AQS有什么用呢？AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的同步器，比如我们提到的ReentrantLock，Semaphore，ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。

当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器，只要之类实现它的几个`protected`方法就可以了

AQS的数据结构

AQS内部使用了一个volatile的变量state来作为资源的标识。同时定义了几个获取和改版state的protected方法，子类可以覆盖这些方法来实现自己的逻辑：

```html
getState()
setState()
compareAndSetState()
```

这三种叫做均是原子操作，其中compareAndSetState的实现依赖于Unsafe的compareAndSwapInt()方法。

而AQS类本身实现的是一些排队和阻塞的机制，比如具体线程等待队列的维护（如获取资源失败入队/唤醒出队等）。它内部使用了一个先进先出（FIFO）的双端队列，并使用了两个指针head和tail用于标识队列的头部和尾部。

但它并不是直接储存线程，而是储存拥有线程的Node节点。

资源共享模式

资源有两种共享模式，或者说两种同步方式：

- 独占模式（Exclusive）：资源是独占的，一次只能一个线程获取。如ReentrantLock。
- 共享模式（Share）：同时可以被多个线程获取，具体的资源个数可以通过参数指定。如Semaphore/CountDownLatch。

一般情况下，子类只需要根据需求实现其中一种模式，当然也有同时实现两种模式的同步类，如`ReadWriteLock`。

### 30.AQS的主要方法源码解析

AQS的设计是基于**模板方法模式**的，它有一些方法必须要子类去实现的，它们主要有：

- isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。
- tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。
- tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。
- tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。
- tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。

这些方法虽然都是`protected`方法，但是它们并没有在AQS具体实现，而是直接抛出异常（虽然不知道这里为什么不使用抽象方法的实现方式）：

```html
protected boolean tryAcquire(int arg) {
    throw new UnsupportedOperationException();
}
```

而AQS实现了一系列主要的逻辑。

获取资源

获取资源的入口是acquire(int arg)方法。arg是要获取的资源的个数，在独占模式下始终为1。我们先来看看这个方法的逻辑：

```html
public final void acquire(int arg) {
    if (!tryAcquire(arg) &&
        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        selfInterrupt();
}
```

首先调用tryAcquire(arg)尝试去获取资源。前面提到了这个方法是在子类具体实现的。

如果获取资源失败，就通过addWaiter(Node.EXCLUSIVE)方法把这个线程插入到等待队列中。其中传入的参数代表要插入的Node是独占式的。

所以**结点进入等待队列后，是调用park使它进入阻塞状态的。只有头结点的线程是处于活跃状态的**。

当然，获取资源的方法除了acquire外，还有以下三个：

- acquireInterruptibly：申请可中断的资源（独占模式）
- acquireShared：申请共享模式的资源
- acquireSharedInterruptibly：申请可中断的资源（共享模式）

> 可中断的意思是，在线程中断时可能会抛出`InterruptedException`

### 28.AQS 原理分析

AQS 原理概览

**AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。**

> CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。

AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。

```html
private volatile int state;//共享变量，使用volatile修饰保证线程可见性
```

状态信息通过 protected 类型的 getState，setState，compareAndSetState 进行操作。

AQS 对资源的共享方式

**AQS 定义两种资源共享方式**

- Exclusive

  （独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁：

  - 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁
  - 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的

- **Share**（共享）：多个线程可同时执行，如`CountDownLatch`、`Semaphore`、`CountDownLatch`、 `CyclicBarrier`、`ReadWriteLock` 我们都会在后面讲到。

`ReentrantReadWriteLock` 可以看成是组合式，因为 `ReentrantReadWriteLock` 也就是读写锁允许多个线程同时对某一资源进行读。

不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在顶层实现好了。

AQS 底层使用了模板方法模式

同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：

1. 使用者继承 `AbstractQueuedSynchronizer` 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放）
2. 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。

**AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法：**

```html
isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。
tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。
tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。
tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。
tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。
```

默认情况下，每个方法都抛出 `UnsupportedOperationException`。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。

以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。

再以 `CountDownLatch` 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后`countDown()` 一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 `await()` 函数返回，继续后余动作。

一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现`tryAcquire-tryRelease`、`tryAcquireShared-tryReleaseShared`中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如`ReentrantReadWriteLock`。

## 线程并发库

### 1.线程并发库介绍

1) java.util.concurrent 包 多线程并发库

java.util.concurrent 包含许多线程安全、测试良好、高性能的并发构建块。不客气地说，创建java.util.concurrent 的目的就是要实现 Collection 框架对数据结构所执行的并发操作。

2) java.util.concurrent.atomic 包 多线程的原子性操作提供的工具类

可以对多线程的基本数据、数组中的基本数据和对象中的基本数据进行多线程的操作（ AtomicInteger 、 AtomicIntegerArray 、 AtomicIntegerFieldUpDater…

- AtomicInteger 类的 boolean compareAndSet(expectedValue, updateValue);
- AtomicIntegerArray 类的 int addAndGet(int i, int delta);

3) java.util.concurrent. lock 包 多线程的锁机制

为锁和等待条件提供一个框架的接口和类，它不同于内置同步和监视器。该框架允许更灵活地使用锁和条件。

本包下有三大接口，下面简单介绍下：

- Lock 接口 支持那些语义不同（重入、公平等）的锁规则，可以在非阻塞式结构的上下文（包括 handover hand 和锁重排算法）中使用这些规则。主要的实现是 ReentrantLock 。
- ReadWriteLock 接口 以类似方式定义了一些读取者可以共享而写入者独占的锁。此包只提供了一个实现，即 ReentrantReadWriteLock ，因为它适用于大部分的标准用法上下文。但程序员可以创建自己的、适用于非标准要求的实现。
- Condition 接口 描述了可能会与锁有关联的条件变量。这些变量在用法上与使用 Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个 Lock 可能与多个 Condition 对象关联。

### 2.java.util.concurrent 包

此包下有一些组件，其中包括：

- 执行程序（线程池）
- 并发队列
- 同步器
- 并发 Collocation

执行程序

Executors 线程池工厂类

线程池，减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务；可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为因为消耗过多的内存，而把服务器累趴下 每个线程需要大约 1MB 内存，线程开的越多，消耗的内存也就越大，最后死机。

Java里面线程池的顶级接口是 Executor ，但是严格意义上讲 Executor 并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是 ExecutorService 。 ThreadPoolExecutor 是 Executors 类的底层实现。

线程池的基本思想还是一种对象池的思想，开辟一块内存空间，里面存放了众多未死亡 的线程，池中线程执行调度由池管理器来处理。当有线程任务时，从池中取一个，执行完成后线程对象归池，这样可以避免反复创建线程对象所带来的性能开销，节省了系统的资源。

ExecutorService 执行器服务

java.util.concurrent.ExecutorService 接口表示一个异步执行机制，使我们能够在后台执行任务。因此一个 ExecutorService 很类似于一个线程池。实际上，存在于 java.util.concurrent 包里的 ExecutorService 实现就是一个线程池实现。

Executor Service 实现

ExecutorService 是个接口，java.util.concurrent包提供了 ExecutorService 接口的以下实现类：

- ThreadPoolExecutor
- ScheduledThreadPoolExecutor

Executor Service 创建

ExecutorService的创建依赖于你使用的具体实现。但是你也可以使用 Executors 工厂类来创建ExecutorService实例。

Executor Service 使用

有几种不同的方式来将任务委托给ExecutorService 去执行：

execute(Runn able)

execute(Runnable)方法要求一个 java.lang.Runnable 对象，然后对它进行异步执行。

```html
ExecutorService executorService = Executors.newSingleThreadExecutor();
executorService.execute(new Runnable() {
    public void run() {
        System.out.println("Asynchronous task");
    }
});
executorService.shutdown();
```

特点： 没有办法得知被执行的 Runnable 的执行结果。如果有需要的话你得使用一个 Callable。

submit(Runnable)

submit(Runnable) 方法也要求一个 Runnable 实现类，但它返回一个 Future 对象。这个 Future 对象可以用来检查 Runnable 是否已经执行完毕。

```html
Future future = executorService.submit(new Runnable() {
    public void run() {
        System.out.println("Asynchronous task");
    }
});
future.get(); //获得执行完 run 方法后的返回值，这里使用的 Runnable ，所以这里没有 返回值，返回的是 null 。
```

submit(Callable)

submit(Callable) 方法类似于 submit(Runnable) 方法，除了它所要求的参数类型之外。 Callable 实例除了它的 call() 方法能够返回一个结果之外和一个 Runnable 很相像。 Runnable.run() 不能够返回一个结果。Callable 的结果可以通过 submit(Callable) 方法返回的 Future 对象进行获取。

```html
Future future = executorService.submit(new Callable(){
    public Object call() throws Exception {
        System.out .println("Asynchronous
        return "Callable Result";
    }
}); 
System.out.println("future.get() = " + future.get());
```

invokeAny()

invokeAny() 方法要求一系列的 Callable 或者其子接口的实例对象。调用这个方法并不会返回一个 Future但它返回其中一个 Callable 对象的结果。无法保证返回的是哪个 Callable 的结果 只能表明其中一个已执行结束。 如果其中一个任务执行结束 或者抛了一个异常 ))，其他 Callable 将被取消。

```html
Set<Callable<String>> callables = new HashSet<Callable<String>>();
callables.add(new Callable<String>() {
    public String call() throws Exception {
        return "Task 1";
    }
}); 
String result = executorService.invokeAny(callables);
```

invokeAll()

invokeAll() 方法将调用你在集合中传给 Exec utorService 的所有 Callable 对象。 invokeAll() 返回一系列的 Future 对象，通过它们你可以获取每个 Callable 的执行结果。 记住，一个任务可能会由于一个异常而结束，因此它可能没有 成功 。无法通过一个 Future 对象来告知我们是两种结束中的哪一种。

```html
List<Future<String>> futures executorService.invokeAll(callables);
for(Future<String> future futures){
    System.out.println("future.get future.get());
}
```

Executors 关闭

使用shutdown 和 shutdownNow 可以关闭线程池 两者的区别： shutdown只是将空闲的线程 interrupt() 了， shutdown （）之前提交的任务可以继续执行直到结束。 shutdownNow是 interrupt 所有线程， 因此大部分线程将立刻被中断。之所以是大部分，而不是全部是因为 interrupt() 方法能力有限。

ThreadPoolExecutor 线程池执行者

java.util.concurrent.ThreadPoolExecutor 是 ExecutorService 接口的一个实现。ThreadPoolExecutor 使用其内部池中的线程执行给定任务(Callable 或者 Runnable)。

ThreadPoolExecutor 包含的线程池能够包含不同数量的线程。池中线程的数量由以下变量决定：

- corePoolSize
- maximumPoolSize

当一个任务委托给线程池时，如果池中线程数量低于 corePoolSize，一个新的线程将被创建，即使池中可能尚有空闲线程。如果内部任务队列已满， 而且有至少 corePoolSize 正在运行， 但是运行线程的数量低于maximumPoolSize，一个新的线程将被创建去执行该任务。

创建ThreadPoolExecutor

```html
ExecutorService threadPoolExecutor=
new ThreadPoolExecutor(
corePoolSize,//池中所保存的线程数，包括空闲线程。
maximumPoolSize,//池中允许的最大线程数。
keepAliveTime,//当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。
TimeUnit.MILLISECONDS,//keepAliveTime 参数的时间单位。
new LinkedBlockingQueue<Runnable>()//执行前用于保持任务的队列。此队列仅保持由 execute 方法提交的 Runnable 任务。
);
```

ScheduledPoolExecutor 定时线程池执行者

java.util.concurrent.ScheduledExecutorService是一个 ExecutorService 它能够将任务延后执行，或者间隔固定时间多次执行。 任务由一个工作者线程异步执行，而不是由提交任务给 ScheduledExecutorService 的那个线程执行。

```html
ScheduledExecutorService scheduledExecutorService=Executors.newScheduledThreadPool(5);
ScheduledFuture scheduledFuture=scheduledExecutorService.schedule(new Callable(){
    public Object call() throws Exception
        System.out.println("Executed!");
        return "Called";
    }
),
5,
TimeUnit.SECONDS);//5 秒后执行
```

首先一个内置5 个线程的 ScheduledExecutorService 被创建。之后一个 Callable 接口的匿名类示例被创建然后传递给 schedule() 方法。后边的俩参数定义了 Callable 将在 5 秒钟之后被执行。

ScheduledExecutorService 的实现

ScheduledExecutorService是一个接口，你要用它的话就得使用 java.util.concurrent 包里对它的某个实现类。ScheduledExecutorService 具有以下实现类： ScheduledThreadPoolExecutor。

创建一个 ScheduledExecutorService

如何创建一个ScheduledExecutorService 取决于你采用的它的实现类。但是你也可以使 用 Executors 工厂类来创建一个 ScheduledExecutorService 实例。

```html
ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(5);
```

ScheduledExecutorService 的使用

一旦你创建了一个ScheduledExecutorService ，你可以通过调用它的以下方法

schedule (Callable task, long de lay, TimeUnit timeunit)

schedule (Runnable task, long delay, TimeUnit timeunit)

scheduleAtFixedRate (Runnable, long initialDelay, long period, TimeUnit timeunit)

这一方法规划一个任务将被定期执行。该任务将会在首个initialDelay 之后得到执行，然后每个 period 时间之后重复执行。

如果给定任务的执行抛出了异常，该任务将不再执行。如果没有任何异常的话，这个任务将会持续循环执行到ScheduledExecutorService 被关闭。 如果一个任务占用了比计划的时间间隔更长的时候，下一次执行将在当前执行结束执行才开始。计划任务在同一时间不会有多个线程同时执行。

scheduleWithFixedDelay (Runnable, long initialDelay, long period, TimeUnit timeunit)

除了 period 有不同的解释之外这个方法和 scheduleAtFixedRate() 非常像。scheduleAtFixedRate() 方法中，period 被解释为前一个执行的开始和下一个执行的开始之间的间隔时间。而在本方法中，period 则被解释为前一个执行的结束和下一个执行的结束之间的间隔。因此这个延迟是执行结束之间的间隔，而不是执行开始之间的间隔。

ScheduledExecutorService 的关闭

正如 ExecutorService，在你使用结束之后你需要把 ScheduledExecutorService 关闭掉。否则他将导致 JVM继续运行，即使所有其他线程已经全被关闭。 你可以使用从 ExecutorService 接口继承来的 shutdown() 或 shutdownNow() 方法将ScheduledExecutorService 关闭。

ForkJoinPool 合并和分叉（线程池）

ForkJoinPool 在 Java 7 中被引入。它和 ExecutorService 很相似，除了一点不同。ForkJoinPool 让我们可以很方便地把任务分裂成几个更小的任务，这些分裂出来的任务也将会提交给 ForkJoinPool。任务可以继续分割成更小的子任务，只要它还能分割。可能听起来有些抽象，因此本节中我们将会解释 ForkJoinPool 是如何工作的，还有任务分割是如何进行的。

分叉

一个使用了分叉和合并原理的任务可以将自己分叉(分割)为更小的子任务，这些子任务可以被并发执行。

通过把自己分割成多个子任务，每个子任务可以由不同的 CPU 并行执行，或者被同一个 CPU 上的不同线程执行。只有当给的任务过大，把它分割成几个子任务才有意义。把任务分割成子任务有一定开销，因此对于小型任务，这个分割的消耗可能比每个子任务并发执行的消耗还要大。 什么时候把一个任务分割成子任务是有意义的，这个界限也称作一个阀值。这要看每个任务对有意义阀值的决定。很大程度上取决于它要做的工作的种类。

合并

当一个任务将自己分割成若干子任务之后，该任务将进入等待所有子任务的结束之中。一旦子任务执行结束，该任务可以把所有结果合并到同一个结果。

当然，并非所有类型的任务都会返回一个结果。如果这个任务并不返回一个结果，它只需等待所有子任务执行完毕。也就不需要结果的合并啦。

所以我们可以将 ForkJoinPool 是一个特殊的线程池，它的设计是为了更好的配合 分叉 和 合并 任务分割的工作。 ForkJoinPool 也在 java.util.concurrent 包中，其完整类名为 java.util.concurrent.ForkJoinPool 。

创建一个 ForkJoinPool

你可以通过其构造子创建一个 ForkJoinPool 。作为传递给 ForkJoinPool 构造子的一个参数，你可以定义你期望的并行级别。并行级别表示你想要传递给 ForkJoinPool 的任务所需的线程或 CPU 数量。

```html
创建了一个并行级别为 4 的 ForkJoinPool
ForkJoinPool forkJoinPool = new ForkJo inPool(4);
```

提交任务到 ForkJoinPool

就像提交任务到 ExecutorService 那样，把任务提交到 ForkJoinPool 。你可以提交两种类型的任务。一种是没有任何返回值的 一个 “行动 ”))，另一种是有返回值的 一个”任务” 。这两种类型分别由 RecursiveAction 和RecursiveTask 表示。接下来介绍如何使用这两种类型的任务，以及如何对它们进行提交。

RecursiveAction

RecursiveAction 是一种没有任何返回值的任务。它只是做 一些工作，比如写数据到磁盘，然后就退出了。一个RecursiveAction 可以把自己的工作分割成更小的几块，这样它们可以由独立的线程或者 CPU 执行。你可以通过继承来实现一个 RecursiveAction 。

将一个虚构的 workLoad 作为参数传给自己的构造子。如果 workLoad 高于一个特定阀值，该工作将被分割为几个子工作，子工作继续分割。如果 workLoad 低于特定阀值，该工作将由MyRecursiveAction 自己执行。

RecursiveTask

RecursiveTask 是一种会返回结果的任务。它可以将自己的工作分割为若干更小任务，并将这些子任务的执行结果合并到一个集体结果。可以有几个水平的分割和合并。

示例也会将工作分割为子任务，并通过 fork() 方法对这些子任务计划执行。此外，还可以通过调用每个子任务的 join() 方法收集它们返回的结果。子任务的结果随后被合并到一个更大的结果，并最终将其返回。对于不同级别的递归，这种子任务的结果合并可能会发生递归。

并发队列-阻塞队列

常用的并发队列有阻塞队列和非阻塞队列，前者使用锁实现，后者则使用CAS 非阻塞算法实现。

阻塞队列:

阻塞队列 (BlockingQueue)是Java util.concurrent 包下重要的数据结构，BlockingQueue 提供了线程安全的队列访问方式：当阻塞队列进行插入数据时，如果队列已满，线程将会阻塞等待直到队列非满；从阻塞队列取数据时，如果队列已空，线程将会阻塞等待直到队列非空。并发包下很多高级同步类的实现都是基于BlockingQueue 实现的。

BlockingQueue 阻塞队列

BlockingQueue 通常用于一个线程生产对象，而另外一个线程消费这些对象的场景。

一个线程将会持续生产新对象并将其插入到队列之中，直到队列达到它所能容纳的临界点。也就是说，它是有限的。如果该阻塞队列到达了其临界点，负责生产的线程将会在往里边插入新对象时发生阻塞。它会一直处于阻塞之中，直到负责消费的线程从队列中拿走一个对象。负责消费的线程将会一直从该阻塞队列中拿出对象。如果消费线程尝试去从一个空的队列中提取对象的话 ，这个消费线程将会处于阻塞之中，直到一个生产线程把一个对象丢进队列。

BlockingQueue 的方法

BlockingQueue 具有 4 组不同的方法用于插入、移除以及对队列中的元素进行检查。如果请求的操作不能得到立即执行的话，每个方法的表现也不同。这些方法如下：

| 方法/处理方式 | 抛出异常  | 返回特殊值 | 一直阻塞 | 超时退出           |
| ------------- | --------- | ---------- | -------- | ------------------ |
| 插入方法      | add(e)    | offer(e)   | put(e)   | offer(e,time,unit) |
| 移除方法      | remove()  | poll()     | take()   | poll(time,unit)    |
| 检查方法      | element() | peek()     | 不可用   | 不可用             |

抛异常： 如果试图的操作无法立即执行，抛一个异常。 特定值： 如果试图的操作无法立即执行，返回一个特定的值 常常是 true / false) 。 阻塞： 如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行。 超时： 如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功，典型的是 true / false) 。

无法向一个BlockingQueue 中插入 null 。如果你试图插入 null BlockingQueue 将会抛出一个NullPointerException。

BlockingQueue 的实现类

BlockingQueue是个接口，你需要使用它的实现之一来使用 BlockingQueue Java .util.concurrent 包下具有以下 BlockingQueue 接口的实现类：

- ArrayBlockingQueue：是一个有界的阻塞队列，其内部实现是将对象放到一个数组里。有界也就意味着，它不能够存储无限多数量的元素。它有一个同一时间能够存储元素数量的上限。你可以在对其初始化的时候设定这个上限，但之后就无法对这个上限进行修改了 译者注：因为它是基于数组实现的，也就具有数组的特性：一旦初始化，大小就无法修改 。
- DelayQueue ：DelayQueue 对元素进行持有直到一个特定的延迟到期。注入其中的元素必须实现java.util.concurrent.Delayed 接口。
- LinkedBlockingQueue ： LinkedBlockingQueue 内部以一个链式结构 链接节点 对其元素进行存储。如果需要的话，这一链式结构可以选择一个上限。如果没有定义上限，将使用 Integer.MAX_VALUE 作为上限。
- PriorityBlockingQueue ： PriorityBlockingQueue 是一个无界的并发队列。它使用了和类java.util.PriorityQueue 一样的排序规则。你无法向这个队列中插入 null 值。所有插入到PriorityBlockingQueue 的元素必须实现 java.lang.Comparable 接口。因此该队列中元素的排序就取决于你自己的 Comparable 实现。
- SynchronousQueue ：SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞，直到另一个线程将该元素从队列中抽走。同 样，如果该队列为空，试图向队列中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素。据此，把这个类称作一个队列显然是夸大其词了。它更多像是一个汇合点。

ArrayBlockingQueue 阻塞队列

ArrayBlockingQueue 内部有个数组 items 用来存放队列元素，putindex 下标标示入队元素下标，takeIndex 是出队下标， count 统计队列元素个数，从定义可知道并没有使用 volatile 修饰，这是因为访问这些变量使用都是在锁块内，并不存在可见性问题。另外有个独占锁 lock 用来对出入队操作加锁，这导致同时只有一个线程可以访问入队出队，另外 notEmpty ，notFull 条件变量用来进行出入队的同步。

另外构造函数必须传入队列大小参数，所以为有界队列 默认是 Lock 为非公平锁。

所谓公平锁：就是在并发环境中，每个线程在获取锁时会先查看此锁维护的等待队列，如果为空，或者当前线程线程是等待队列的第一个，就占有锁，否则就会加入到等待队列中，以后会按照 FIFO 的规则从队列中取到自己。 非公平锁：比较粗鲁，上来就直接尝试占有锁，如果尝试失败，就再采用类似公平锁那种方式。

ArrayBlockingQueue 方法

offer 方法

在队尾插入元素，如果队列满则返回 false ，否者入队返回 true 。

这里由于在操作共享变量前加了锁，所以不存在内存不可见问题，加过锁后获取的共享变量都是从主内存获取的，而不是在 CPU 缓存或者寄存器里面的值，释放锁后修改的共享变量值会刷新会主内存中。 另外这个队列是使用循环数组实现，所以计算下一个元素存放下标时候有些特殊。另外insert 后调用notEmpty.signal(); 是为了激活调用 notEmpty.await() 阻塞后放入 notEmpty 条件队列中的线程。

Put 操作

在队列尾部添加元素，如果队列满则等待队列有空位置插入后返回 。

Poll 操作

从队头获取并移除元素，队列为空，则返回 null 。

Take 操作

从队头获取元素，如果队列为空则阻塞直到队列有元素。

需要注意的是如果队列为空，当前线程会被挂起放到 notEmpty 的条件队列里面，直到入队操作执行调用notEmpty.signal 后当前线程才会被激活， await 才会返回。

Peek 操作

返回队列头元素但不移除该元素，队列为空，返回 null 。

Size 操作 获取队列元素个数，非常精确因为计算 size 时候加了独占锁，其他线程不能入队或者出队或者删除元素 。

ArrayBlockingQueue 通过使用全局独占锁实现同时只能有一个线程进行入队或者出队操作，这个锁的粒度比较大，有点类似在方法上添加 synchronized 的意味。其中 offer,poll 操作通过简单的加锁进行入队出队操作，而 put,take则使用了条件变量实现如果队列满则等待，如果队列空则等待，然后分别在出队和入队操作中发送信号激活等待线程实现同步。另外相比 LinkedBlockingQueue ArrayBlockingQueue 的 size 操作的结果是精确的，因为计算前加了全局锁。

```html
final BlockingQueue queue = new ArrayBlockingQueue(3);
```

LinkedBlockingQueue 阻塞队列

LinkedBlockingQueue 类图

LinkedBlockingQueue 中也有两个 Node 分别用来存放首尾节点，并且里面有个初始值为 0 的原子变量 count用来记录队列元素个数，另外里面有两个 ReentrantLock 的独占锁，分别用来控制元素入队和出队加锁，其中 takeLock用来控制同时只有一个线程可以从队列获取元素，其他线程必须等待， putLock 控制同时只能有一个线程可以获取锁去添加元素，其他线程必须等待。另外 notEmpty 和 notFull 用来实现入队和出队的同步。 另外由于出入队是两个非公平独占锁，所以可以同时又一个线程入队和一个 线程出队，其实这个是个生产者 消费者模型。

LinkedBlockingQueue 方法

带时间的 Offer 操作- 生产者

```html
public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException {
```

带时间的 poll 操作 -消费者

获取并移除队首元素，在指定的时间内去轮询队列看有没有首元素有则返回，否者超时后返回 null 。

```html
public E poll(long timeout, TimeUnit unit) throws InterruptedException {
```

首先获取独占锁，然后进入循环当当前队列有元素才会退出循环，或者超时了，直接返回 null 。 超时前退出循环后，就从队列移除元素，然后计数器减去一，如果减去 1 前队列元素大于 1 则说明当前移除后队列还有元素，那么就发信号激活其他可能阻塞到当前条件信号的线程。 最后如果减去 1 前队列元素个数 最大值，那么移除一个后会腾出一个空间来，这时候可以激活可能存在的入队阻塞线程。

put 操作 -生产者

与带超时时间的 poll 类似不同在于 put 时候如果当前队列满了它会一直等待其他线程调用 notFull.signal 才会被唤醒。

take 操作 -消费者

与带超时时间的 poll 类似不同在于 take 时候如果当前队列空了它会一直等待其他线程调用 notEmpty.signal() 才会被唤醒。

size 操作 -消费者

当前队列元素个数，如代码直接使用原子变量

peek 操作 获取但是不移除当前队列的头元素，没有则返回 null 。

remove 操作

删除队列里面的一个元素，有则删除返回 true ，没有则返回 fal se ，在删除操作时候由于要遍历队列所以加了双重锁，也就是在删除过程中不允许入队也不允许出队操作 。

开源框架的使用 tomcat 中任务队列 TaskQueue 。

可知 TaskQueue 继承了 LinkedBlockingQueue 并且泛化类型固定了为 Runnalbe. 重写了 offer,poll take 方法。tomcat 中有个线程池 ThreadPoolExecutor ，在 NIOEndPoint 中当 acceptor 线程接受到请求后，会把任务放入队列，然后 poller 线程从队列里面获取任务，然后就 把 任务放入线程池执行。这个 ThreadPoolExecutor 中的的一个参数就是 TaskQueue 。 先看看 ThreadPoolExecutor 的参数如果是普通 LinkedB lockingQueue 是怎么样的执行逻辑：

当调用线程池方法 execute() 方法添加一个任务时：

- 如果当前运行的线程数量小于 corePoolSize ，则创建新线程运行该任务
- 如果当前运行的线程数量大于或等于 corePoolSize ，则将这个任务放入阻塞队列。
- 如果当前队列满了，并且当前运行的线程数量小于 maximumPoolSize ，则创建新线程运行该任务
- 如果当前队列满了，并且当前运行的线程数量大于或等于 maximumPoolSize ，那么线程池将会抛出RejectedExecutio nException 异常。

如果线程执行完了当前任务，那么会去队列里面获取一个任务来执行，如果任务执行完了，并且当前线程数大于corePoolSize ，那么会根据线程空闲时间 keepAliveTime 回收一些线程保持线程池 corePoolSize 个线程。

并发库中的 BlockingQueue 是一个比较好玩的类，顾名思义，就是阻塞队列。该类主要提供了两个方法 put() 和take()，前者将一个对象放到队列中，如果队列已经满了，就等待直到有空闲节点；后者从 head 取一个对象，如果没有对象，就等待直 到有可取的对象。

PriorityBlockingQueue 无界阻塞优先级队列

PriorityBlockingQueue 是带优先级的无界阻塞队列，每次出队都返回优先级最高的元素，是二叉树最小堆的实现，研究过数组方式存放最小堆节点的都知道，直接遍历队列元素是无序的。

PriorityBlockingQueue 内部有个数组 queue 用来存放队列元素， size 用来存放队列元素个数，allocationSpinLockOffset 是用来在扩容队列时候做 cas 的，目的是保证只有一个线程可以进行扩容。 由于这是一个优先级队列所以有个比较器 comparator 用来比较元素大小。 lock 独占锁对象用来控制同时只能有一个线程可以进行入队出队操作。 notEmpty 条件变量用来实现 take 方法阻塞模式。这里没有 notFull 条件变量是因为这里的 put 操作是非阻塞的，为啥要设计为非阻塞 的是因为这是无界队列。

最后 PriorityQueue q 用来搞序列化的。

PriorityBlockingQueue 方法

Offer 操作 在队列插入一个元素，由于是无界队列，所以一直为成功返回 true;

Poll 操作 在队列头部获取并移除一个元素，如果队列为空，则返回 null

Put 操作 内部调用的 offer, 由于是无界队列，所以不需要阻塞

Take 操作 获取队列头元素，如果队列为空则阻塞。

Size 操作 获取队列元个数，由于加了独占锁所以返回结果是精确的

PriorityBlockingQueue类似于 ArrayBlockingQueue 内部使用一个独 占锁来控制同时只有一个线程可以进行入队和出队，另外前者只使用了一个 notEmpty 条件变量而没有 notFull 这是因为前者是无界队列，当 put 时候永远不会处于 await 所以也不需要被唤醒。 PriorityBlockingQueue始终保证出队的元素是优先级最高的元素，并且可以定制优先级的规则，内部通过使用一个二叉树最小堆算法来维护内部数组，这个数组是可扩容的，当当前元素个数 最大容量时候会通过算法扩容。 值得注意的是为了避免在扩容操作时候其他线程不能进行出队操作，实现上使用了先释放锁，然后通过cas 保证同时只有一个线程可以扩容成功。

PriorityBlockingQueue类是 JDK 提供的优先级队列 本身是线程安全的 内部使用显示锁 保证线程安全 。 PriorityBlockingQueue存储的对象必须是实现 Comparable 接口的 因为 PriorityBlockingQueue 队列会根据内部存储的每一个元素的 compareTo 方法比较每个元素的大小 。 这样在 take 出来的时候会根据优先级将优先级最小的最先取出 。

SychronousQueue 同步队列

经典的生产者-消费者模式，操作流程是这样的： 有多个生产者，可以并发生产产品，把产品置入队列中，如果队列满了，生产者就会阻塞； 有多个消费者，并发从队列中获取产品，如果队列空了，消费者就会阻塞；

SynchronousQueue 也是一个队列来的，但它的特别之处在于它内部没有容器，一个生产线程，当它生产产品（即put 的时候），如果当前没有人想要消费产品(即当前没有线程执行take)，此生产线程必须阻塞，等待一个消费线程调用take 操作，take 操作将会唤醒该生产线程，同时消费线程会获取生产线程的产品（即数据传递），这样的一个过程称为一次配对过程(当然也可以先take 后put,原理是一样的)。

不像ArrayBlockingQueue 、 LinkedBlockingDeque 之类的阻塞队列依赖 AQS 实现并发操作，SynchronousQueue 直接使用 CAS 实现线程的安全访问。队列的实现策略通常分为公平模式和非公平模式。

公平模式下的模型： 公平模式下，底层实现使用的是TransferQueue 这个内部队列，它有一个 head 和 tail 指针，用于指向当前正在等待匹配的线程节点。

1、线程 put1 执行 put(1) 操作，由于当前没有配对的消费线程，所以 put1 线程入队列，自旋一小会后睡眠等待。

2、接着，线程 put2 执行了 put(2) 操作，跟前面一样， put2 线程入队列，自旋一小会后睡眠等待。

3、这时候，来了一个线程 ta ke1 ，执行了 take 操作，由于 tail 指向 put2 线程， put2 线程跟 take1 线程配对了 一 put 一 take)take)，这时 take1 线程不需要入队，但是请注意了，这时候，要唤醒的线程并不是 put2 ，而是 put1 。为何？大家应该知道我们现在讲的是公平策略，所谓公平就是谁先入队了，谁就优先被唤醒。

公平策略总结下来就是：队尾匹配队头出队。

执行后 put1 线程被唤醒， take1 线程的 take() 方法返回了 1(put1 线程的数据 ))，这样就实现了线程间的一对一通信。

4、最后，再来一个线程take2，执行take 操作，这时候只有put2 线程在等候，而且两个线程匹配上了，线程put2 被唤醒， take2 线程take 操作返回了2(线程put2 的数据)，这时候队列又回到了起点。

非公平模式下的模型：

非公平模式底层的实现使用的是TransferStack，一个栈，实现中用head 指针指向栈顶。

1、线程 put1 执行 put(1) 操作，由于当前没有配对的消费线程，所以 put1 线程入队列，自旋一小会后睡眠等待。

2、接着，线程 put2 执行了 put(2) 操作，跟前面一样， put2 线程入队列，自旋一小会后睡眠等待。

3、这时候，来了一个线程take1，执行了take 操作，这时候发现栈顶为put2 线程，匹配成功，但是实现会先把take1 线程入栈，然后take1 线程循环执行匹配put2 线程逻辑，一旦发现没有并发冲突，就会把栈顶指针直接指向 put1 线程。

4、最后，再来一个线程take2，执行take 操作，这跟步骤3 的逻辑基本是一致的，take2 线程入栈，然后在循环中匹配put1 线程，最终全部匹配完毕，栈变为空，恢复初始状态。

DeplayQueue 延时无界阻塞队列

DelayQueue，DelayQueue 是一个无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部是延迟期满后保存时间最长的Delayed 元素。 DelayQueue 阻塞队列在我们系统开发中也常常会用到，例如：缓存系统的设计，缓存中的对象，超过了空闲时间，需要从缓存中移出；任务调度系统，能够准确的把握任务的执行时间。我们可能需要通过线程处理很多时间上要求很严格的数据，如果使用普通的线程，我们就需要遍历所有的对象，一个一个的检 查看数据是否过期等，首先这样在执行上的效率不会太高，其次就是这种设计的风格也大大的影响了数据的精度。一个需要12:00 点执行的任务可能12:01 才执行,这样对数据要求很高的系统有更大的弊端。由此我们可以使用DelayQueue。

DelayQueue 是一个 BlockingQueue ，其特化的参数是 Delayed 。

Delayed扩展了 Comparable 接口，比较的基准为延时的时间值， Delayed 接口的实现类 getDelay 的返回值应为固定值（ final ）。 DelayQueue 内部是使用 PriorityQueue 实现的。

DelayQueue = BlockingQueue +PriorityQueue + Delayed

DelayQueue 定义和原理

DelayQueue的关键元素 BlockingQueue 、 PriorityQueue 、 Delayed 。可以这么说， DelayQueue 是一个使用优先队列（ Prio rityQueue ）实现的 BlockingQueue 优先队列的比较基准值是时间。

DelayQueue内部的实现使用了一个优先队列。当调用 DelayQueue 的 offer 方法时，把 Delayed 对象加入到优先队列 q 中。

DelayQueue的 take 方法，把优先队列 q 的 first 拿出来（ peek ），如果 没有达到延时阀值，则进行 await处理。

非阻塞队列

与阻塞队列相反，非阻塞队列的执行并不会被阻塞，无论是消费者的出队，还是生产者的入队。 在底层，非阻塞队列使用的是CAS(compare and swap） 来实现线程执行的非阻塞。

与阻塞队列相同，非阻塞队列中的常用方法，也是出队和入队。

入队方法：

- add()add()：底层调用 offer();
- offer() Queue 接口继承下来的方法，实现队列的入队操作，不会阻碍线程的执行，插入成功返回 true

出队方法：

- poll()poll()：移动头结点指针，返回头结点元素，并将头结点元素出队；队列为空，则返回 null
- peek()peek()：移动头结点指针，返回头结点元素，并不会将头结点元素出队；队列为空，则返回 null

非阻塞算法 CAS

CAS乐观锁的实现往往需要硬件的支持，多数处理器都都实现了一个 CAS 指令，实现“ Compare And Swap ”的语义（这里的 swap 是“换入”，也就是 set ），构成了基本的乐观锁。 CAS包含 3 个操作数：

- 需要读写的内存位置 V
- 进行比较的值 A
- 拟写入的新值 B

当且仅当位置V 的值等于 A 时， CAS 才会通过原子方式用新值 B 来更新位置 V 的值；否则不会执行任何操作。无论位置 V 的值是否等于 A ，都将返回 V 原有的值。

ConcurrentLinkedQueue 非阻塞无界链表队列

ConcurrentLinkedQueue是一个线程安全的队列，基于链表结构实现，是一个无界队列，理论上来说队列的长度可以无限扩大。

与其他队列相同，ConcurrentLinkedQueue 也采用的是先进先出（ FIFO ）入队规则，对元素进行排序。当我们向队列中添加元素时，新插入的元素会插入到队列的尾部；而当我们获取一个元素时，它会从队列的头部中取出。 因为ConcurrentLinkedQueue 是链表结构，所以当入队时，插入的元素依次向后延伸，形成链表；而出队时，则从链表的第一个元素开始获取，依次递增；

值得注意的是，在使用ConcurrentLinkedQueue 时，如果涉及到队列是否为空的判断，切记不可使用 size()==0的做法，因为在 size() 方法中，是通过遍历整个链表来实现的，在队列元素很多的时候， size() 方法十分消耗性能和时间，只是单纯的判断队列为空使用 isEmp ty() 即可！！！

ConcurrentLinkedQueue 中有两个 volatile 类型的 Node 节点分别用来存在列表的首尾节点，其中 head节点存放链表第一个 item 为 null 的节点， tail 则并不是总指向最后一个节点。 Node 节点内部则维护一个变量 item用来存放节点的值， next 用来存放下一个节点，从而链接为一个单向无界列表。

```html
public ConcurrentLinkedQueue() {
    head = tail = new Node<E>(null);
}
```

ConcurrentLinkedQuere 方法

Offer 操作 offer操作是在链表末尾添加一个元素。

poll 操作 poll操作是在链表头部获取并且移除一个元素。

size 操作 获取当前队列元素个数，在并发环境下不是很有用，因为使用CAS没有加锁所以从调用size函数到返回结果期间有可能增删元素，导致统计的元素个数不精确。

remove 操作 如果队列里面存在该元素则删除给元素，如果存在多个则删除第一个，并返回true，否者返回false。

contains 操作 判断队列里面是否含有指定对象，由于是遍历整个队列，所以类似size 不是那么精确，有可能调用该方法时候元素还在队列里面，但是遍历过程中才把该元素删除了，那么就会返回false.

ConcurrentLinkedQuere 总结 ConcurrentLinkedQueue 使用CAS 非阻塞算法实现使用CAS 解决了当前节点与next 节点之间的安全链接和对当前节点值的赋值。由于使用CAS 没有使用锁，所以获取size 的时候有可能进行offer，poll 或者remove 操作，导致获取的元素个数不精确，所以在并发情况下size 函数不是很有用。另外第一次peek 或者first 时候会把head 指向第一个真正的队列元素。 下面总结下如何实现线程安全的，可知入队出队函数都是操作volatile 变量：head，tail。所以要保证队列线程安全只需要保证对这两个Node 操作的可见性和原子性，由于volatile 本身保证可见性，所以只需要看下多线程下如果保证对着两个变量操作的原子性。 对于offer 操作是在tail 后面添加元素，也就是调用tail.casNext 方法，而这个方法是使用的CAS 操作，只有一个线程会成功，然后失败的线程会循环一下，重新获取tail，然后执行casNext 方法。对于poll 也是这样的。

ConcurrentHashMap 非阻塞Hash 集合

ConcurrentHashMap是 Java 并发包中提供的一个线程安全且高效的 HashMap 实现。

ConcurrentHashMap是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 是一种可重入锁ReentrantLock ，在 Concurrent HashMap 里扮演锁的角色， HashEntry 则用于存储键值对数据。一个ConcurrentHashMap 里包含一个 Segment 数组， Segment 的结构和 HashMap 类似，是一种数组和链表结构， 一个 Segment 里包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素， 每个 Segment 守护者一个HashEntry 数组里的元素 当对 HashEntry 数组的数据进行修改时，必须首先获得它对应的 Segment 锁。

HashMap：先说 HashMap, HashMap 是线程不安全的，在并发环境下，可能会形成环状链表（扩容时可能造成），导致 get 操作时， cpu 空转，所以，在并发环境中使用 HashMap是非常危险的。

HashTable：HashTable 和 HashMap 的实现原理几乎一样，差别无非是 1.HashTable 不允许 key 和 value 为null 2.HashTable 是线程安全的。但是 HashTable 线程安全的策略实现代价却太大了，简单粗暴， get/put 所有相关操作都是 synchronized 的，这相当于给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞，相当于将所有的操作串行化，在竞争激烈的并发场景中性能就会非常差。如下图

HashTable性能差主要是由于所有操作需要竞争同一把锁，而如果容器中有多把锁，每一把锁锁一段数据，这样在多线程访问时不同段的数据时，就不会存在锁竞争了，这样便可以有效地提高并发效率。这就是ConcurrentHashMap 所采用的 分段锁 思想。

ConcurrentLinkedQuere 源码解析

ConcurrentHashMap采用了非常精妙的 分段锁 策略， ConcurrentHashMap 的主干是个 Segment 数组。

```html
final Segment<K,V>[] segments;
```

Segment 继承了 ReentrantLock ，所以它就是一种可重入锁 ReentrantLock) 。在 ConcurrentHashMap，一个Segment 就是一个子哈希表， Segment 里维护了一个 HashEntry 数组，并发环境下，对于不同 Segment的数据进行操作是不用考虑锁竞争的。

所以，对于同一个Segment 的操作才需考虑线程同步，不同的 Segment 则无需考虑。 Segment 类似于 HashMap一个 Se gment 维护着一个 HashEntry 数组。

```html
transient volatile HashEntry<K,V>[] table;
static final class HashEntry<K,V> {
    final int hash;
    final K key;
    volatile V value;
    volatile HashEntry<K,V> next;
    //其他省略
}
```

Segment 的构造方法

```html
Segment(float lf, int threshold, HashEntry<K,V>[] tab) {
    this.loadFactor = 负载因子
    this.threshold = threshold;// 阈值
    this.table = tab;// 主干数组即 HashEntry 数组
}
```

ConcurrentHashMap 的构造方法

```html
public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrency Level) 
```

初始化方法有三个参数，如果用户不指定则会使用默认值，ini tialCapacity 为 16 loadFactor 为 0.75 （负载因子，扩容时需要参考）， concurrentLevel 为 16 。

Segment 数组的大小 ssize 是由 concurrentLevel 来决定的，但是却不一定等于concurrentLevel， ssize 一定是大于或等于 concurrentLevel 的最小的 2 的次幂。比如：默认情况下concurrentLevel 是 16 ，则 ssize 为 16 ；若 concurrentLevel 为 14 ssize 为 16 ；若 concurrentL evel 为17 ，则 ssize 为 32 。为什么 Segment 的数组大小一定是 2 的次幂？其实主要是便于通过按位与的散列算法来定位 Segment 的 index 。

put 方法

```html
public V put(K key, V value) {
    Segment<K,V> s;
    //concurrentHashMap 不允许 key/value 为空
    if (value == null)
        throw new NullPointerException();
    //hash 函数对 key 的 hashCode 重新散列，避免差劲的不合理的 hashcode ，保证散列均匀
    int hash = hash(key);
    //返回的 hash 值无符号右移 segmentShift 位与段掩码进行位运算，定位 segment
    int j = (hash >>> segmentShift) & segmentMask;
    if ((s = (Segment<K,V>)UNSAFE.getObject // nonvolatile; recheck
        (segments, (j << SSHIFT) + SBASE)) == null) // in ensureSegment
        s = ensureSegment(j);
    return s.put(key, hash, value, false);
}
```

put 的主要逻辑也就两步： 1.定位 segment 并确保定位的 Segment 已初始化 2.调用 Segment 的 put 方法。

segmentShift 和 segmentMask 这两个全局变量的主要作用是用来定位 Segment int j =(hash >>>segmentShift) & segmentMask 。 segmentMask ：段掩码，假如 segments 数组长度为 16 ，则段掩码为 16 1=15 segments 长度为32 ，段掩码为 32 1=31 。这样得到的所有 bit 位都为 1 ，可以更好地保证散列的均匀性. segmentShift：2 的 sshift 次方等于 ssize， segmentShift=32 -sshift 。若 segments 长度为 16，segmentShift=32 -4=28; 若 segments 长度为 32 ，segmentShift=32- 5=27 。而计算得出的 hash 值最大为 32 位，无符号右移 segmentShift ，则意味 着只保留高几位（其余位是没用的），然后与段掩码 segmentMask位运算来定位 Segment 。

Get 操作

```html
public V get(Object key) {
    Segment<K,V> s;
    HashEntry<K,V>[] tab;
    int h = hash(key);
    long u = (((h >>> segmentShift) & segmentMask) << SSHIFT) + SBASE;
    //先定位 Segment ，再定位 HashEntry
    if ((s = (Segment<K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &&(tab = s.table) != null){
        for (HashEn try<K,V> e = (HashEntry<K,V>) UNSAFE.getObjectVolatile(tab, ((long)(((tab.length 1) & h)) << TSHIFT) +                  TBASE);e != null; e = e.next) {
            K k;
            if ((k = e.key) == key || (e.hash == h && key.equals(k)))
                return e.value;
        }
    }
    return null;
}
```

get方法无需加锁，由于其中涉及到的共享变量都使用 volatile 修饰， volatile 可以保证内存可见性，所以不会读取到过期数据。

ConcurrentSkipListMap 非阻塞 Hash 跳表集合

ConcurrentSkipListMap也是和 TreeMap ，它们都是有序的哈希表。但是，它们是有区别的。 第一，它们的线程安全机制不同，TreeMap 是非线程安全的，而 ConcurrentSkipListMap 是线程安全的。 第二， ConcurrentSkipListMap 是通过跳表实现的，而 TreeMap 是通过红黑 树实现的。

什么是 SkipList

Skip list(跳表）是一种可以代替平衡树的数据结构，默认是按照 Key 值升序的。 Skip list 让已排序的数据分布在多层链表中，以 0 1 随机数决定一个数据的向上攀升与否，通过“空间来换取时间”的一个算法，在每个节点中增加了向前的指针，在插入、删除、查找时可以忽略一些不可能涉及到的结点，从而提高了效率。

从概率上保持数据结构的平衡比显示的保持数据结构平衡要简单的多。对于大多数应用，用Skip list 要比用树算法相对简单。由于 Skip list 比较简单，实现起来会比较容易，虽然和平衡树有着相同的时间复杂度 (O( 但是 skiplist 的常数项会相对小很多。 Skip list 在空间上也比较节省。一个节点平均只需要 1.333 个指针（甚至更少）。

SkipList性质 (1) 由很多层结构组成， level 是通过一定的概率随机产生的。

(2)每一层都是一个有序的链表，默认是升序，也可以根据创建映射时所提供的 Comparator 进行排序，具体取决于使用的构造方法。 (3) 最底层 (Level 的链表包含所有元素。 (4) 如果一个元素出现在 Level i 的链表中，则它在 Level i 之下的链表也都会出现。 (5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素。

什么是 ConcurrentSkipListMap ConcurrentSkipListMap提供了一种线程安全的并发访问的排序映射表。内部是 SkipList （跳表）结构实现在理论上能够在 O(log(n)) 时间内完成查找、插入、删除操作。 注意，调用 ConcurrentSkipListMap 的 size时，由于多个线程可以同时对映射表进行操作，所以映射表需要遍历整个链表才能返回元素个数，这个操作 是个 O(log(n)) 的操作。

ConcurrentSkipListMap 数据结构

说明：ConcurrentSkipListMap 的数据结构使用的是跳表，每一个 HeadIndex 、 Index 结点都会包含一个对 Node 的引用，同一垂直方向上的 Index 、 HeadIndex 结点都包含了最底层的 Node 结点的引用。并且层级越高，该层级的结点（ HeadIndex 和 Index ）数越少。 Node 结点之间使用单链表结构。

ConcurrentSkipListMap主要用到了 Node 和 Index 两种节点的存储方式，通过 volatile 关键字实现了并发的操作。

### 3.java.util.concurrent.atomic 包

AtomicBoolean原子性布尔

AtomicBoolean是 java.util.concurrent.atomic 包下的原子变量，这个包里面提供了一组原子类。其基本的特性就是在多线程环境下 ，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到该方法执行完成，才由 JVM 从等待队列中选择一个另一个线程进入，这只是一种逻辑上的理解。实际上是借助硬件的相关指令来实现的，不会阻塞线程 或者说只是在硬件级别上阻塞了 。 AtomicBoolean，在这个 Boolean 值的变化的时候不允许在之间插入，保持操作的原子性。

boolean compareAndSet(expectedValue, updateValue) 这个方法主要两个作用。

1. 比较 AtomicBoolean 和 expect 的值，如果一致，执行方法内的语句。其实就是一个 if 语句。
2. 把 AtomicBoolean 的值设成 updat e, 比较最要的是这两件事是一气呵成的，这连个动作之间不会被打断，任何内部或者外部的语句都不可能在两个动作之间运行。为多线程的控制提供了解决的方案。

创建一个 AtomicBoolean

```html
AtomicBoolean atomicBoolean = new AtomicBoolean();
AtomicBoolean atomicBoolean = new AtomicBoolean(true);
```

获得 AtomicBoolean 的值

```html
boolean value = atomicBoolean.get();
```

设置 AtomicBoolean 的值

```html
atomicBoolean.set(false);
```

交换 AtomicBoolean 的值

你可以 通过 getAndSet() 方法来交换一个 AtomicBoolean 实例的值。 getAndSet() 方法将返回AtomicBoolean 当前的值，并将为 AtomicBoolean 设置一个新值。示例如下：

```html
AtomicBoolean atomicBoolean = new AtomicBoolean(true);
boolean oldValue = atomicBoolean.getAndSet(false);
```

以上代码执行后oldValue 变量的值为 true atomicBoolean 实例将持有 false 值。代码成功将AtomicBoolean 当前值 ture 交换为 false 。

比较并设置 AtomicBoolean 的值

compareAndSet() 方法允许你对 AtomicBoolean 的当前值与一个期望值进行比较，如果当前值等于期望值的话，将会对 AtomicBoolean 设定一个新值。 compareAndSet() 方法是原子性的，因此在同一时间之内有单个线程执行它。因此 c ompareAndSet() 方法可被用于一些类似于锁的同步的简单实现。以下是一个

```html
AtomicBoolean atomicBoolean = new AtomicBoolean(true);
boolean expectedValue = true;
boolean newValue = false;
boolean wasNewValueSet = atomicBoolean.compareAndSet(expectedValue, newValue);
```

AtomicInteger原子性整型

AtomicInteger，一个提供原子操作的 Integer 的类。在 Java 语言中， ，++i 和 i++ 操作并不是线程安全的，在使用的时候，不可避免的会用到 synchronized 关键字。而 AtomicInteger 则通过一种线程安全的加减操作接口。

```html
public final int get() // 获取当前的值
public final int getAndSet(int newValue)// 获取当前的值，并设置新的值
public final int getAndIncrement()// 获取当前的值，并自增
public final int getAndDecrement() // 获取当前的值，并自减
public final int getAndAdd(int delta) 获取当前的值，并加上预期的值
```

使用AtomicInteger 是非常的安全的 而且因为 AtomicInteger 由硬件提供原子操作指令实现的。在非激烈竞争的情况下，开销更小，速度更快。 AtomicInteger 是使用非阻塞算法来实现并发控制的 。

```html
// setup to use Unsafe.compareAndSwapInt for updates
private static final Unsafe unsafe = Unsafe.getUnsafe();
private static final long valueOffset;
static {
    try {
        valueOffset=unsafe.objectFieldOffset(AtomicInteger.class.g etDeclaredField("value"));
    } catch (Exception ex) {
        throw new Error(ex);
    }
}
private volatile int value;
```

这里，unsafe 是 java 提供的获得对对象内存地址访问的类，注释已经清楚的写出了，它的作用就是在更新操作时提供“比较并替换”的作用。实际上就是 AtomicInteger 中的一个工具。 valueOffset 是用来记录 va lue 本身在内存的便宜地址的，这个记录，也主要是为了在更新操作在内存中找到 value 的位置，方便比较。 注意： value 是用来存储整数的时间变量，这里被声明为 volatile ，就是为了保证在更新操作时，当前线程可以拿到 value 最新的值（并发环境下， value 可能已经被其他线程更新了）。 优点：最大的好处就是可以避免多线程的优先级倒置和死锁情况的发生，提升在高并发处理下的性能。

创建一个 Atomic Integer

```html
AtomicInteger atomicInteger = new AtomicInteger();
AtomicInteger atomicInteger = new AtomicInteger(123);
```

获得 Atomic Integer 的值

```html
int theValue = atomicInteger.get();
```

设置 Atomic Integer 的值

```html
atomicInteger.set(234);
```

比较并设置 AtomicInteger 的值

AtomicInteger 类也通过了一个原子性的 compareAndSet() 方法。这一方法将 AtomicInteger 实例的当前值与期望值进行比较，如果二者相等，为 AtomicInteger 实例设置一个新值。

```html
AtomicInteger atomicInteger = new AtomicInteger(123);
int expectedValue = 123;
int newValue = 234;
atomicInteger.compareAndSet(expectedValue, newValue);
```

首先新建一个初始值为 123 的 AtomicInteger 实例。然后将 AtomicInteger 与期望值 123 进行比较，如果相等，将 AtomicInteger 的值更新为 234 。

增加 Atomic Integer 的值

```html
public final int add And Get (int add Value)// 在原来的数值上增加 新的值 ，并返回新值
public final int getAndIncrement()// 获取当前的值，并自增
public final int i ncrement And get() // 自减 ，并获得自减后的值
public final int getAndAdd(int delta) // 获取当前的值，并加上预期的值
```

第一个 addAndGet() 方法给 AtomicInteger 增加了一个值，然后返回增加后的值。 getAndAdd() 方法为AtomicInteger 增加了一个值，但返回的是增加以前的 AtomicInteger 的值。

减小 Atomic Integer 的值

```html
public final int decrementAndGet
public final int getAndDecrement()
```

decrementAndGet() 将 AtomicInteger 的值减一，并返回减一后的值。getAndDecrement() 也将 AtomicInteger 的值减一，但它返回的是减一之前的值。

AtomicIntegerArray原子性整型数组

java.util.concurrent.atomic.AtomicIntegerArray类提供了可以以原子方式读取和写入的底层 int 数组的操作，还包含高级原子操作。 Atomi cIntegerArray 支持对底层 int 数组变量的原子操作。 它具有获取和设置方法，如在变量上的读取和写入。 也就是说，一个集合与同一变量上的任何后续 get 相关联。 原子 compareAndSet 方法也具有这些内存一致性功能。

AtomicIntegerArray本质上是对 int[] 类型的封装。使用 Unsafe 类通过 CAS 的方式控制 int[] 在多线程下的安全性。

```html
获得数组第 i 个下标的元素
public final int get(int i)
获得数组的长度
publi c final int length()
将数组第 i 个下标设置为 newValue ，并返回旧的值
public final int getAndSet(int i, int newValue)
进行 CAS 操作，如果第 i 个下标的元素等于 expect ，则设置为 update ，设置成功返回 true
public final boolean compareAndSet(int i, int expect, int update)
将第 i 个下标的元素加 1
public final int getAndIncrement(int i)
将第 i 个下标的元素减 1
public final int getAndDecrement(int i)
将第 i 个下标的元素增加 delta delta 可以是负数）
public final int getAndAdd(int i, int delta)
```

AtomicLong、 AtomicLongArray原子性整型数组

AtomicLong、 AtomicLongArray 的 API 跟 AtomicInteger 、 AtomicIntegerArray 在使用方法都是差不多的。区别在于用前者是使用原子方式更新的 long 值和 long 数组，后者是使用原子方式更新的 Integer 值和 Integer 数组。两者的相同处在于它们此类确实扩展了 Number ，允许那些处理基于数字类的工具和实用工具进行统一访问。

### 4.java.util.concurrent. lock 包

## 面试题

### 1.如何控制某个方法允许并发访问线程的个数？

可以使用Semaphore 控制，创建了一个 Semaphore 对象，并且初始化 5 个信号。semaphore.acquire()请求一个信号（消费一个信号），如果信号被用完了则等待，semaphore.release()释放一个信号，释放的信号新的线程就可以使用了。

### 2.三个线程 a、 b、 c并发运行， b,c需要 a线程的数据怎么实现

### 3.同一个类中的 2个方法都加了同步锁，多个线程能同时访问同一个类中的这两个方法吗？

多个线程不可访问同一个类中的 2 个加了 Lock 锁的方法。

使用 synchronized 时，当我们访问同一个类对象的时候，是同一把锁，所以可以访问该对象的其他 synchronized 方法。

### 4.启动一个线程是用run()还是start()?

启动一个线程是调用start() 方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由 JVM 调度并执行，这并不意味着线程就会立即运行。 run()方法是线程启动后要进行回调（ callback ）的方法。

### 5.高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？

1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 c）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考其他有关线程池的文章。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦

### 6.CopyOnWrite容器

Copy-On-Write即写时复制，简称COW。其思想是当我们往一个容器中添加元素时，不是直接新增，而是对当前容器进行Copy，复制出一份一模一样的新容器，然后往新容器中添加元素，添加成功后，再将原容器的引用指向新的容器，在读的时候仍然是读原容器，替换引用后就改为读新容器了。这样做的一个好处就是读写分离了，读的使用可以使用并发读并且是不需要加锁的。当然这种读写分离的思想是读和写分别在不同的容器。

虽然CopyOnWrite容器采用读写分离的思路避免了线程安全的问题，但是它仍然存在一些缺陷：内存占用和数据不一致问题。 内存占用：因为在写的时候采用复制的思想，那么写的时候内存里面会同时驻扎两个数组对象的内存。如果对象占用内存空间较大，那么可能会引发频繁的Yong GC和Full GC。 数据不一致问题：因为时效性的问题，写的时候读不一定可以里面读取到数据，例如你add(1)，那么在get(1)的时候不一定能够得到，所以CopyOnWrite容器只能保证数据的最终一致性。 最后，CopyOnWrite容器适用于读多写少的并发场景，如果写太频繁了则会导致大量数据复制操作，这会严重影响系统的性能。

### 7.如果你提交任务时，线程池队列已满，这时会发生什么

这里区分一下： 1、如果使用的是无界队列LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务。

2、如果使用的是有界队列比如ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中， ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务， 默认是AbortPolicy。

### 8.Java 线程数过多会造成什么异常？

1、线程的生命周期开销非常高 2、消耗过多的CPU 资源 如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争CPU资源时还将产生其他性能的开销。 3、降低稳定性 JVM 在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。

### 9.规范

给线程起个有意义的名字，这样可以方便找 Bug。 缩小同步范围，例如对于 synchronized，应该尽量使用同步块而不是同步方法。 多用同步类少用 wait() 和 notify()。首先，CountDownLatch, CyclicBarrier, Semaphore 和 Exchanger 这些同步类简化了编码操作，而用 wait() 和notify() 很难实现对复杂的控制流；其次，这些同步类是由最好的企业编写和维护，在后续的 JDK 中还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合，例如应该使用 ConcurrentHashMap 而不是 Hashtable。 使用本地变量和不可变类来保证线程安全。 使用线程池而不是直接创建 Thread 对象，这是因为创建线程代价很高，线程池可以有效地利用有限的线程来启动任务。 使用 BlockingQueue 实现生产者消费者问题。

### 10.并发包(J.U.C)下面，都用过什么

concurrent下面的包 Executor 用来创建线程池，在实现Callable接口时，添加线程。 FeatureTask 此 FutureTask 的 get 方法所返回的结果类型。 TimeUnit Semaphore LinkedBlockingQueue 所用过的类 Executor

### 11.规范

【强制】线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。

```html
public ThreadPoolExecutor(int corePoolSize,
 int maximumPoolSize,
 long keepAliveTime,
 TimeUnit unit,
 BlockingQueue<Runnable> workQueue) {
 this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
 Executors.defaultThreadFactory(), defaultHandler);
 }
```

这个方法里面有几个参数 corePoolSize 要保留在池中的线程数，也就是线程池核心池的大小 maximumPoolSize 最大线程数 keepAliveTime 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。 unit keepAliveTime 参数的时间单位 workQueue 用来储存等待执行任务的队列。 threadFactory 线程工厂 handler 默认的拒绝执行处理程序

### 12.乐观锁和悲观锁

乐观锁(Optimistic Locking)

乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来 的就是数据库 性能的大量开销， 特别是对长事务而言，这样的开销往往无法承受。相对悲观锁而言，乐观锁更倾向于开发运用。

乐观锁，大多是基于数据版本（ Version ）记录机制实现。 何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决 方案中，一般是通过为数据库表增加一个 “version” 字段来实现。 读取出数据时，将此版本号一同读出，之后更新时，对此版本号加 一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对， 如果提交的数据版本号大于数据库表当前版本号， 则予以更新，否则认为是过期数据。

悲观锁

指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处 理）修改持保守态度， 因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有 数据库层提供的锁机制才能真正保证数据访问的排他性， 否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 因为悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数 据就会阻塞直到它拿到锁。

### 13.如何让 10 个线程按照顺序打印 0123456789？（写代码实现）

1. 设定一个 orderNum，每个线程执行结束之后，更新 orderNum，指明下一个要执行的线程。并且唤醒所有的等待线程。
2. 在每一个线程的开始，要 while 判断 orderNum 是否等于自己的要求值！！不是，则 wait，是则执行本线程。

### 14.如何让一段程序并发的执行，并最终汇总结果？

- 1、CountDownLatch：允许一个或者多个线程等待前面的一个或多个线程完成，构造一个 CountDownLatch 时指定需要 countDown 的点的数量，每完成一点就 countDown 一下。当所有点都完成，CountDownLatch 的 `#await()` 就解除阻塞。

- 2、CyclicBarrier：可循环使用的 Barrier ，它的作用是让一组线程到达一个 Barrier 后阻塞，直到所有线程都到达 Barrier 后才能继续执行。

  > CountDownLatch 的计数值只能使用一次，CyclicBarrier 可以通过使用 reset 重置，还可以指定到达栅栏后优先执行的任务。

- 3、Fork/Join 框架，fork 把大任务分解成多个小任务，然后汇总多个小任务的结果得到最终结果。使用一个双端队列，当线程空闲时从双端队列的另一端领取任务。

### 15.有三个线程T1,T2,T3,如何保证顺序执行？

在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个 线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用 T2，T2调用T1)，这样T1就会先完成而T3最后完成。 实际上先启动三个线程中哪一个都行， 因为在每个线程的run方法中用join方法限定了三个线程的执行顺序

## 阻塞栈

### 1.阻塞栈

对于阻塞栈，与阻塞队列相似。不同点在于栈是“后入先出”的结构，每次操作的是栈顶，而队列是“先进先出”的结构，每次操作的是队列头。 这里要特别说明一点的是，阻塞栈是Java6 的新特征。 Java 为阻塞栈定义了接口： java.util.concurrent.BlockingDeque 。

## Java 内存模型（JMM）

### 1.主内存与工作内存

处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。 加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。 所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。

### 2.内存间交互操作

Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。

- read：把一个变量的值从主内存传输到工作内存中
- load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中
- use：把工作内存中一个变量的值传递给执行引擎
- assign：把一个从执行引擎接收到的值赋给工作内存的变量
- store：把工作内存的一个变量的值传送到主内存中
- write：在 store 之后执行，把 store 得到的值放入主内存的变量中
- lock：作用于主内存的变量，把一个变量标识为一条线程独占状态
- unlock：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定

如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行 read 和 load 操作，如果把变量从工作内存中同步回主内存中，就要按顺序地执行 store和 write 操作。Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。也就是 read 和 load 之间，store 和 write 之间是可以插入其他指令的，如对主内存中的变量a、b进行访问时，可能的顺序是read a，read b，load b， load a。

Java内存模型还规定了在执行上述8种基本操作时，必须满足如下规则：

- 不允许 read 和 load、store 和 write 操作之一单独出现
- 不允许一个线程丢弃它的最近 assign 的操作，即变量在工作内存中改变了之后必须同步到主内存中
- 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中
- 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量。即就是对一个变量实施 use 和 store 操作之前，必须先执行过了 assign 和 load 操作。
- 一个变量在同一时刻只允许一条线程对其进行lock操作，lock 和 unlock必须成对出现
- 如果对一个变量执行 lock 操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行 load 或 assign 操作初始化变量的值
- 如果一个变量事先没有被 lock 操作锁定，则不允许对它执行 unlock 操作；也不允许去 unlock 一个被其他线程锁定的变量。
- 对一个变量执行 unlock 操作之前，必须先把此变量同步到主内存中（执行 store 和 write 操作）。

### 3.内存模型三大特性

1.原子性

概念 事物有原子性，这个概念大概都清楚，即一个操作或多个操作要么执行的过程中不被任何因素打断，要么不执行。 如何实现原子性？ 通过同步代码块 synchronized 或者 local 锁来确保原子性

Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即load、store、read 和 write 操作可以不具备原子性。

AtomicInteger 能保证多个线程修改的原子性。

除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。

2.可见性

可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。 主要有有三种实现可见性的方式：

- volatile
- synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。
- final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。

对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。

3.有序性

有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。 在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。

### 4.指令重排序

在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。 指令重排序包括：编译器重排序和处理器重排序 重排序分三种类型：

1. 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
2. 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理 器可以改变语句对应机器指令的执行顺序。
3. 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。

从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel 称之为 memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。

数据依赖性

如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型：

- 写后读，a = 1;b = a; 写一个变量之后，再读这个位置。
- 写后写，a = 1;a = 2; 写一个变量之后，再写这个变量。
- 读后写，a = b;b = 1; 读一个变量之后，再写这个变量。

上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。

as-if-serial语义

as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和 处理器 都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。

重排序对多线程的影响

在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。

### 5.先行发生原则（happens-before）

Happens-before 是用来指定两个操作之间的执行顺序。提供跨线程的内存可见性。 在 Java 内存模型中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必然存在 happens-before 关系。 上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。

1. 单一线程原则：在一个线程内，在程序前面的操作先行发生于后面的操作。
2. 管程锁定规则：对一个锁的解锁（unlock ），总是 happens-before 于随后对这个锁的加锁（lock）
3. volatile 变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。
4. 线程启动规则：Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。
5. 线程加入规则：Thread 对象的结束先行发生于 join() 方法返回。
6. 线程中断规则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。
7. 对象终结规则：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。
8. 传递性：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。

### 6.线程安全

线程安全定义

一个类在可以被多个线程安全调用时就是线程安全的。

线程安全分类

线程安全不是一个非真即假的命题，可以将共享数据按照安全程度的强弱顺序分成以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立。

1.不可变

不可变（Immutable）的对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要再采取任何的线程安全保障措施，只要一个不可变的对象被正确地构建出来，那其外部的可见状态永远也不会改变，永远也不会看到它在多个线程之中处于不一致的状态。

不可变的类型：

- final 关键字修饰的基本数据类型；
- String
- 枚举类型
- Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的子类型的原子类 AtomicInteger和 AtomicLong 则并非不可变的。

多线程环境下，应当尽量使对象成为不可变，来满足线程安全。

2.绝对线程安全

不管运行时环境如何，调用者都不需要任何额外的同步措施。

3.相对线程安全

相对的线程安全需要保证对这个对象单独的操作是线程安全的，在调用的时候不需要做额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。 在 Java 语言中，大部分的线程安全类都属于这种类型，例如 Vector、HashTable、Collections 的 synchronizedCollection() 方法包装的集合等。

4.线程兼容 线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用，我们平常说一个类不是线程安全的，绝大多数时候指的是这一种情况。Java API 中大部分的类都是属于线程兼容的，如与前面的 Vector 和 HashTable 相对应的集合类 ArrayList 和HashMap 等。

5.线程对立 线程对立是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码。由于 Java 语言天生就具备多线程特性，线程对立这种排斥多线程的代码是很少出现的，而且通常都是有害的，应当尽量避免。

### 7.线程安全的实现方法

1.阻塞同步（互斥同步）

synchronized 和 ReentrantLock。 互斥同步最主要的问题就是进行线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。 互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。

2.非阻塞同步

随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种同步操作称为非阻塞同步。 乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。 硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。 J.U.C 包里面的整数原子类 AtomicInteger，其中的 compareAndSet() 和 getAndIncrement() 等方法都使用了 Unsafe 类的 CAS 操作。

3.无同步方案

要保证线程安全，并不是一定就要进行同步，两者没有因果关系。同步只是保证共享数据争用时的正确性的手段，如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，因此会有一些代码天生就是线程安全的。 （一）可重入代码（Reentrant Code） 这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。 可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。 （二）栈封闭 多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在栈中，属于线程私有的。

（三）线程本地存储（Thread Local Storage） 如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。 符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完，其中最重要的一个应用实例就是经典 Web 交互模型中的 “一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。 可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。

## 并发容器

### 1.什么是并发容器的实现？

何为同步容器？可以简单地理解为通过 `synchronized`来实现同步的容器，如果有多个线程调用同步容器的方法，它们将会串行执行。

- 比如 Vector，Hashtable，以及 `Collections#synchronizedSet()`，`Collections#synchronizedList()` 等方法返回的容器。
- 可以通过查看 Vector，Hashtable 等这些同步容器的实现代码，可以看到这些容器实现线程安全的方式就是将它们的状态封装起来，并在需要同步的方法上加上关键字 `synchronized` 。

并发容器，使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性。

- 例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁。在这种锁机制下，允许任意数量的读线程并发地访问 map ，并且执行读操作的线程和写操作的线程也可以并发的访问 map ，同时允许一定数量的写操作线程并发地修改 map ，所以它可以在并发环境下实现更高的吞吐量。
- 再例如，CopyOnWriteArrayList 。

### 2.SynchronizedMap 和 ConcurrentHashMap 有什么区别？

- SynchronizedMap
  - 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map 。
- ConcurrentHashMap
  - 使用分段锁来保证在多线程下的性能。ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将 hash 表分为 16 个桶，诸如 get,put,remove 等常用操作只锁当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有 16 个写线程执行，并发性能的提升是显而易见的。【注意，这块是 JDK7 的实现。在 JDK8 中，具体的实现已经改变】
  - 另外 ConcurrentHashMap 使用了一种不同的迭代方式。在这种迭代方式中，当 iterator 被创建后集合再发生改变就不再是抛出 ConcurrentModificationException 异常，取而代之的是在改变时 `new` 新的数据从而不影响原有的数据，iterator 完成后再将头指针替换为新的数据 ，这样 iterator 线程可以使用原来老的数据，而写线程也可以并发的完成改变。

### 3.**Java 中 ConcurrentHashMap 的并发度是什么？**

在 JDK8 前，ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16 ，这样在多线程情况下就能避免争用。

在 JDK8 后，它摒弃了 Segment（锁段）的概念，而是启用了一种全新的方式实现，利用 CAS 算法。同时加入了更多的辅助变量来提高并发度

### 4.**ConcurrentHashMap 为何读不用加锁？**

在 JDK7 以及以前

- HashEntry 中的key、hash、next均为final类型，只能表头插入、删除结点。
  - HashEntry 类的 `value` 域被声明为 `volatile` 型。
  - 不允许用 `null` 作为键和值，当读线程读到某个 HashEntry 的 `value` 域的值为 `null` 时，便知道产生了冲突——发生了重排序现象（put 方法设置新 `value` 对象的字节码指令重排序），需要加锁后重新读入这个 `value` 值。
- `volatile` 变量 `count` 协调读写线程之间的内存可见性，写操作后修改 `count` ，读操作先读 `count`，根据 happen-before 传递性原则写操作的修改读操作能够看到。

在 JDK8 开始

- Node 的 `val` 和 `next` 均为 `volatile` 型。
- `#tabAt(..,)` 和 `#casTabAt(...)` 对应的 Unsafe 操作实现了 `volatile` 语义。

## 十、Java 内存模型

Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。

### 主内存与工作内存

处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。

加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。

所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。

线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。

### 内存间交互操作

Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。

- read：把一个变量的值从主内存传输到工作内存中
- load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中
- use：把工作内存中一个变量的值传递给执行引擎
- assign：把一个从执行引擎接收到的值赋给工作内存的变量
- store：把工作内存的一个变量的值传送到主内存中
- write：在 store 之后执行，把 store 得到的值放入主内存的变量中
- lock：作用于主内存的变量
- unlock

### 内存模型三大特性

\1. 原子性

Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。

有一个错误认识就是，int 等原子性的类型在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。

为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。

下图演示了两个线程同时对 cnt 进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入旧值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。

AtomicInteger 能保证多个线程修改的原子性。

除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。

\2. 可见性

可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。

主要有三种实现可见性的方式：

- volatile
- synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。
- final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。

对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。

\3. 有序性

有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。

也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。

### 先行发生原则

上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。

\1. 单一线程原则

> Single Thread rule

在一个线程内，在程序前面的操作先行发生于后面的操作。

\2. 管程锁定规则

> Monitor Lock Rule

一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。

\3. volatile 变量规则

> Volatile Variable Rule

对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。

\4. 线程启动规则

> Thread Start Rule

Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。

\5. 线程加入规则

> Thread Join Rule

Thread 对象的结束先行发生于 join() 方法返回。

\6. 线程中断规则

> Thread Interruption Rule

对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。

\7. 对象终结规则

> Finalizer Rule

一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。

\8. 传递性

> Transitivity

如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。

## 十一、线程安全

多个线程不管以何种方式访问某个类，并且在主调代码中不需要进行同步，都能表现正确的行为。

线程安全有以下几种实现方式：

### 不可变

不可变（Immutable）的对象一定是线程安全的，不需要再采取任何的线程安全保障措施。只要一个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不一致的状态。多线程环境下，应当尽量使对象成为不可变，来满足线程安全。

不可变的类型：

- final 关键字修饰的基本数据类型
- String
- 枚举类型
- Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的原子类 AtomicInteger 和 AtomicLong 则是可变的。

对于集合类型，可以使用 Collections.unmodifiableXXX() 方法来获取一个不可变的集合。

```html
public class ImmutableExample {
    public static void main(String[] args) {
        Map<String, Integer> map = new HashMap<>();
        Map<String, Integer> unmodifiableMap = Collections.unmodifiableMap(map);
        unmodifiableMap.put("a", 1);
    }
}
Exception in thread "main" java.lang.UnsupportedOperationException
    at java.util.Collections$UnmodifiableMap.put(Collections.java:1457)
    at ImmutableExample.main(ImmutableExample.java:9)
```

Collections.unmodifiableXXX() 先对原始的集合进行拷贝，需要对集合进行修改的方法都直接抛出异常。

```html
public V put(K key, V value) {
    throw new UnsupportedOperationException();
}
```

### 互斥同步

synchronized 和 ReentrantLock。

### 非阻塞同步

互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。

互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。

随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。

\1. CAS

乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。

\2. AtomicInteger

J.U.C 包里面的整数原子类 AtomicInteger 的方法调用了 Unsafe 类的 CAS 操作。

以下代码使用了 AtomicInteger 执行了自增的操作。

```html
private AtomicInteger cnt = new AtomicInteger();

public void add() {
    cnt.incrementAndGet();
}
```

以下代码是 incrementAndGet() 的源码，它调用了 Unsafe 的 getAndAddInt() 。

```html
public final int incrementAndGet() {
    return unsafe.getAndAddInt(this, valueOffset, 1) + 1;
}
```

以下代码是 getAndAddInt() 源码，var1 指示对象内存地址，var2 指示该字段相对对象内存地址的偏移，var4 指示操作需要加的数值，这里为 1。通过 getIntVolatile(var1, var2) 得到旧的预期值，通过调用 compareAndSwapInt() 来进行 CAS 比较，如果该字段内存地址中的值等于 var5，那么就更新内存地址为 var1+var2 的变量为 var5+var4。

可以看到 getAndAddInt() 在一个循环中进行，发生冲突的做法是不断的进行重试。

```html
public final int getAndAddInt(Object var1, long var2, int var4) {
    int var5;
    do {
        var5 = this.getIntVolatile(var1, var2);
    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));

    return var5;
}
```

\3. ABA

如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。

J.U.C 包提供了一个带有标记的原子引用类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更高效。

### 无同步方案

要保证线程安全，并不是一定就要进行同步。如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性。

\1. 栈封闭

多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。

```html
public class StackClosedExample {
    public void add100() {
        int cnt = 0;
        for (int i = 0; i < 100; i++) {
            cnt++;
        }
        System.out.println(cnt);
    }
}
public static void main(String[] args) {
    StackClosedExample example = new StackClosedExample();
    ExecutorService executorService = Executors.newCachedThreadPool();
    executorService.execute(() -> example.add100());
    executorService.execute(() -> example.add100());
    executorService.shutdown();
}
100
100
```

\2. 线程本地存储（Thread Local Storage）

如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。

符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完。其中最重要的一个应用实例就是经典 Web 交互模型中的“一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。

可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。

对于以下代码，thread1 中设置 threadLocal 为 1，而 thread2 设置 threadLocal 为 2。过了一段时间之后，thread1 读取 threadLocal 依然是 1，不受 thread2 的影响。

```html
public class ThreadLocalExample {
    public static void main(String[] args) {
        ThreadLocal threadLocal = new ThreadLocal();
        Thread thread1 = new Thread(() -> {
            threadLocal.set(1);
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            System.out.println(threadLocal.get());
            threadLocal.remove();
        });
        Thread thread2 = new Thread(() -> {
            threadLocal.set(2);
            threadLocal.remove();
        });
        thread1.start();
        thread2.start();
    }
}
1
```

每个 Thread 都有一个 ThreadLocal.ThreadLocalMap 对象。

```html
/* ThreadLocal values pertaining to this thread. This map is maintained
 * by the ThreadLocal class. */
ThreadLocal.ThreadLocalMap threadLocals = null;
```

当调用一个 ThreadLocal 的 set(T value) 方法时，先得到当前线程的 ThreadLocalMap 对象，然后将 ThreadLocal->value 键值对插入到该 Map 中。

```html
public void set(T value) {
    Thread t = Thread.currentThread();
    ThreadLocalMap map = getMap(t);
    if (map != null)
        map.set(this, value);
    else
        createMap(t, value);
}
```

get() 方法类似。

```html
public T get() {
    Thread t = Thread.currentThread();
    ThreadLocalMap map = getMap(t);
    if (map != null) {
        ThreadLocalMap.Entry e = map.getEntry(this);
        if (e != null) {
            @SuppressWarnings("unchecked")
            T result = (T)e.value;
            return result;
        }
    }
    return setInitialValue();
}
```

ThreadLocal 从理论上讲并不是用来解决多线程并发问题的，因为根本不存在多线程竞争。

在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。

\3. 可重入代码（Reentrant Code）

这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。

可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。

## synchronized

### 1.synchronized

synchronized关键字用于为Java对象、方法、代码块提供线程安全的操作。synchronized属于独占式的悲观锁，同时属于可重入锁。在使用synchronized修饰对象时，同一时刻只能有一个线程对该对象进行访问；在synchronized修饰方法、代码块时，同一时刻只能有一个线程执行该方法体或代码块，其他线程只有等待当前线程执行完毕并释放锁资源后才能访问该对象或执行同步代码块。 Java中的每个对象都有个monitor对象，加锁就是在竞争monitor对象。对代码块加锁是通过在前后分别加上monitorenter 和monitorexit指令实现的，对方法是否加锁是通过一个标记位来判断的。

### 2.synchronized的作用范围

- synchronized作用于成员变量和非静态方法时，锁住的是对象的实例，即this对象。
- synchronized作用于静态方法时，锁住的是Class 实例，又因为Class 的相关数据存储在永久带PermGen（jdk1.8 则是metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁，会锁所有调用该方法的线程；
- synchronized作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。

### 3.synchronized核心组件

在synchronized 内部包括ContentionList 、EntryList 、WaitSet、OnDeck、Owner、!Owner这6个区域，每个区域的数据都代表锁的不同状态。

- ContentionList：锁竞争队列，所有请求锁的线程都被放在竞争队列中。
- EntryList：竞争候选列表，在Contention List中有资格成为候选者来竞争锁资源的线程被移动到了Entry List中。
- WaitSet：等待集合，调用wait方法后被阻塞的线程将被放在WaitSet中。
- OnDeck：竞争候选者，在同一时刻最多只有一个线程在竞争锁资源，该线程的状态被称为OnDeck。
- Owner：竞争到锁资源的线程被称为Owner状态线程。
- !Owner ： 在Owner 线程释放锁后， 会从Owner 的状态变成!Owner。

### 4.synchronized实现

1. JVM 每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList 会被大量的并发线程进行CAS 访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList 中作为候选竞争线程。
2. Owner 线程会在unlock 时，将ContentionList 中的部分线程迁移到EntryList 中，并指定EntryList 中的某个线程为OnDeck 线程（一般是最先进去的那个线程）。
3. Owner 线程并不直接把锁传递给OnDeck 线程，而是把锁竞争的权利交给OnDeck，OnDeck 需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM 中，也把这种选择行为称之为“竞争切换”。
4. OnDeck 线程获取到锁资源后会变为Owner 线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner 线程被wait 方法阻塞，则转移到WaitSet 队列中，直到某个时刻通过notify或者notifyAll 唤醒，会重新进去EntryList 中。
5. 处于ContentionList、EntryList、WaitSet 中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux 内核下采用pthread_mutex_lock 内核函数实现的）。
6. Synchronized 是非公平锁。 Synchronized 在线程进入ContentionList 时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占OnDeck 线程的锁资源。
7. 每个对象都有个monitor 对象，加锁就是在竞争monitor 对象，代码块加锁是在前后分别加上monitorenter 和monitorexit 指令来实现的，方法加锁是通过一个标记位来判断的
8. synchronized 是一个重量级操作，需要调用操作系统相关接口，性能是低效的，有可能给线程加锁消耗的时间比有用操作消耗的时间更多。
9. Java1.6，synchronized 进行了很多的优化，有适应自旋、锁消除、锁粗化、轻量级锁及偏向锁等，效率有了本质上的提高。在之后推出的Java1.7 与1.8 中，均对该关键字的实现机理做了优化。引入了偏向锁和轻量级锁。都是在对象头中有标记位，不需要经过操作系统加锁。
10. 锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。这种升级过程叫做锁膨胀；
11. JDK 1.6 中默认是开启偏向锁和轻量级锁，可以通过-XX:-UseBiasedLocking 来禁用偏向锁。

### 5.syncrhonized

加锁，syncrhonized，一旦说某个线程加了一把锁之后，就会保证，其他的线程没法去读取和修改这个变量的值了，同一时间，只有一个线程可以读这个数据以及修改这个数据，别的线程都会卡在尝试获取锁那儿

修饰普通方法

直接synchronized修饰一个普通的方法，那么就是对当前这个对象实例在加锁，访问同一个对象实例的synchronized方法，同一时间只有一个线程可以做到

```html
synchronized(myObject) {
}
```

但是如果是两个线程，分别进入不同的对象的synchronized方法或者代码片段，这个没事，因为是在不同的对象上加锁。

其实synchronized一个代码片段，有更加常见的一种写法，就是用this，其实意思就是基于当前这个对象实例来加锁：

```html
synchronized(this) {
}
```

修饰静态方法

synchronized一个静态方法，就是对这个类的Class对象加锁，每个类都对应了一个Class对象，那么对同一个类的synchronized静态方法，同一时间只能有一个线程加锁进入其中

```html
synchronized(MyObject.class) {
}
```

### 6.synchronized底层原理

synchronized底层的原理，是跟jvm指令和monitor有关系的

你如果用到了synchronized关键字，在底层编译后的jvm指令中，会有monitorenter和monitorexit两个指令

```html
monitorenter
// 代码对应的指令
monitorexit
```

每个对象都有一个关联的monitor，比如一个对象实例就有一个monitor，一个类的Class对象也有一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联的monitor的lock锁

monitor里面有一个计数器，从0开始的。如果一个线程要获取monitor的锁，就看看他的计数器是不是0，如果是0的话，那么说明没人获取锁，他就可以获取锁了，然后对计数器加1

这个monitor的锁是支持重入加锁的，如果一个线程第一次synchronized那里，获取到了myObject对象的monitor的锁，计数器加1，然后第二次synchronized那里，会再次获取myObject对象的monitor的锁，这个就是重入加锁了，然后计数器会再次加1，变成2

这个时候，其他的线程在第一次synchronized那里，会发现说myObject对象的monitor锁的计数器是大于0的，意味着被别人加锁了，然后此时线程就会进入block阻塞状态，什么都干不了，就是等着获取锁

接着如果出了synchronized修饰的代码片段的范围，就会有一个monitorexit的指令，在底层。此时获取锁的线程就会对那个对象的monitor的计数器减1，如果有多次重入加锁就会对应多次减1，直到最后，计数器是0

然后后面block住阻塞的线程，会再次尝试获取锁，但是只有一个线程可以获取到锁

### 7.32位Java虚拟机中的long和double变量写操作为何不是原子的？

原子性这块，特例，32位虚拟机里的long/double类型的变量的简单赋值写操作，不是原子的，long i = 30，double c = 45.0，在32位虚拟机里就不是原子的，因为long和double是64位的

0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000

如果多个线程同时并发的执行long i = 30，long是64位的，就会导致有的线程在修改i的高32位，有的线程在修改i的低32位，多线程并发给long类型的变量进行赋值操作，在32位的虚拟机下，是有问题的

就可能会导致多线程给long i = 30赋值之后，导致i的值不是30，可能是-3333344429，乱码一样的数字，就是因为高低32位赋值错了，就导致二进制数字转换为十进制之后是一个很奇怪的数字

如果对变量加上了volatile，就可以保证在32位java虚拟机里面，对long/double变量的赋值写是原子的了

### 8.可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器

每个处理器都有自己的寄存器（register），所以多个处理器各自运行一个线程的时候，可能导致某个变量给放到寄存器里去，接着就会导致各个线程没法看到其他处理器寄存器里的变量的值修改了

可见性的第一个问题，首先，就有可能在寄存器的级别，导致变量副本的更新，无法让其他处理器看到

然后一个处理器运行的线程对变量的写操作都是针对写缓冲来的（store buffer）并不是直接更新主内存，所以很可能导致一个线程更新了变量，但是仅仅是在写缓冲区里罢了，没有更新到主内存里去

这个时候，其他处理器的线程是没法读到他的写缓冲区的变量值的，所以此时就是会有可见性的问题，这是第二个可见性发生的场景

然后即使这个时候一个处理器的线程更新了写缓冲区之后，将更新同步到了自己的高速缓存里（cache，或者是主内存），然后还把这个更新通知给了其他的处理器，但是其他处理器可能就是把这个更新放到无效队列里去，没有更新他的高速缓存

此时其他处理器的线程从高速缓存里读数据的时候，读到的还是过时的旧值

可见性发生的问题

如果要实现可见性的话，其中一个方法就是通过MESI协议，这个MESI协议实际上有很多种不同的时间，因为他不过就是一个协议罢了，具体的实现机制要靠具体底层的系统如何实现

根据具体底层硬件的不同，MESI协议的实现是有区别的

比如说MESI协议有一种实现，就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样大家的缓存不就实现同步了，然后各个处理器的线程看到的数据就一样了

为了实现MESI协议，有两个配套的专业机制要给大家说一下：flush处理器缓存、refresh处理器缓存。

flush处理器缓存，他的意思就是把自己更新的值刷新到高速缓存里去（或者是主内存），因为必须要刷到高速缓存（或者是主内存）里，才有可能在后续通过一些特殊的机制让其他的处理器从自己的高速缓存（或者是主内存）里读取到更新的值

除了flush以外，他还会发送一个消息到总线（bus），通知其他处理器，某个变量的值被他给修改了

refresh处理器缓存，他的意思就是说，处理器中的线程在读取一个变量的值的时候，如果发现其他处理器的线程更新了变量的值，必须从其他处理器的高速缓存（或者是主内存）里，读取这个最新的值，更新到自己的高速缓存中

所以说，为了保证可见性，在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一整套机制来保障的

要记住，flush和refresh，这两个操作，flush是强制刷新数据到高速缓存（主内存），不要仅仅停留在写缓冲器里面；refresh，是从总线嗅探发现某个变量被修改，必须强制从其他处理器的高速缓存（或者主内存）加载变量的最新值到自己的高速缓存里去

内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh，MESI协议是如何与内存屏障搭配使用的（flush、refresh）

volatile boolean isRunning = true;

isRunning = false; => 写volatile变量，就会通过执行一个内存屏障，在底层会触发flush处理器缓存的操作；while(isRunning) {}，读volatile变量，也会通过执行一个内存屏障，在底层触发refresh操作

之前给大家讲过那个volatile关键字的作用，对一个变量加了volatile修饰之后，对这个变量的写操作，会执行flush处理器缓存，把数据刷到高速缓存（或者是主内存）中，然后对这个变量的读操作，会执行refresh处理器缓存，从其他处理器的高速缓存（或者是主内存）中，读取最新的值

当然跟我们之前说的有一点点不一样，因为之前说的是写volatile变量的时候，一个是强制刷主内存，一个是过期掉其他处理器的高速缓存中的数据；读volatile变量的时候，会发现高速缓存中的值过期，然后强制从主内存加载最新值

其实这个东西吧，你没发现么，效果是一样的，他其实本质都是让一个线程写了volatie变量之后，另外一个变量立马可以读到volatile变量的值，只不过MESI协议的底层具体实现，根据cpu等硬件的不同，有多种不同的实现方式罢了

### 9.乱序执行机制

指令不一定说是拿到了一个指令立马可以执行的，比如有的指令是要进行网络通信、磁盘读写，获取锁，很多种，有的指令不是立马就绪可以执行的，为了提升效率，在现代处理器里面都是走的指令的乱序执行机制

把编译好的指令一条一条读取到处理器里，但是哪个指令先就绪可以执行，就先执行，不是按照代码顺序来的。每个指令的结果放到一个重排序处理器中，重排序处理器把各个指令的结果按照代码顺序应用到主内存或者写缓冲器里

这就导致处理器可能压根儿就是乱序在执行我们代码编译后的指令

### 10.synchronized锁同时对原子性、可见性以及有序性的保证

原子性、可见性、有序性，三块东西，都重新从比较细节和底层的层面，都在硬件的级别去给大家说了一下，到底是怎么回事，为什么会发生这个问题，从底层的层面来说了一下，以及大体上有没有什么办法可以来解决这些问题

原子性，基本的赋值写操作都是可以保证原子性的，复杂的操作是无法保证原子性的

可见性，MESI协议、flush、refresh，配合起来，才可以解决可见性

有序性，三个层次，最后一个层次有4种重排（LoadLoad、StoreStore、LoadStore、StoreLoad）

synchronized关键字，同时可以保证原子性、可见性以及有序性的

原子性的层面而言，他加了以后，有一个加锁和释放锁的机制，加锁了之后，同一段代码就只有他可以执行了

可见性，可以保证可见性的，他会通过加入一些内存屏障，他在同步代码块对变量做的写操作，都会在释放锁的时候，全部强制执行flush操作，在进入同步代码块的时候，对变量的读操作，全部会强制执行refresh的操作

更新的数据，别的线程只要进入代码块，就一定可以读到的

有序性，synchronized关键字，他会通过加各种各样的内存屏障，来保证说，解决LoadLoad、StoreStore等等重排序

### 11.Store内存屏障

按照可见性来划分的话，内存屏障可以分为Load屏障和Store屏障。

Load屏障的作用是执行refresh处理器缓存的操作，说白了就是对别的处理器更新过的变量，从其他处理器的高速缓存（或者主内存）加载数据到自己的高速缓存来，确保自己看到的是最新的数据。

Store屏障的作用是执行flush处理器缓存的操作，说白了就是把自己当前处理器更新的变量的值，都刷新到高速缓存（或者主内存）里去

在monitorexit指令之后，会有一个Store屏障，让线程把自己在同步代码块里修改的变量的值都执行flush处理器缓存的操作，刷到高速缓存（或者主内存）里去；然后在monitorenter指令之后会加一个Load屏障，执行refresh处理器缓存的操作，把别的处理器修改过的最新值加载到自己高速缓存里来

所以说通过Load屏障和Store屏障，就可以让synchronized保证可见性。

按照有序性保障来划分的话，还可以分为Acquire屏障和Release屏障。

在monitorenter指令之后，Load屏障之后，会加一个Acquire屏障，这个屏障的作用是禁止读操作和读写操作之间发生指令重排序。在monitorexit指令之前，会加一个Release屏障，这个屏障的作用是禁止写操作和读写操作之间发生重排序。

所以说，通过 Acquire屏障和Release屏障，就可以让synchronzied保证有序性，只有synchronized内部的指令可以重排序，但是绝对不会跟外部的指令发生重排序。

synchronized：

（1）原子性：加锁和释放锁，ObjectMonitor

（2）可见性：加了Load屏障和Store屏障，释放锁flush数据，加锁会refresh数据

（3）有序性：Acquire屏障和Release屏障，保证同步代码块内部的指令可以重排，但是同步代码块内部的指令和外面的指令是不能重排的

在volatile变量写操作的前面会加入一个Release屏障，然后在之后会加入一个Store屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作

在volatile变量读操作的前面会加入一个Load屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；在之后会加入一个Acquire屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序

那个Acquire屏障其实就是LoadLoad屏障 + LoadStore屏障，Release屏障其实就是StoreLoad屏障 + StoreStore屏障

### 12.深入分析缓存一致性协议

因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）

MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个处理器可以执行，其实也会通过排他锁的机制保证就一个处理器能写

之前说过那个cache entry的flag代表了缓存数据的状态，MESI协议中划分为：

（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用

（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已

（3）exclusive：独占的，标记为E，这个意思就是当前处理器对这个数据独占了，只有他可以有这个副本，其他的处理器都不能包含这个副本

（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache entry，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

MESI协议规定了一组消息，就说各个处理器在操作内存数据的时候，都会往总线发送消息，而且各个处理器还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个处理器的协作

下面来详细的图解MESI协议的工作原理，处理器0读取某个变量的数据时，首先会根据index、tag和offset从高速缓存的拉链散列表读取数据，如果发现状态为I，也就是无效的，此时就会发送read消息到总线

接着主内存会返回对应的数据给处理器0，处理器0就会把数据放到高速缓存里，同时cache entry的flag状态是S

在处理器0对一个数据进行更新的时候，如果数据状态是S，则此时就需要发送一个invalidate消息到总线，尝试让其他的处理器的高速缓存的cache entry全部变为I，以获得数据的独占锁。

其他的处理器1会从总线嗅探到invalidate消息，此时就会把自己的cache entry设置为I，也就是过期掉自己本地的缓存，然后就是返回invalidate ack消息到总线，传递回处理器0，处理器0必须收到所有处理器返回的ack消息

接着处理器0就会将cache entry先设置为E，独占这条数据，在独占期间，别的处理器就不能修改数据了，因为别的处理器此时发出invalidate消息，这个处理器0是不会返回invalidate ack消息的，除非他先修改完再说

接着处理器0就是修改这条数据，接着将数据设置为M，也有可能是把数据此时强制写回到主内存中，具体看底层硬件实现

然后其他处理器此时这条数据的状态都是I了，那如果要读的话，全部都需要重新发送read消息，从主内存（或者是其他处理器）来加载，这个具体怎么实现要看底层的硬件了，都有可能的

这套机制其实就是缓存一致性在硬件缓存模型下的完整的执行原理

写缓冲器和无效队列

MESI协议如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了

所以为了解决这个问题，硬件层面引入了写缓冲器和无效队列

写缓冲器的作用是，一个处理器写数据的时候，直接把数据写入缓冲器，同时发送invalidate消息，然后就认为写操作完成了，接着就干别的事儿了，不会阻塞在这里。接着这个处理器如果之后收到其他处理器的ack消息之后

才会把写缓冲器中的写结果拿出来，通过对cache entry设置为E加独占锁，同时修改数据，然后设置为M

其实写缓冲器的作用，就是处理器写数据的时候直接写入缓冲器，不需要同步阻塞等待其他处理器的invalidate ack返回，这就大大提升了硬件层面的执行效率了

包括查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发

引入无效队列，就是说其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

通过引入写缓冲器和无效队列，一个处理器要写数据的话，这个性能其实很高的，他直接写数据到写缓冲器，发送一个validate消息出去，就立马返回，执行别的操作了；其他处理器收到invalidate消息之后直接放入无效队列，立马就返回invalidate ack

### 13.解决有序性问题

内存屏障，Acquire屏障，Release屏障，但是都是由基础的StoreStore屏障,StoreLoad屏障，可以避免指令重排序的效果

StoreStore屏障，会强制让写数据的操作全部按照顺序写入写缓冲器里，他不会让你第一个写到写缓冲器里去，第二个写直接修改高速缓存了

resource = loadResource();

StoreStore屏障

loaded = true;

StoreLoad屏障，他会强制先将写缓冲器里的数据写入高速缓存中，接着读数据的时候强制清空无效队列，对里面的validate消息全部过期掉高速缓存中的条目，然后强制从主内存里重新加载数据

a = 1; // 强制要求必须直接写入高速缓存，不能停留在写缓冲器里，清空写缓冲器里的这条数据

int b = c;

## 原子类

### 1.AtomicInteger

在多线程程序中，诸如++i或i++等运算不具有原子性， 因此不是安全的线程操作。我们可以通过synchronized 或ReentrantLock 将该操作变成一个原子操作， 但是synchronized 和ReentrantLock均属于重量级锁。因此JVM为此类原子操作提供了一些原子操作同步类，使得同步操作（线程安全操作）更加方便、高效， 它便是AtomicInteger。 AtomicInteger为提供原子操作的Integer的类，常见的原子操作类还有AtomicBoolean 、AtomicInteger 、AtomicLong 、AtomicReference等，它们的实现原理相同，区别在于运算对象的类型不同。还可以通过AtomicReference<V>将一个对象的所有操作都转化成原子操作。AtomicInteger 的性能通常是synchronized 和 ReentrantLock的好几倍。

### 2.AtomicInteger

判断此时此刻是否是某个值，如果是，则修改，如果不是则重新查询一个最新的值，再次执行判断，这个操作叫做CAS，Compare and Set

Atomic原子类底层核心的原理就是CAS，无锁化，乐观锁，每次尝试修改的时候，就对比一下，有没有人修改过这个值，没有人修改，自己就修改，如果有人修改过，就重新查出来最新的值，再次重复那个过程

### 3.AtomicInteger源码剖析

无限重复循环以及CAS操作

拿你刚刚获取到的那个l的值，他认为当前的value的值

去跟底层当前目前AtomicInteger对象实例中的value的值去进行比较，这就是compare的过程,如果是一样的话,就会set的过程，也就是将value的值给设置为：l（之前拿到的值） + 1（递增的值）

如果l（获取到的值），跟AtomicInteger + valueOffset获取到的当前的值，不一样的话，此时compareAndSwapInt方法就会返回false，while循环里拿到的是false的话，就会自动进入下一轮的循环

如果是成功的话，会返回一个l的值，是递增1之前的一个旧的值，所以会在外层方法中加1返回，告诉你当前累加1之后最新的值

底层CPU指令是如何实现CAS语义的

最最底层，用了一个native方法，不是java写的，走的是底层的c代码，可以通过发送一些cpu的指令，来确保说CAS的那个过程，绝对是原子的，具体是怎么来实现呢？以前的cpu会通过一些指令来锁掉某一小块的内存，后来会做了一些优化，他可以保证仅仅只有一个线程在同一时间可以对某块小内存中的数据，做CAS的操作

compare -> set，这是一系列的步骤，在执行这个步骤的时候，是每个线程都是原子的，有一个线程在执行CAS一系列的比较和设置的过程中，其他的线程是不能来执行的

cpu指令来实现

cpu会通过一些轻量级的锁小块内存的机制来实现

保证整个并发的性能要好的多

### 4.Atomic原子类体系的CAS语义存在的三大缺点分析

1、ABA问题：如果某个值一开始是A，后来变成了B，然后又变成了A，你本来期望的是值如果是第一个A才会设置新值，结果第二个A一比较也ok，也设置了新值，跟期望是不符合的。所以atomic包里有AtomicStampedReference类，就是会比较两个值的引用是否一致，如果一致，才会设置新值

假设一开始变量i = 1，你先获取这个i的值是1，然后累加了1，变成了2，但是在此期间，别的线程将i -> 1 -> 2 -> 3 -> 1， 这个期间，这个值是被人改过的，只不过最后将这个值改成了跟你最早看到的值一样的值，结果你后来去compareAndSet的时候，会发现这个i还是1，就将它设置成了2，就设置成功了

说实话，用AtomicInteger，常见的是计数，所以说一般是不断累加的，所以ABA问题比较少见

2、无限循环问题：大家看源码就知道Atomic类设置值的时候会进入一个无限循环，只要不成功，就不停循环再次尝试，这个在高并发修改一个值的时候其实挺常见的，比如你用AtomicInteger在内存里搞一个原子变量，然后高并发下，多线程频繁修改，其实可能会导致这个compareAndSet()里要循环N次才设置成功，所以还是要考虑到的。

JDK 1.8引入的LongAdder来解决，是一个重点，分段CAS思路

3、多变量原子问题：一般的AtomicInteger，只能保证一个变量的原子性，但是如果多个变量呢？你可以用AtomicReference，这个是封装自定义对象的，多个变量可以放一个自定义对象里，然后他会检查这个对象的引用是不是一个。

### 5.LongAdder

Java 8提供的一个对AtomicLong改进后的一个类，LongAdder

大量线程并发更新一个原子类的时候，天然的一个问题就是自旋，会导致并发性能还是有待提升，比synchronized当然好很多了

分段迁移，某一个线程如果对一个Cell更新的时候，发现说出现了很难更新他的值，出现了多次自旋的一个问题，如果他CAS失败了，自动迁移段，他会去尝试更新别的Cell的值，这样的话就可以让一个线程不会盲目的等待一个cell的值

## volatile

### 1.volatile关键字的作用

Java除了使用了synchronized保证变量的同步，还使用了稍弱的同步机制，即volatile变量。volatile也用于确保将变量的更新操作通知到其他线程。 volatile变量具备两种特性：一种是保证该变量对所有线程可见，在一个线程修改了变量的值后，新的值对于其他线程是可以立即获取的；一种是volatile禁止指令重排，即volatile变量不会被缓存在寄存器中或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 因为在访问volatile变量时不会执行加锁操作，也就不会执行线程阻塞，因此volatile变量是一种比synchronized关键字更轻量级的同步机制。volatile主要适用于一个变量被多个线程共享，多个线程均可针对这个变量执行赋值或者读取的操作。 在有多个线程对普通变量进行读写时，每个线程都首先需要将数据从内存中复制变量到CPU缓存中，如果计算机有多个CPU，则线程可能都在不同的CPU中被处理，这意味着每个线程都需要将同一个数据复制到不同的CPU Cache中，这样在每个线程都针对同一个变量的数据做了不同的处理后就可能存在数据不一致的情况。

如果将变量声明为volatile，JVM就能保证每次读取变量时都直接从内存中读取，跳过CPU Cache这一步，有效解决了多线程数据同步的问题。

需要说明的是，volatile关键字可以严格保障变量的单次读、写操作的原子性，但并不能保证像i++这种操作的原子性，因为i++在本质上是读、写两次操作。volatile 在某些场景下可以代替synchronized，但是volatile不能完全取代synchronized的位置，只有在一些特殊场景下才适合使用volatile。比如，必须同时满足下面两个条件才能保证并发环境的线程安全。

- 对变量的写操作不依赖于当前值（比如i++），或者说是单纯的变量赋值（boolean flag=true）。
- 该变量没有被包含在具有其他变量的不变式中，也就是说在不同的volatile变量之间不能互相依赖，只有在状态真正独立于程序内的其他内容时才能使用volatile。

volatile关键字的使用方法比较简单，直接在定义变量时加上volatile关键字即可：

```html
volatile Boolean flag = false;
```

### 2.volatile

只要开了多个线程，一定会有一些这种问题，某个线程修改一个变量值，其他线程要立刻感知到这个变量值的变化，但是如果你不用volatile，会有问题

有线程修改了一个变量的值，结果其他的线程感知不到这个变量值的修改

volatile关键字，尽量让你修改了一个变量之后，其他的线程可以立即看到这个变量的最新的值

cpu缓存模型 -> java内存模型 -> 原子性、可见性、有序性 -> volatile的作用 -> volatile的底层原理 -> volatile实战

cpu缓存模型

cpu在他和主内存之间加了一层缓存，主内存的数据会被加载到cpu本地缓存里去，cpu后面会读写自己的缓存

cpu缓存模型，其实默认情况下是有问题的，特别是多线程并发运行的时候，导致说各个cpu的本地缓存，跟主内存，没有同步，一个数据，在各个地方，可能都不一样，就会导致数据的不一致

总线加锁机制

某个cpu如果要读一个数据，会通过一个总线，对这个数据加一个锁，其他的cpu就没法去读和写这个数据了，只有当这个cpu修改完了以后，其他cpu可以读到最新的数据

这个总线加锁机制，效率太差了，一旦说多个线程出现了对某个共享变量的访问之后，就会导致说，可能串行化的问题，多个cpu多线程并发运行的时候，效率很差

MESI协议

MESI协议，缓存一致性协议

修改本地缓存，立马刷主存，其他cpu本地缓存立马工期，重新从主存加载

:lock前缀指令 -> 内存屏障

内存模型

Java内存模型是跟cpu缓存模型是类似的，基于cpu缓存模型来建立的java内存模型，只不过java内存模型是标准化的，屏蔽掉底层不同的计算机的区别

线程的工作内存和主内存

read（从主存读取），load（将主存读取到的值写入工作内存），use（从工作内存读取数据来计算），assign（将计算好的值重新赋值到工作内存中），store（将工作内存数据写入主存），write（将store过去的变量值赋值给主存中的变量）

并发编程的三大特性

**（1）可见性**

**（2）原子性**

对于一个i++的操作，只要是多个线程并发运行来执行这行代码，其实的话，他都是不保证原子性的，如果保证原子性的，第一个线程i++，i = 1；第二个线程，i++，i = 2

volatile，就是保证线程之间可见性的一个关键字

**（3）有序性**

编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序

volatile保证多线程的可见性

只要flag变成了1，然后线程不是要将flag = 1写回工作内存吗？assign操作，此时如果这个flag变量是加了volatile关键字的话，那么此时会这样子，就是说一定会强制保证说assign之后，就立马执行store + write，刷回到主内存里去

保证只要工作内存一旦变为flag = 1，主内存立马变成flag = 1

此外，如果这个变量是加了volatile关键字的话，此时他就会让其他线程的工作内存中的这个flag变量的缓存，会过期

线程2如果再从工作内存里读取flag变量的值，发现他已经过期了，此时就会重新从主内存里来加载这个flag = 1的值

通过volatile关键字，可以实现的一个效果就是说，有一个线程修改了值，其他线程可以立马感知到这个值

volatile无法保证原子性

volatile保证有序性

java中有一个happens-before原则：

编译器、指令器可能对代码重排序，乱排，要守一定的规则，happens-before原则，只要符合happens-before的原则，那么就不能胡乱重排，如果不符合这些规则的话，那就可以自己排序

程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作

锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作

volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 ，volatile变量写，再是读，必须保证是先写，再读

传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C

线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作

线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生

线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行

对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始

上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。

但是如果没满足上面的规则，那么就可能会出现指令重排，就这个意思。这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。

volatile保证有序性，因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。

### 3.volatile的底层实现原理：lock指令以及内存屏障

（1）lock指令

对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改

如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了

lock前缀指令 + MESI缓存一致性协议

（2）内存屏障：禁止重排序

```html
Load1：
int localVar = this.variable
Load2：
int localVar = this.variable2 
```

LoadLoad屏障：Load1；LoadLoad；Load2，确保Load1数据的装载先于Load2后所有装载指令，他的意思，Load1对应的代码和Load2对应的代码，是不能指令重排的

```html
Store1：
this.variable = 1
StoreStore屏障
Store2：
this.variable2 = 2
```

StoreStore屏障：Store1；StoreStore；Store2，确保Store1的数据一定刷回主存，对其他cpu可见，先于Store2以及后续指令

volatile的作用是什么呢？

```html
volatile variable = 1
this.variable = 2 => store操作
int localVariable = this.variable => load操作
```

对于volatile修改变量的读写操作，都会加入内存屏障

每个volatile写操作前面，加StoreStore屏障，禁止上面的普通写和他重排；每个volatile写操作后面，加StoreLoad屏障，禁止跟下面的volatile读/写重排

每个volatile读操作后面，加LoadLoad屏障，禁止下面的普通读和voaltile读重排；每个volatile读操作后面，加LoadStore屏障，禁止下面的普通写和volatile读重排