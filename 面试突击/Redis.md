# Redis

## 1.Redis

Redis是一个开源（BSD许可）的内存中的数据结构存储系统，可以用作数据库、缓存和消息中间件，支持多种类型的数据结构，例如String（字符串）、Hash（散列）、List（列表）、Set（集合）、ZSet（有序集合）、Bitmap（位图）、HyperLogLog（超级日志）和Geospatial（地理空间）。Redis内置了复制、Lua脚本、LRU驱动事件、事务和不同级别的磁盘持久化，并通过Redis哨兵（Sentinel）模式和集群模式（Cluster）提供高可用性（High Availability）。

Redis不但支持丰富的数据类型，还支持分布式事务、数据分片、数据持久化等功能，是分布式系统中不可或缺的内存数据库服务。

## 2.Redis的数据类型

Redis支持String、Hash、List、Set、ZSet、Bitmap、HyperLogLog和Geospatial这 8种数据类型。

（1）String：String是Redis基本的数据类型，一个key对应一个value。String类型的值最大能存储512MB数据。Redis的String数据类型支持丰富的操作命令。常用的String操作命令。

这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。

- Setnx：只有在key不存在时才设置key的值
- Getrange：返回key中字符串值的子字符
- Mset：同时设置一个或多个key-value对
- Setex：将值value关联到key，并将key的过期时间设为seconds（以秒为单位）
- SET：设置指定key的值
- Get：获取指定key的值
- Getbit：获取key所对应的字符串值指定偏移量上的位（bit）
- Setbit：设置或清除key所对应的字符串值指定偏移量上的位（bit）
- Decr：将key中存储的数字值减1
- Decrby：将key所对应的值减去给定的减量值
- Strlen：返回key所储存的字符串值的长度
- Msetnx：同时设置一个或多个key-value对，在且仅在所有给定的key都不存在时
- Incrby：将key所存储的值加上给定的增量值
- Incrbyfloat：将key所存储的值加上给定的浮点增量值
- Setrange：用value参数覆写给定key所存储的字符串值，从偏移量offset开始
- Psetex：和SETEX相似，但它以毫秒为单位设置key的生存时间，而不是像SETEX那样，以秒为单位
- Append：如果key已经存在并且是一个字符串，则append将value追加到该字符串的末尾
- Getset：将给定key的值设为value，并返回key的旧值（old value）
- Mget：获取（一个或多个）给定的key的值
- Incr：将在key中存储的数字值加1

（2）Hash：Redis Hash是一个键值（key->value）对集合。Redis的Hash列表支持的操作。

这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是**这个对象没嵌套其他的对象**）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的**某个字段**。

- Hmset：同时将多个field-value（域-值）对设置到散列表key中
- Hmget：获取所有给定字段的值
- Hset：将散列表key中field字段的值设为value
- Hgetall：获取散列表中指定key的所有字段和值
- Hget：获取存储在散列表中指定字段的值
- Hexists：查看散列表key中指定的字段是否存在
- Hincrby：为散列表key中指定字段的整数值加上增量increment
- Hlen：获取散列表中字段的数量
- Hdel：删除一个或多个散列表字段
- Hvals：获取散列表中的所有值
- Hincrbyfloat：为散列表key中指定字段的浮点数值加上增量increment
- Hkeys：获取所有散列表中的字段
- Hsentnx：只有在字段field不存在时，才设置散列表字段的值

（3）List：Redis List是简单的字符串列表，按照插入顺序排序。我们可以添加一个元素到列表的头部（左边）或者尾部（右边）。列表最多可存储231-1（4 294 967295≈4亿多）个元素。

比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。

比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。

比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。

- Lindex：通过索引获取列表中的元素
- Rpush：在列表中添加一个或多个值
- Lrange：获取列表指定范围内的元素
- Rpoplpush：移除列表的最后一个元素，将该元素添加到另一个列表中并返回
- Blpop：移除并获取列表的第一个元素，如果列表没有元素，则会阻塞列表直到等待超时或发现可移除的元素
- Brpop：移除并获取列表的最后一个元素，如果列表没有元素，则会阻塞列表直到等待超时或发现可移除的元素
- Brpoplpush：从列表中弹出一个值，将弹出的元素插入另一个列表中并返回它，如果列表没有元素，则会阻塞列表直到等待超时或发现可移除的元素
- Lrem：移除列表元素
- Llen：获取列表长度
- Ltrim：对一个列表进行修剪（trim），让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除
- Lpop：移出并获取列表的第一个元素
- Lpushx：将一个或多个值插入已存在的列表头部
- Linsort：在列表的元素前或后插入元素
- Rpop：移除并获取列表的最后一个元素
- Lset：通过索引设置列表元素的值
- Lpush：将一个或多个值插入列表头部
- Rpushx：为已存在的列表添加值

（4）Set：Set是String类型的无序集合。集合是通过散列表实现的，所以添加、删除、查找的复杂度都是O(1)。Set支持的操作。

直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。

可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。

把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。

- Sunion：返回所有给定集合的并集
- Scard：获取集合的成员数
- Srandmember：返回集合中的一个或多个随机数
- Smembers：返回集合中的所有成员
- Sinter：返回给定所有集合的交集
- Srem：移除集合中的一个或多个成员
- Smove：将member元素从source集合移动到destination集合
- Sadd：向集合中添加一个或多个成员
- Sismember：判断member元素是否是集合key的成员
- Sdiffstore：返回给定集合的差集并将其存储在destination中
- Sdiff：返回给定集合的差集
- Sscan：迭代集合中的元素
- Sinterstore：返回给定集合的交集并将其存储在destination中
- Sunionstore：返回给定集合的并集并将其存储在destination中
- Spop：移除并返回集合中的一个随机元素

（5）ZSet：Redis ZSet和Set一样也是String类型元素的集合，且不允许有重复的成员，不同的是，每个元素都会关联一个double类型的分数。Redis正是通过分数来为集合中的成员进行从小到大的排序的。RedisZSet支持的操作。

- Zrevrank：返回有序集合中指定成员的排名，有序集合中的成员按分数值递减（从大到小）排序
- Zlexcount：在有序集合中计算指定字典区间内的成员数量
- Zunionstore：计算给定的一个或多个有序集的并集，并将其存储在新的key中
- Zremrangebyrank：移除有序集合中给定的排名区间的所有成员
- Zoard：获取有序集合的成员数
- Zrem：移除有序集合中的一个或多个成员
- Zinterstore：计算给定的一个或多个有序集合的交集并将结果集存储在新的有序集合key中
- Zrank：返回有序集合中指定成员的索引
- Zincrby：在有序集合中对指定成员的分数加上增量increment
- Zrangebyscore：通过分数返回有序集合指定区间内的成员
- Zrangebylex：通过字典区间返回有序集合的成员
- Zscore：返回有序集合中成员的分数值
- Zremrangebyscore：移除有序集合中给定分数区间内的所有成员
- Zscan：迭代有序集合中的元素（包括元素成员和元素分值）
- Zrevrangebyscore：返回有序集合中指定分数区间内的成员，分数从高到低排序
- Zremrangebylex：移除有序集合中给定字典区间内的所有成员
- Zrevrange：返回有序集合中指定区间内的成员，通过索引按分数从高到底排序
- Zrange：通过索引区间返回有序集合成指定区间内的成员
- Zcount：计算有序集合中指定分数区间内的成员数量
- Zadd：向有序集合添加一个或多个成员，或者更新已存在成员的分数

（6）Bitmap：通过操作二进制位记录数据。Redis Bitmap支持的操作。

- setbit：设置Bitmap值
- getbit：获取Bitmap值
- bitcount：获取指定范围内值为1的个数
- destkey：对Bitmap进行操作，可以是and（交集），or（并集），not（非集）或xor（异或）

（7）HyperLogLog：被用于估计一个Set中元素数量的概率性的数据结构。Redis HyperLogLog支持的操作。

- PFADD：添加指定的元素到HyperLogLog中
- PFCOUNT：返回给定HyperLogLog的基数估算值
- PFMERGE：将多个HyperLogLog合并为一个HyperLogLog

（8）Geospatial：用于地理空间关系计算，支持的操作。

- GEOHASH：返回一个或多个位置元素的Geohash表示
- GEOPOS：从key里返回所有给定位置的元素的位置（经度和纬度）
- GEODIST：返回两个给定位置之间的距离
- GEORADIUS：以给定的经纬度为中心，找出某一半径内的元素
- GEOADD：将指定的地理空间位置（维度，经度，名称）添加到指定的key中
- GEORADIUSBYMEMBER：找出位于指定范围内的元素，中心点由给定的位置元素决定

## 3.Redis管道

Redis是基于请求/响应协议的TCP服务。在客户端向服务器发送一个查询请求后，需要监听Socket的返回，该监听过程一直阻塞，直到服务器有结果返回。由于Redis集群是部署在多个服务器上的，所以Redis的请求/响应模型在每次请求时都要跨网络在不同的服务器之间传输数据，这样每次查询都存在一定的网络延迟（服务器之间的网络延迟一般在 20ms左右）。由于服务器一般采用多线程处理业务，并且内存操作效率很高，所以如果一次请求延迟 20ms，则多次请求的网络延迟会不断累加。也就是说，在分布式环境下，Redis的性能瓶颈主要体现在网络延迟上。

Redis的管道技术指在服务端未响应时，客户端可以继续向服务端发送请求，并最终一次性读取所有服务端的响应。管道技术能减少客户端和服务器交互的次数，将客户端的请求批量发送给服务器，服务器针对批量数据分别查询并统一回复，能显著提高Redis的性能。

## 4.Redis的事务

Redis支持分布式环境下的事务操作，其事务可以一次执行多个命令，事务中的所有命令都会序列化地顺序执行。事务在执行过程中，不会被其他客户端发送来的命令请求打断。服务器在执行完事务中的所有命令之后，才会继续处理其他客户端的其他命令。Redis的事务操作分为开启事务、命令入队列、执行事务三个阶段。Redis的事务执行流程如下：

（1）事务开启：客户端执行Multi命令开启事务。

（2）提交请求：客户端提交命令到事务。

（3）任务入队列：Redis将客户端请求放入事务队列中等待执行。

（4）入队状态反馈：服务器返回QURUD，表示命令已被放入事务队列。

（5）执行命令：客户端通过Exec执行事务。

（6）事务执行错误：在Redis事务中如果某条命令执行错误，则其他命令会继续执行，不会回滚。可以通过Watch监控事务执行的状态并处理命令执行错误的异常情况。

（7）执行结果反馈：服务器向客户端返回事务执行的结果。

Redis事务的相关命令有Multi、Exec、Discard、Watch和Unwatch。

- Multi：标记一个事务块的开始
- Exec：执行所有事务块内二点命令
- Discard：取消事务，放弃执行事务块内的所有命令
- Watch：监视一个（或多个）key，如果在事务执行之前这个（或这些）key被其他命令改动，那么事务将被打断。
- Unwatch：取消Watch命令对所有key的监视

```html
public void transactionSet(Map<String,Object> commandList){
    //开启事务权限
    redisTemplate.setEnableTransactionSupport(true);
    try{
        //开启事务
        redisTemplate.multi();
        //执行事务命令
        for(Map.Entry<String,Object>entry:commandList.entrySet()){
            String mapKey=entry.getKey();
            Object mapValue=entry.getValue();
            redisTemplate.opsForValue().set(mapKey,mapValue)
        }
        //成功就提交
        redisTemplate.exec();
    }catch(Exception e){
        //失败就回滚
        redisTemplate.discard();
    }
}
```

## 5.Redis发布、订阅

Redis发布、订阅是一种消息通信模式：发送者（Pub）向频道（Channel）发送消息，订阅者（Sub）接收频道上的消息。Redis客户端可以订阅任意数量的频道，发送者也可以向任意频道发送数据。图8-7展示了1个发送者（pub1）、1个频道（channe0）和3个订阅者（sub1、sub2、sub3）的关系。由于3个订阅者sub1、sub2、sub3都订阅了频道channel0，在发送者pub1向频道channel0发送一条消息后，这条消息就会被发送给订阅它的三个客户端。

Redis常用的消息订阅与发布命令:

- PSUBSCRIBE：订阅一个或多个符合给定模式的频道
- PUBSUB：查看订阅与发布系统的状态
- PUBLISH：将信息发送到指定的频道
- SUBSCRIBE：订阅给定的一个或多个频道的信息
- UNSUBSCRIBE：指退订给定的频道

## 6.Redis集群数据复制的原理

Redis提供了复制功能，可以实现在主数据库（Master）中的数据更新后，自动将更新的数据同步到从数据库（Slave）。一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。

Redis的主从数据复制原理如下。

（1）一个从数据库在启动后，会向主数据库发送SYNC命令。

（2）主数据库在接收到SYNC命令后会开始在后台保存快照（即RDB持久化的过程），并将保存快照期间接收到的命令缓存起来。在该持久化过程中会生成一个.rdb快照文件。

（3）在主数据库快照执行完成后，Redis会将快照文件和所有缓存的命令以.rdb快照文件的形式发送给从数据库。

（4）从数据库收到主数据库的.rdb快照文件后，载入该快照文件到本地。

（5）从数据库执行载入后的.rdb快照文件，将数据写入内存中。以上过程被称为复制初始化。

（6）在复制初始化结束后，主数据库在每次收到写命令时都会将命令同步给从数据库，从而保证主从数据库的数据一致。

在Redis中开启复制功能时需要在从数据库配置文件中加入如下配置，对主数据库无须进行任何配置：

```html
#slave of master_address master_port
slave of 127.0.0.1 9000
#如果master有密码，则需要设置masterauth
masterauth=123
```

在上述配置中，slaveof后面的配置分别为主数据库的IP地址和端口，在主数据库开启了密码认证后需要将masterauth设置为主数据库的密码，在配置完成后重启Redis，主数据库上的数据就会同步到从数据库上。

## 8.Redis的集群模式及工作原理

Redis有三种集群模式：主从模式、哨兵模式和集群模式。

（1）主从模式：所有的写请求都被发送到主数据库上，再由主数据库将数据同步到从数据库上。主数据库主要用于执行写操作和数据同步，从数据库主要用于执行读操作缓解系统的读压力。

Redis的一个主库可以拥有多个从库，从库还可以作为其他数据库的主库。如图 8-7所示，Master的从库有Slave-0和Slave-1，同时Slave-1作为Slave-1-0和Slave-1-1的主库。

（2）哨兵模式：在主从模式上添加了一个哨兵的角色来监控集群的运行状态。哨兵通过发送命令让Redis服务器返回其运行状态。哨兵是一个独立运行的进程，在监测到Master宕机时会自动将Slave切换成Master，然后通过发布与订阅模式通知其他从服务器修改配置文件，完成主备热切。

（3）集群模式：Redis集群实现了在多个Redis节点之间进行数据分片和数据复制。基于Redis集群的数据自动分片能力，我们能够方便地对Redis集群进行横向扩展，以提高Redis集群的吞吐量。基于Redis集群的数据复制能力，在集群中的一部分节点失效或者无法进行通信时，Redis仍然可以基于副本数据对外提供服务，这提高了集群的可用性。

Redis集群遵循如下原则。

- 所有Redis节点彼此都通过PING-PONG 机制互联，内部使用二进制协议优化传输速度和带宽。
- 在集群中超过半数的节点检测到某个节点Fail后将该节点设置为Fail状态。
- 客户端与Redis节点直连，客户端连接集群中任何一个可用节点即可对集群进行操作。
- Redis-Cluster把所有的物理节点都映射到0～16383的slot（槽）上，Cluster负责维护每个节点上数据槽的分配。Redis的具体数据分配策略为：在Redis集群中内置了16384个散列槽；在需要在Redis集群中放置一个Key-Value时，Redis会先对Key使用CRC16 算法算出一个结果，然后把结果对16384 求余数，这样每个Key都会对应一个编号为0～16383的散列槽；Redis会根据节点的数量大致均等地将散列槽映射到不同的节点。

## 9.缓存预热

缓存预热指在用户请求数据前先将数据加载到缓存系统中，用户查询事先被预热的缓存数据，以提高系统查询效率。缓存预热一般有系统启动加载、定时加载等方式。

## 10.缓存更新

缓存更新指在数据发生变化后及时将变化后的数据更新到缓存中。常见的缓存更新策略有以下4种。

- 定时更新：定时将底层数据库内的数据更新到缓存中，该方法比较简单，适合需要缓存的数据量不是很大的应用场景。
- 过期更新：定时将缓存中过期的数据更新为最新数据并更新缓存的过期时间。
- 写请求更新：在用户有写请求时先写数据库同时更新缓存，这适用于用户对缓存数据和数据库的数据有实时强一致性要求的情况。
- 读请求更新：在用户有读请求时，先判断该请求数据的缓存是否存在或过期，如果不存在或已过期，则进行底层数据库查询并将查询结果更新到缓存中，同时将查询结果返回给用户。

## 11.缓存淘汰策略

在缓存数据过多时需要使用某种淘汰算法决定淘汰哪些数据。常用的淘汰算法有以下几种。

- FIFO（First In First Out，先进先出）：判断被存储的时间，离目前最远的数据优先被淘汰。
- LRU（Least Recently Used，最近最少使用）：判断缓存最近被使用的时间，距离当前时间最远的数据优先被淘汰。
- LFU（Least Frequently Used，最不经常使用）：在一段时间内，被使用次数最少的缓存优先被淘汰。

## 12.缓存雪崩

缓存雪崩指在同一时刻由于大量缓存失效，导致大量原本应该访问缓存的请求都去查询数据库，而对数据库的CPU和内存造成巨大压力，严重的话会导致数据库宕机，从而形成一系列连锁反应，使整个系统崩溃。一般有以下3种处理方法。

- 请求加锁：对于并发量不是很多的应用，使用请求加锁排队的方案防止过多请求数据库。
- 失效更新：为每一个缓存数据都增加过期标记来记录缓存数据是否失效，如果缓存标记失效，则更新数据缓存。
- 设置不同的失效时间：为不同的数据设置不同的缓存失效时间，防止在同一时刻有大量的数据失效。

对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。

这就是缓存雪崩。

缓存雪崩的事前事中事后的解决方案如下：

- 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。
- 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死。
- 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 Redis。如果 ehcache 和 Redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 Redis 中。

限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？**走降级**！可以返回一些默认的值，或者友情提示，或者空值。

好处：

- 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。
- 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。
- 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来了。

## 13.缓存穿透

缓存穿透指由于缓存系统故障或者用户频繁查询系统中不存在（在系统中不存在，在自然数据库和缓存中都不存在）的数据，而这时请求穿过缓存不断被发送到数据库，导致数据库过载，进而引发一连串并发问题。 比如用户发起一个userName为zhangsan的请求，而在系统中并没有名为zhangsan的用户，这样就导致每次查询时在缓存中都找不到该数据，然后去数据库中再查询一遍。由于zhangsan用户本身在系统中不存在，自然返回空，导致请求穿过缓存频繁查询数据库，在用户频繁发送该请求时将导致数据库系统负载增大，从而可能引发其他问题。常用的解决缓存穿透问题的方法有布隆过滤器和cache null策略。

- 布隆过滤器：指将所有可能存在的数据都映射到一个足够大的Bitmap中，在用户发起请求时首先经过布隆过滤器的拦截，一个一定不存在的数据会被这个布隆过滤器拦截，从而避免对底层存储系统带来查询上的压力。
- cache null策略：指如果一个查询返回的结果为null（可能是数据不存在，也可能是系统故障），我们仍然缓存这个null结果，但它的过期时间会很短，通常不超过 5 分钟；在用户再次请求该数据时直接返回null，而不会继续访问数据库，从而有效保障数据库的安全。其实cache null策略的核心原理是：在缓存中记录一个短暂的（数据过期时间内）数据在系统中是否存在的状态，如果不存在，则直接返回null，不再查询数据库，从而避免缓存穿透到数据库上。

对于系统 A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。

黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。

举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“**视缓存于无物**”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。

解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 `set -999 UNKNOWN` 。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。

## 14.缓存降级

缓存降级指由于访问量剧增导致服务出现问题（如响应时间慢或不响应）时，优先保障核心业务的运行，减少或关闭非核心业务对资源的使用。常见的服务降级策略如下。

- 写降级：在写请求增大时，可以只进行Cache的更新，然后将数据异步更新到数据库中，保证最终一致性即可，即将写请求从数据库降级为Cache。
- 读降级：在数据库服务负载过高或数据库系统故障时，可以只对Cache进行读取并将结果返回给用户，在数据库服务正常后再去查询数据库，即将读请求从数据库降级为Cache。这种方式适用于对数据实时性要求不高的场景，保障了在系统发生故障的情况下用户依然能够访问到数据，只是访问到的数据相对有延迟。

## 15.Memcache 与 Redis 的区别都有哪些？

存储方式不同：memcache 把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小；Redis有部份存在硬盘上，这样能保证数据的持久性。 数据支持类型：memcache 对数据类型支持相对简单；Redis 有复杂的数据类型。

使用底层模型不同：它们之间底层实现方式，以及与客户端之间通信的应用协议不一样，Redis 自己构建了 vm 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 value 值大小不同：Redis 最大可以达到 1gb；memcache 只有 1mb。

## 16.redis 的线程模型

redis 内部使用文件事件处理器 `file event handler`，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。

文件事件处理器的结构包含 4 个部分：

- 多个 socket
- IO 多路复用程序
- 文件事件分派器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。

首先，redis 服务端进程初始化的时候，会将 server socket 的 `AE_READABLE` 事件与连接应答处理器关联。

客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 `AE_READABLE` 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给**连接应答处理器**。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 `AE_READABLE` 事件与命令请求处理器关联。

假设此时客户端发送了一个 `set key value` 请求，此时 redis 中的 socket01 会产生 `AE_READABLE` 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 `AE_READABLE` 事件，由于前面 socket01 的 `AE_READABLE` 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 `key value` 并在自己内存中完成 `key value` 的设置。操作完成后，它会将 socket01 的 `AE_WRITABLE` 事件与命令回复处理器关联。

如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 `AE_WRITABLE` 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 `ok`，之后解除 socket01 的 `AE_WRITABLE` 事件与命令回复处理器的关联。

这样便完成了一次通信。

## 17.为啥 redis 单线程模型也能效率这么高？

- 纯内存操作。
- 核心是基于非阻塞的 IO 多路复用机制。
- C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。
- 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。

redis6.0开始引入多线程

注意！redis6.0之后的版本抛弃了单线程模型这一设计，原本使用单线程运行的redis也开始选择性的使用多线程模型。

前面还在强调redis单线程模型的高效性，现在为什么又要引入多线程？这其实说明redis在有些方面，单线程已经不具备优势了，因为读写网络的read/write系统调用在redis执行期间占用了大部分CPU时间，如果把网络读写做成多线程的方式对性能会有很大提升。

redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。之所以这么设计是不想redis因为多线程而变得复杂，需要去控制key，lua，事务，LPUSH/LPOP等等的并发问题。

总结

redis选择使用单线程模型处理客户端的请求主要还是因为CPU不是redis服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络I/O操作上；而redis引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间也能减少对redis主线程阻塞的时间，提高执行的效率。

## 18.redis 的过期策略都有哪些？

Redis 过期策略是：**定期删除+惰性删除**。

所谓**定期删除**，指的是 Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。

假设 Redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 Redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的**灾难**。实际上 Redis 是每隔 100ms **随机抽取**一些 key 来检查和删除的。

但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，Redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。

> 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。

但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 Redis 内存块耗尽了，咋整？

答案是：**走内存淘汰机制**。

## 19.内存淘汰机制

Redis 内存淘汰机制有以下几个：

- no-eviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。
- **allkeys-lru**：当内存不足以容纳新写入数据时，在**键空间**中，移除最近最少使用的 key（这个是**最常用**的）。
- allkeys-random：当内存不足以容纳新写入数据时，在**键空间**中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。
- volatile-lru：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，移除最近最少使用的 key（这个一般不太合适）。
- volatile-random：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，**随机移除**某个 key。
- volatile-ttl：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，有**更早过期时间**的 key 优先移除。

## 20.如何保证 redis 的高并发和高可用？

如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。

redis 实现**高并发**主要依靠**主从架构**，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。

如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。

redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。

## 21.Redis 主从架构

单机的 Redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑**读高并发**的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的**读请求全部走从节点**。这样也可以很轻松实现水平扩容，**支撑读高并发**。

Redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

### Redis replication 的核心机制

- Redis 采用**异步方式**复制数据到 slave 节点，不过 Redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；
- 一个 master node 是可以配置多个 slave node 的；
- slave node 也可以连接其他的 slave node；
- slave node 做复制的时候，不会 block master node 的正常工作；
- slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；
- slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。

注意，如果采用了主从架构，那么建议必须**开启** master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。

另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能**确保启动的时候，是有数据的**，即使采用了高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。

### Redis 主从复制的核心原理

当启动一个 slave node 的时候，它会发送一个 `PSYNC` 命令给 master node。

如果这是 slave node 初次连接到 master node，那么会触发一次 `full resynchronization` 全量复制。此时 master 会启动一个后台线程，开始生成一份 `RDB` 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。 `RDB` 文件生成完毕后， master 会将这个 `RDB` 发送给 slave，slave 会先**写入本地磁盘，然后再从本地磁盘加载到内存**中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。

主从复制的断点续传

从 Redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。

master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 `resynchronization` 。

> 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。

无磁盘化复制

master 在内存中直接创建 `RDB` ，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 `repl-diskless-sync yes` 即可。

```html
repl-diskless-sync yes

# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来
repl-diskless-sync-delay 5
```

过期 key 处理

slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。

## 22.复制的完整流程

slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的 `host` 和 `ip` ，但是复制流程没开始。

slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 `ping` 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node **第一次执行全量复制**，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。

全量复制

- master 执行 bgsave ，在本地生成一份 rdb 快照文件。
- master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60 秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s)
- master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。
- 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。

```html
client-output-buffer-limit slave 256MB 64MB 60
```

- slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时**基于旧的数据版本**对外提供服务。
- 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。

增量复制

- 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。
- master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。
- master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。

heartbeat

主从节点互相都会发送 heartbeat 信息。

master 默认每隔 10 秒发送一次 heartbeat，slave node 每隔 1 秒发送一个 heartbeat。

异步复制

master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。

## 23.Redis 如何才能做到高可用

如果系统在 365 天内，有 99.99% 的时间，都是可以哗哗对外提供服务的，那么就说系统是高可用的。

一个 slave 挂掉了，是不会影响可用性的，还有其它的 slave 在提供相同数据下的相同的对外的查询服务。

但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。

Redis 的高可用架构，叫做 `failover` **故障转移**，也可以叫做主备切换。

master node 在故障时，自动检测，并且将某个 slave node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 Redis 的主从架构下的高可用。





## 30.Redis 集群模式的工作原理能说一下么？

现在的 Redis 集群模式，可以做到在多台机器上，部署多个 Redis 实例，每个实例存储一部分的数据，同时每个 Redis 主实例可以挂 Redis 从实例，自动确保说，如果 Redis 主实例挂了，会自动切换到 Redis 从实例上来。

现在 Redis 的新版本，大家都是用 Redis cluster 的，也就是 Redis 原生支持的 Redis 集群模式。

如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 Redis 主从架构的高可用性。

Redis cluster，主要是针对**海量数据+高并发+高可用**的场景。Redis cluster 支撑 N 个 Redis master node，每个 master node 都可以挂载多个 slave node。这样整个 Redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。

## 31.Redis cluster

Redis cluster 介绍

- 自动将数据进行分片，每个 master 上放一部分数据
- 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的

在 Redis cluster 架构下，每个 Redis 要放开两个端口号，比如一个是 6379，另外一个就是 加 1w 的端口号，比如 16379。

16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议， `gossip` 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。

节点间的内部通信机制

基本通信原理

集群元数据的维护有两种方式：集中式、Gossip 协议。Redis cluster 节点间采用 gossip 协议进行通信。

**集中式**是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 `storm` 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。

Redis 维护集群元数据采用另一个方式， `gossip` 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。

**集中式**的**好处**在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；**不好**在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。

gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。

- 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 `ping` 消息，同时其它几个节点接收到 `ping` 之后返回 `pong` 。
- 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。

gossip 协议

gossip 协议包含多种消息，包含 `ping` , `pong` , `meet` , `fail` 等等。

- meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。

```html
Redis-trib.rb add-node
```

其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。

- ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。
- pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。
- fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。

ping 消息深入

ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。

每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 `cluster_node_timeout / 2` ，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 `cluster_node_timeout` 可以调节，如果调得比较大，那么会降低 ping 的频率。

每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 `3` 个其它节点的信息，最多包含 `总节点数减 2` 个其它节点的信息。

## 32.分布式寻址都有哪些算法？

分布式寻址算法

- hash 算法（大量缓存重建）
- 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）
- Redis cluster 的 hash slot 算法

hash 算法

来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致**大部分的请求过来，全部无法拿到有效的缓存**，导致大量的流量涌入数据库。

一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。

来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环**顺时针“行走”**，遇到的第一个 master 节点就是 key 所在位置。

在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。

燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成**缓存热点**的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。

Redis cluster 的 hash slot 算法

Redis cluster 有固定的 `16384` 个 hash slot，对每个 `key` 计算 `CRC16` 值，然后对 `16384` 取模，可以获取 key 对应的 hash slot。

Redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 `hash tag` 来实现。

任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。

## 33.Redis cluster 的高可用与主备切换原理

Redis cluster 的高可用的原理，几乎跟哨兵是类似的。

判断节点宕机

如果一个节点认为另外一个节点宕机，那么就是 `pfail` ，**主观宕机**。如果多个节点都认为另外一个节点宕机了，那么就是 `fail` ，**客观宕机**，跟哨兵的原理几乎一样，sdown，odown。

在 `cluster-node-timeout` 内，某个节点一直没有返回 `pong` ，那么就被认为 `pfail` 。

如果一个节点认为某个节点 `pfail` 了，那么会在 `gossip ping` 消息中， `ping` 给其他节点，如果**超过半数**的节点都认为 `pfail` 了，那么就会变成 `fail` 。

从节点过滤

对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。

检查每个 slave node 与 master node 断开连接的时间，如果超过了 `cluster-node-timeout * cluster-slave-validity-factor` ，那么就**没有资格**切换成 `master` 。

从节点选举

每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node `（N/2 + 1）` 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。

从节点执行主备切换，从节点切换为主节点。

与哨兵比较

整个流程跟哨兵相比，非常类似，所以说，Redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。

## 34.缓存击穿

缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。

不同场景下的解决方式可如下：

- 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。
- 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。
- 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前主动地重新构建缓存或者延后缓存的过期时间，以保证所有的请求能一直访问到对应的缓存。

## 35.如何保证缓存与数据库的双写一致性？

面试题剖析

一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统**不是严格要求** “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：**读请求和写请求串行化**，串到一个**内存队列**里去。

串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。

Cache Aside Pattern

最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- 更新的时候，**先更新数据库，然后再删除缓存**。

**为什么是删除缓存，而不是更新缓存？**

原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于**比较复杂的缓存数据计算的场景**，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，**这个缓存到底会不会被频繁访问到？**

举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有**大量的冷数据**。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。**用到缓存才去算缓存。**

其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。

最初级的缓存不一致问题及解决方案

问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。

解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。

比较复杂的数据不一致问题分析

数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，**查到了修改前的旧数据**，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了...

**为什么上亿流量高并发场景下，缓存会出现这个问题？**

只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就**可能会出现上述的数据库+缓存不一致的情况**。

**解决方案如下：**

更新数据的时候，根据**数据的唯一标识**，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。

一个队列对应一个工作线程，每个工作线程**串行**拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

这里有一个**优化点**，一个队列中，其实**多个更新缓存请求串在一起是没意义的**，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。

待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。

如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。

高并发的场景下，该解决方案要注意的问题：

- 读请求长时阻塞

由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。

该解决方案，最大的风险点在于说，**可能数据更新很频繁**，导致队列中积压了大量更新操作在里面，然后**读请求会发生大量的超时**，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。

另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要**部署多个服务**，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致**读请求的长时阻塞**。

一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。

**如果一个内存队列中可能积压的更新操作特别多**，那么你就要**加机器**，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。

其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。

我们来**实际粗略测算一下**。

如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。

经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。

- 读请求并发量过高

这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。

但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。

- 多服务实例部署的请求路由

可能这个服务部署了多个实例，那么必须**保证**说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器**路由到相同的服务实例上**。

比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。

- 热点商品的路由问题，导致请求的倾斜

万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。

## 36.Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？

这个也是线上非常常见的一个问题，就是**多客户端同时并发写**一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。

而且 Redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。

某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。

你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。

每次要**写之前，先判断**一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。

## 37.Redis 怎么实现分布式锁？

Redis 分布式锁其实就是在系统里面占一个“坑”，其他程序也要占“坑”的时候，占用成功了就可以继续执行，失败了就只能放弃或稍后重试。 占坑一般使用 setnx(set if not exists)指令，只允许被一个程序占有，使用完调用 del 释放锁。

## 38.Redis 分布式锁有什么缺陷？

Redis 分布式锁不能解决超时的问题，分布式锁有一个超时时间，程序的执行如果超出了锁的超时时间就会出现问题。

## 39.一个字符串类型的值能存储最大容量是多少？

512M

## 40.为什么edis 需要把所有数据放到内存中？

Redis 为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis 具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O 速度为严重影响redis 的性能。在内存越来越便宜的今天， redis 将会越来越受欢迎。如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。

## 41.Redis 的同步机制了解么？

Redis 可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb 文件全量同步到复制节点，复制节点接受完成后将rdb 镜像加载到内存。加载完成后， 再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程

## 42.说说Redis 哈希槽的概念？

答： Redis 集群没有使用一致性hash,而是引入了哈希槽的概念， Redis 集群有16384 个哈希槽，每个key 通过CRC16 校验后对16384 取模来决定放置哪个槽，集群的每个节点负责一部分hash 槽。

## 43.Redis 集群最大节点个数是多少？

16384 个。

## 44.Redis 的内存用完了会发生什么？

如果达到设置的上限，Redis 的写命令会返回错误信息（ 但是读命令还可以正常返回。）或者你可以将Redis 当缓存来使用配置淘汰机制，当Redis 达到内存上限时会冲刷掉旧的内容。

## 45.缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题

一、缓存雪崩 我们可以简单的理解为：由于原有缓存失效，新缓存未到期间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓 存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一 系列连锁反应，造成整个系统崩溃。 解决办法： 大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效 时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开。 二、缓存穿透 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再 查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 解决办法; 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从 而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故 障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次 到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 5TB的硬盘上放满了数据，请写一个算法将这些数据进行排重。如果这些数据是一些32bit大小的数据该如何解决？如果是64bit的呢？ 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。 Bitmap： 典型的就是哈希表 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了 布隆过滤器（推荐） 就是引入了k(k>1)k(k>1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。 Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，如果通过 其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确 定该元素存在于集合中。这便是Bloom-Filter的基本思想。 Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 三、缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加 载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 1、直接写个缓存刷新页面，上线时手工操作下； 2、数据量不大，可以在项目启动的时候自动进行加载； 3、定时刷新缓存 四、缓存更新 除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常 见的策略有两种： （1）定时去清理过期的缓存； （2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数 据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较 复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡 五、缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有 损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 以参考日志级别设置预案： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； （3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降 级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一 起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查 询，而是直接返回默认值给用户

## 46.单线程的redis为什么这么快

(一)纯内存操作 (二)单线程操作，避免了频繁的上下文切换 (三)采用了非阻塞I/O多路复用机制

## 47.为什么Redis的操作是原子性的，怎么保证原子性的？

对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。 Redis的操作之所以是原子性的，是因为Redis是单线程的。 Redis本身提供的所有API都是原子操作，Redis中的事务其实是要保证批量操作的原子性。 多个命令在并发中也是原子性的吗？ 不一定， 将get和set改成单命令操作，incr 。使用Redis的事务，或者使用Redis+Lua==的方式实现。

## 48.Redis事务

Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的 Redis会将一个事务中的所有命令序列化，然后按顺序执行。 1.redis 不支持回滚“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。 2.如果在一个事务中的命令出现错误，那么所有的命令都不会执行； 3.如果在一个事务中出现运行错误，那么正确的命令会被执行。 1）MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执 行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 2）EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。当操作被打断时，返回空值 nil 。 3）通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。 4）WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之 后的事务就不会执行，监控一直持续到EXEC命令

## 49.Redis 如何做内存优化？

尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一 个散列表里面。比如你的 web 系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的 key,而是应该把这个用户的 所有信息存储到一张散列表里面.

## 50.Redis 回收进程如何工作的？

一个客户端运行了新的命令，添加了新的数据。Redi 检查内存使用情况，如果大于 maxmemory 的限制, 则根据设定好的策略进行回收。 一个新的命令被执行，等等。所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。如果一个命令的结果 导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。

## 51.一个 Redis 实例最多能存放多少的 keys？List、Set、Sorted Set 他们最多能存放多少元素

理论上 Redis 可以处理多达 232 的 keys，并且在实际中进行了测试，每个实例至少存放了 2 亿 5 千万的 keys。我们正在测试一些较大的 值。任何 list、set、和 sorted set 都可以放 232 个元素。换句话说，Redis 的存储极限是系统中的可用内存值。

## 52.Redis 最适合的场景？

1、会话缓存（Session Cache） 最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他存储（如 Memcached）的优势在于：Redis 提供 持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容 易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台Magento 也提供 Redis 的插件。 2、全页缓存（FPC） 除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不 会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地 FPC。 再次以 agento 为例，Magento提供一个插件来使用 Redis 作为全 页缓存后端。 此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过 的页面。 3、队列 Reids 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis能作为一个很好的消息队列平台来使用。Redis 作为队列使用的 操作，就类似于本地程序语言（如 Python）对 list 的 push/pop 操作。 如果你快速的在 Google中搜索“Redis queues”，你马上就能找到大 量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 4，排行榜/计数器 Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的 非常简单，Redis 只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的 10个用户–我们称之为“user_scores”， 我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执 行： ZRANGE user_scores 0 10 WITHSCORES Agora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数 据的， 你可以在这里看到。 5、发布/订阅 最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可 作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！

## 53.假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？

使用 keys 指令可以扫出指定模式的 key 列表。 对方接着追问：如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问 题？ 这个时候你要回答 redis 关键的一个特性：redis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕， 服务才能恢复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做 一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。

## 54.使用过 Redis 做异步队列么，你是怎么用的？

一般使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有消息的时候，要适当 sleep 一会再重试。 如果对方追问可不可以不用 sleep 呢？ list 还有个指令叫 blpop，在没有消息的时候，它会阻塞住直到消息到来。如果对方追问能不能生产一次消费多次呢？使用 pub/sub 主题订 阅者模式，可以实现1:N 的消息队列。 如果对方追问 pub/sub 有什么缺点？ 在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 RabbitMQ等。 如果对方追问 redis 如何实现延时队列？ 我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用 sortedset，拿时间戳作为score，消息内容作为 key 调用 zadd 来生产消息，消费者用 zrangebyscore 指令获取 N 秒之前的数据轮询进行 处理。到这里，面试官暗地里已经对你竖起了大拇指。但是他不知道的是此刻你却竖起了中指，在椅子背后。

## 55.使用过 Redis 分布式锁么，它是什么回事

先拿 setnx 来争抢锁，抢到之后，再用 expire 给锁加一个过期时间防止锁忘记了释放。 这时候对方会告诉你说你回答得不错，然后接着问如果在 setnx 之后执行 expire之前进程意外 crash 或者要重启维护了，那会怎么样？ 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果 是你主动思考出来的，然后回答：我记得 set 指令有非常复杂的参数，这个应该是可以同时把 setnx 和expire 合成一条指令来用的！对方这 时会显露笑容，心里开始默念：摁，这小子还不错。

## 56.redis可以实现什么效果？

redis：持久化、复制（主从架构）、哨兵（高可用，主备切换）、redis cluster（海量数据+横向扩容+高可用/主备切换）。

持久化：高可用的一部分，在发生redis集群灾难的情况下（比如说部分master+slave全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用。

复制：主从架构，master -> slave 复制，读写分离的架构，写master，读slave，横向扩容slave支撑更高的读吞吐，读高并发，10万，20万，30万，上百万，QPS，横向扩容。

哨兵：高可用，主从架构，在master故障的时候，快速将slave切换成master，实现快速的灾难恢复，实现高可用性。

redis cluster：多master读写，数据分布式的存储，横向扩容，水平扩容，快速支撑高达的数据量+更高的读写QPS，自动进行master -> slave的主备切换，高可用。

让底层的缓存系统，redis，实现能够任意水平扩容，支撑海量数据（1T+，几十T，10G * 600 redis = 6T），支撑很高的读写QPS（redis单机在几万QPS，10台，几十万QPS），高可用性（给我们每个redis实例都做好AOF+RDB的备份策略+容灾策略，slave -> master主备切换）。

1T+海量数据、10万+读写QPS、99.99%高可用性。

## 57.redis的企业级的架构

第一套

redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性）。

可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99%。

第二套

海量数据。

redis cluster。

多master分布式存储数据，水平扩容。

支撑更多的数据量，1T+以上没问题，只要扩容master即可。

读写QPS分别都达到几十万都没问题，只要扩容master即可，redis cluster，读写分离，支持不太好，readonly才能去slave上读。

支撑99.99%可用性，也没问题，slave -> master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下）。

### 58.缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题

一、缓存雪崩 我们可以简单的理解为：由于原有缓存失效，新缓存未到期间 (例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访 问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从 而形成一系列连锁反应，造成整个系统崩溃。 解决办法： 大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据 库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓 存失效时间分散开。 二、缓存穿透 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在 缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请 求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。

解决办法; 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存 在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故 障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设 置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单 粗暴。 5TB的硬盘上放满了数据，请写一个算法将这些数据进行排重。如果这些数据是一些32bit大小的数据该 如何解决？如果是64bit的呢？ 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。 Bitmap： 典型的就是哈希表 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空 间、时间来完成了。 布隆过滤器（推荐） 就是引入了k(k>1)k(k>1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过 程。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。 Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突， 我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定 不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这 便是Bloom-Filter的基本思想。 Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 三、缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系 统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据 库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 1、直接写个缓存刷新页面，上线时手工操作下； 2、数据量不大，可以在项目启动的时候自动进行加载； 3、定时刷新缓存； 四、缓存更新 除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的 业务需求进行自定义的缓存淘汰，常见的策略有两种： （1）定时去清理过期的缓存； （2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数 据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过 来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 五、缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然 需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开 关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结 算）。 以参考日志级别设置预案： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级， 并发送告警；

（3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的 最大阀值，此时可以根据情况自动降级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要 的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查 询，而是直接返回默认值给用户。

## 58.热点数据和冷数据是什么

热点数据，缓存才有价值 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不 大。频繁修改的数据，看情况考虑使用缓存 对于上面两个例子，寿星列表、导航信息都存在一个特点，就是信息修改频率不高，读取通常非常高的 场景。 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。 再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。 数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那 就没有太大价值了。 那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压 力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助 手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同 步保存到Redis缓存，减少数据库压力。

## 59.Redis 为什么是单线程的

官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内 存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的 方案了（毕竟采用多线程会有很多麻烦！）Redis利用队列技术将并发访问变为串行访问 1）绝大部分请求是纯粹的内存操作（非常快速）2）采用单线程,避免了不必要的上下文切换和竞争条

件 3）非阻塞IO优点： 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度 都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除如何解决redis的并发 竞争key问题 同时有多个子系统去set一个key。这个时候要注意什么呢？ 不推荐使用redis的事务机制。因为我们的 生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候， 这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。 (1)如果对这个key操作，不要求顺序： 准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可 (2)如果对这个key操作，要求顺序： 分布式锁+时间戳。 假设这会系统B先抢到锁，将key1设置为 {valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操 作了。以此类推。 (3) 利用队列，将set方法变成串行访问也可以redis遇到高并发，如果保证读写key的一致性 对redis的操作都是具有原子性的,是线程安全的操作,你不用考虑并发问题,redis内部已经帮你处理好并 发的问题了。

## 60.为什么Redis的操作是原子性的，怎么保证原子性的？

对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。 Redis的操作之所以是原子性的，是因为Redis是单线程的。 Redis本身提供的所有API都是原子操作，Redis中的事务其实是要保证批量操作的原子性。 多个命令在并发中也是原子性的吗？ 不一定， 将get和set改成单命令操作，incr 。使用Redis的事务，或者使用Redis+Lua==的方式实现.

## 61.双写一致方案

1、先删除缓存，后更更新数据库

缓存删除成功后为空了了，但是数据库却失败了了，还是原来的旧数据。 如果这时候有请求过来的话，⼀一看缓存中没有数据，于是就到数据库读取了了旧数据更更新到缓存中。 如果你的项⽬目并发量量很低的话，每天访问量量就那么点，那这么⽤用没⽑毛病，很少情况下才会出现数据不不⼀一 致的问题。

2、缓存延时双删策略

如果同时来了了两个请求，⼀一个写请求，⼀一个读请求。 写请求先删除Redis中的数据，然后去数据库进行更新操作。读请求判断Redis中有没有数据，没有数据 时去请求数据库，拿到数据后写入缓存中。 但是写请求此时并没有更新成功，或者执行了一个事务还没有成功。 这样的话，读请求拿到未修改的旧数据写入缓存。过了一会儿，写请求将数据库更新成功了，那么此时 缓存与库中的数据就不一致了。

写请求过来先把 Redis缓存删掉，等数据库更新成功后，异步等待一段时间再次把缓存删掉。这种方案 读取速度快，但是会出现短时间的脏数据。

## 62.5种数据类型

1、string是redis最基本的类型，可以理解成与memcached⼀模⼀样的类型，⼀个key对应⼀个value。value不仅是 string，也可以是数字。string类型是⼆进制安全的，意思是redis的string类型可以包含任何数据，⽐如jpg图⽚或者序 列化的对象。string类型的值最⼤能存储512M。 2、Hash是⼀个键值（key-value）的集合。redis的hash是⼀个string的key和value的映射表，Hash特别适合存储对 象。常⽤命令：hget,hset,hgetall等。 3、list列表是简单的字符串列表，按照插⼊顺序排序。可以添加⼀个元素到列表的头部（左边）或者尾部（右边） 常⽤ 命令：lpush、rpush、lpop、rpop、lrange(获取列表⽚段)等。应⽤场景：list应⽤场景⾮常多，也是Redis最重要的数 据结构之⼀，⽐如twitter的关注列表，粉丝列表都可以⽤list结构来实现。数据结构：list就是链表，可以⽤来当消息队 列⽤。redis提供了List的push和pop操作，还提供了操作某⼀段的api，可以直接查询或者删除某⼀段的元素。实现⽅ 式：redis list的是实现是⼀个双向链表，既可以⽀持反向查找和遍历，更⽅便操作，不过带来了额外的内存开销。 4、set是string类型的⽆序集合。集合是通过hashtable实现的。set中的元素是没有顺序的，⽽且是没有重复的。常⽤ 命令：sdd、spop、smembers、sunion等。应⽤场景：redis set对外提供的功能和list⼀样是⼀个列表，特殊之处在于 set是⾃动去重的，⽽且set提供了判断某个成员是否在⼀个set集合中。 5、zset和set⼀样是string类型元素的集合，且不允许重复的元素。常⽤命令：zadd、zrange、zrem、zcard等。使⽤ 场景：sorted set可以通过⽤⼾额外提供⼀个优先级（score）的参数来为成员排序，并且是插⼊有序的，即⾃动排序。 当你需要⼀个有序的并且不重复的集合列表，那么可以选择sorted set结构。和set相⽐，sorted set关联了⼀个double 类型权重的参数score，使得集合中的元素能够按照score进⾏有序排列，redis正是通过分数来为集合中的成员进⾏从⼩ 到⼤的排序。实现⽅式：Redis sorted set的内部使⽤HashMap和跳跃表(skipList)来保证数据的存储和有 序，HashMap⾥放的是成员到score的映射，⽽跳跃表⾥存放的是所有的成员，排序依据是HashMap⾥存的score，使 ⽤跳跃表的结构可以获得⽐较⾼的查找效率，并且在实现上⽐较简单。

## 63.redis缓存怎么用的

我是结合spring boot使⽤的。⼀般有两种⽅式，⼀种是直接通过RedisTemplate来使⽤，另⼀种是使⽤spring cache集成Redis（也就是注解的⽅式）。

缓存注解

1、@Cacheable 根据⽅法的请求参数对其结果进⾏缓存 key：缓存的key，可以为空，如果指定要按照SPEL表达式编写，如果不指定，则按照⽅法的所有参数进⾏组合。 value：缓存的名称，必须指定⾄少⼀个（如 @Cacheable (value='user')或者@Cacheable(value={'user1','user2'})） condition：缓存的条件，可以为空，使⽤SPEL编写，返回true或者false，只有为true才进⾏缓存。 2、@CachePut 根据⽅法的请求参数对其结果进⾏缓存，和@Cacheable不同的是，它每次都会触发真实⽅法的调⽤。参数描述 ⻅上。 3、@CacheEvict 根据条件对缓存进⾏清空 key：同上 value：同上 condition：同上 allEntries：是否清空所有缓存内容，缺省为false，如果指定为true，则⽅法调⽤后将⽴即清空所有缓存 beforeInvocation：是否在⽅法执⾏前就清空，缺省为false，如果指定为true，则在⽅法还没有执⾏的时候就清空缓存。缺 省情况下，如果⽅法执⾏抛出异常，则不会清空缓存。

缓存问题

缓存和数据库数据⼀致性问题：分布式环境下⾮常容易出现缓存和数据库间数据⼀致性问题，针对这⼀点，如果项⽬对 缓存的要求是强⼀致性的，那么就不要使⽤缓存。我们只能采取合适的策略来降低缓存和数据库间数据不⼀致的概率，⽽⽆ 法保证两者间的强⼀致性。合适的策略包括合适的缓存更新策略，更新数据库后及时更新缓存、缓存失败时增加重试机制。

## 64.缓存穿透和击穿、雪崩

缓存雪崩

缓存中所有Key同⼀时间⼤⾯积失效，瞬间Redis跟没有⼀样

处理缓存雪崩简单，在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值就好了，这样可以保证数据不会再同⼀时间⼤⾯积失效。如果Redis是集群部署，将热点数据均匀分布在不同的Redis库中也能避免全部失效。或者设置热点数据永不过期，有更新操作就更新缓存就好了。

缓存穿透

缓存穿透是指缓存和数据库中都没有的数据，⽽⽤⼾（⿊客）不断发起请求，击垮数据库。

缓存穿透我会在接⼝层增加校验，⽐如⽤⼾鉴权，参数做校验，不合法的校验直接return，⽐如id做基础校验，id<=0直接拦截。

Redis⾥还有⼀个⾼级⽤法**布隆过滤器（Bloom Filter）**这个也能很好的预防缓存穿透的发⽣，他的原理也很简单，就是利⽤⾼效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查DB刷新KV再return。

缓存击穿

缓存击穿不同的是缓存击穿是指⼀个Key⾮常热点，在不停地扛着⼤量的请求，⼤并发集中对这⼀个点进⾏访问，当这个Key在失效的瞬间，持续的⼤并发直接落到了数据库上，就在这个Key的点上击穿了缓存。

设置热点数据永不过期，或者加上互斥锁就搞定了。

## 65.Redis是单线程的，为什么还能这么快吗？

第⼀：Redis完全基于内存，绝⼤部分请求是纯粹的内存操作，⾮常迅速，数据存在内存中，类似于 HashMap，HashMap的优势就是查找和操作的时间复杂度是O(1)。第⼆：数据结构简单，对数据操作也简单。第三：采⽤单 线程，避免了不必要的上下⽂切换和竞争条件，不存在多线程导致的CPU切换，不⽤去考虑各种锁的问题，不存在加锁释放 锁操作，没有死锁问题导致的性能消耗。第四：使⽤多路复⽤IO模型，⾮阻塞IO。

## 66.redis的持久化机制

redis为了保证效率，数据缓存在了内存中，但是会周期性的把更新的数据写⼊磁盘或者把修改操作写⼊追加的记录⽂件 中，以保证数据的持久化。Redis的持久化策略有两种：1、RDB：快照形式是直接把内存中的数据保存到⼀个dump的⽂件 中，定时保存，保存策略。2、AOF：把所有的对Redis的服务器进⾏修改的命令都存到⼀个⽂件⾥，命令的集合。Redis默认 是快照RDB的持久化⽅式。当Redis重启的时候，它会优先使⽤AOF⽂件来还原数据集，因为AOF⽂件保存的数据集通常⽐ RDB⽂件所保存的数据集更完整。你甚⾄可以关闭持久化功能，让数据只在服务器运⾏时存。

DB是怎么⼯作的？

默认Redis是会以快照"RDB"的形式将数据持久化到磁盘的⼀个⼆进制⽂件dump.rdb。⼯作原理简单说⼀下：当Redis 需要做持久化时，Redis会fork⼀个⼦进程，⼦进程将数据写到磁盘上⼀个临时RDB⽂件中。当⼦进程完成写临时⽂件后，将 原来的RDB替换掉，这样的好处是可以copy-on-write。

RDB的优点是：这种⽂件⾮常适合⽤于备份：⽐如，你可以在最近的24⼩时内，每⼩时备份⼀次，并且在每个⽉的每⼀ 天也备份⼀个RDB⽂件。这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。RDB⾮常适合灾难恢复。RDB 的缺点是：如果你需要尽量避免在服务器故障时丢失数据，那么RDB不合适你。

AOF

使⽤AOF做持久化，每⼀个写命令都通过write函数追加到appendonly.aof中，AOF可以做到全程持久化，只需要在配置中开启 appendonly yes。这样redis每执⾏⼀个修改数据的命令，都会把它添加到AOF⽂件中，当redis重启时，将会读取AOF⽂件进⾏重放，恢复到redis关闭前的最后时刻。

使⽤AOF的优点是会让redis变得⾮常耐久。可以设置不同的fsync策略，aof的默认策略是每秒钟 fsync⼀次，在这种配置下，就算发⽣故障停机，也最多丢失⼀秒钟的数据。缺点是对于相同的数据集来说，AOF的⽂件体积 通常要⼤于RDB⽂件的体积。根据所使⽤的fsync策略，AOF的速度可能会慢于RDB。

选择

如果你⾮常关⼼你的数据，但仍然可以承受数分钟内的数据丢失，那么可以额只使⽤RDB持久。AOF将Redis执⾏的每⼀ 条命令追加到磁盘中，处理巨⼤的写⼊会降低Redis的性能，不知道你是否可以接受。数据库备份和灾难恢复：定时⽣成RDB 快照⾮常便于进⾏数据库备份，并且RDB恢复数据集的速度也要⽐AOF恢复的速度快。当然了，redis⽀持同时开启RDB和 AOF，系统重启后，redis会优先使⽤AOF来恢复数据，这样丢失的数据会最少。

## 67.主从复制

redis单节点存在单点故障问题，为了解决单点问题，⼀般都需要对redis配置从节点，然后使⽤哨兵来监听主节点的存活状态，如果主节点挂掉，从节点能继续提供缓存功能。

主从配置结合哨兵模式能解决单点故障问题，提⾼redis可⽤性。从节点仅提供读操作，主节点提供写操作。对于读多写少的状况，可给主节点配置多个从节点，从⽽提⾼响应效率。

关于复制过程，是这样的：1、从节点执.slaveof 【masterIP][masterPort]，保存主节点信息 2、从 节点中的定时任务发现主节点信息，建.和主节点的socket连接 3、从节点发送Ping信号，主节点返回Pong，两边能互相通 信 4、连接建.后，主节点将所有数据发送给从节点（数据同步） 5、主节点把当前的数据同步给从节点后，便完成了复制的 建.过程。接下来，主节点就会持续的把写命令发送给从节点，保证主从数据.致性。

数据同步的过程

redis2.8之前使⽤sync【runId][offset]同步命令，redis2.8之后使⽤psync[runId] [offset]命令。两者不同在于，sync命令仅⽀持全量复制过程，psync⽀持全量和部分复制。介绍同步之前，先介绍⼏个概 念：runId：每个redis节点启动都会⽣成唯⼀的uuid，每次redis重启后，runId都会发⽣变化。offset：主节点和从节点都各 ⾃维护⾃⼰的主从复制偏移量offset，当主节点有写⼊命令时，offset=offset+命令的字节⻓度。从节点在收到主节点发送的 命令后，也会增加⾃⼰的offset，并把⾃⼰的offset发送给主节点。这样，主节点同时保存⾃⼰的offset和从节点的offset， 通过对⽐offset来判断主从节点数据是否⼀致。repl_backlog_size：保存在主节点上的⼀个固定⻓度的先进先出队列，默认 ⼤⼩是1MB。（1）主节点发送数据给从节点过程中，主节点还会进⾏⼀些写操作，这时候的数据存储在复制缓冲区中。从节 点同步主节点数据完成后，主节点将缓冲区的数据继续发送给从节点，⽤于部分复制。（2）主节点响应写命令时，不但会把 命名发送给从节点，还会写⼊复制积压缓冲区，⽤于复制命令丢失的数据补救。

从节点发送psync【runId][offset]命令，主节点有三种响应：（1）FULLRESYNC：第⼀次连接，进⾏ 全量复制 （2）CONTINUE：进⾏部分复制 （3）ERR：不⽀持psync命令，进⾏全量复制

全量复制

1、从节点发送psync ? -1命令（因为第⼀次发送，不知道主节点的runId，所以为?，因为是第⼀次复制，所以offset=-1）。 2、主节点发现从节点是第⼀次复制，返回FULLRESYNC {runId} {offset}，runId是主节点的runId，offset是主节点⽬前的 offset。 3、从节点接收主节点信息后，保存到info中。 4、主节点在发送FULLRESYNC后，启动bgsave命令，⽣成RDB⽂件（数据持久化）。 5、主节点发送RDB⽂件给从节点。到从节点加载数据完成这段期间主节点的写命令放⼊缓冲区。 6、从节点清理⾃⼰的数据库数据。 7、从节点加载RDB⽂件，将数据保存到⾃⼰的数据库中。 8、如果从节点开启了AOF，从节点会异步重写AOF⽂件。

部分复制

1、部分复制主要是Redis针对全量复制的过⾼开销做出的⼀种优化措施，使⽤psync[runId] [offset]命令实现。当从节点正在复制主节点时，如果出现⽹络闪断或者命令丢失等异常情况时，从节点会向主节点要求补发丢失 的命令数据，主节点的复制积压缓冲区将这部分数据直接发送给从节点，这样就可以保持主从节点复制的⼀致性。补发的这部分 数据⼀般远远⼩于全量数据。2、主从连接中断期间主节点依然响应命令，但因复制连接中断命令⽆法发送给从节点，不过主节 点内的复制积压缓冲区依然可以保存最近⼀段时间的写命令数据。3、当主从连接恢复后，由于从节点之前保存了⾃⾝已复制的 偏移量和主节点的运⾏ID。因此会把它们当做psync参数发送给主节点，要求进⾏部分复制。4、主节点接收到psync命令后⾸先 核对参数runId是否与⾃⾝⼀致，如果⼀致，说明之前复制的是当前主节点；之后根据参数offset在复制积压缓冲区中查找，如果 offset之后的数据存在，则对从节点发送+COUTINUE命令，表⽰可以进⾏部分复制。因为缓冲区⼤⼩固定，若发⽣缓冲溢出，则 进⾏全量复制。5、主节点根据偏移量把复制积压缓冲区⾥的数据发送给从节点，保证主从复制进⼊正常状态。

主从复制会存在以下问题

1、⼀旦主节点宕机，从节点晋升为主节点，同时需要修改应⽤⽅的主节点地址，还需要命令 所有从节点去复制新的主节点，整个过程需要⼈⼯⼲预。2、主节点的写能⼒受到单机的限制。3、主节点的存储能⼒受到单 机的限制。4、原⽣复制的弊端在早期的版本中也会⽐较突出，⽐如：redis复制中断后，从节点会发起psync。此时如果同步 不成功，则会进⾏全量同步，主库执⾏全量备份的同时，可能会造成毫秒或秒级的卡顿。

## 68.Sentine（哨兵）

Redis Sentine（l 哨兵）主要功能包括主节点存活检测、主从运⾏情况检 测、⾃动故障转移、主从切换。Redis Sentinel最⼩配置是⼀主⼀从。Redis的Sentinel系统可以⽤来管理多个Redis服务 器，该系统可以执⾏以下四个任务：1、监控：不断检查主服务器和从服务器是否正常运⾏。2、通知：当被监控的某个redis 服务器出现问题，Sentinel通过API脚本向管理员或者其他应⽤程序发出通知。3、⾃动故障转移：当主节点不能正常⼯作 时，Sentinel会开始⼀次⾃动的故障转移操作，它会将与失效主节点是主从关系的其中⼀个从节点升级为新的主节点，并且 将其他的从节点指向新的主节点，这样⼈⼯⼲预就可以免了。4、配置提供者：在Redis Sentinel模式下，客⼾端应⽤在初始 化时连接的是Sentinel节点集合，从中获取主节点的信息。

哨兵的⼯作原理

1、每个Sentinel节点都需要定期执⾏以下任务：每个Sentinel以每秒⼀次的频率，向它所知的主服务器、从服务器以及其他的 Sentinel实例发送⼀个PING命令。

2、如果⼀个实例距离最后⼀次有效回复PING命令的时间超过down-after-milliseconds所指定的值，那么这个实例会被 Sentinel标记为主观下线。

3、如果⼀个主服务器被标记为主观下线，那么正在监视这个服务器的所有Sentinel节点，要以每秒⼀次的频率确认主服务器的确 进⼊了主观下线状态。

4、如果⼀个主服务器被标记为主观下线，并且有⾜够数量的Sentinel（⾄少要达到配置⽂件指定的数量）在指定的时间范围内同 意这⼀判断，那么这个主服务器被标记为客观下线。

5、⼀般情况下，每个Sentinel会以每10秒⼀次的频率向它已知的所有主服务器和从服务器发送INFO命令，当⼀个主服务器被标 记为客观下线时，Sentinel向下线主服务器的所有从服务器发送INFO命令的频率，会从10秒⼀次改为每秒⼀次。

6、Sentinel和其他Sentinel协商客观下线的主节点的状态，如果处于SDOWN状态，则投票⾃动选出新的主节点，将剩余从节点 指向新的主节点进⾏数据复制。

7、当没有⾜够数量的Sentinel同意主服务器下线时，主服务器的客观下线状态就会被移除。当主服务器重新向Sentinel的PING 命令返回有效回复时，主服务器的主观下线状态就会被移除。

## 性能

### 1.Redis 常见的性能问题都有哪些？如何解决？

（1 ）、 Master 写内存快照， save 命令调度 rdbSave 函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以 Master 最好不要写内存快照。 （2 ）、 Master AOF 持久化，如果不重写 AOF 文件，这个持久化方式对性能的影响是最小的，但是 AOF 文件会不断增大， AOF 文件过大会影响 Master 重启的恢复速度。 Master 最好不要做任何持久化工作，包括内存快照和 AOF日志文件，特别是不要启用内存快照做持久化 如果数据比较关键，某个 Slave 开启 AOF 备份数据，策略为每秒同步一次。 （3 ）、 Master 调用 BGREWRITEAOF 重写 AOF 文件， AOF 在重写的时候会占大量的 CPU 和内存资源，导致服务 load 过高，出现短暂服务暂停现象。 （4 ）、 Redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性， Slave 和 Master 最好在同一个局域网内

### 2.生产环境中的 Redis 是怎么部署的？

看看你了解不了解你们公司的 Redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 Redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 Redis 给几个 G 的内存？设置了哪些参数？压测后你们 Redis 集群承载多少 QPS？

Redis cluster，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 QPS 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。

机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10g 内存，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

5 台机器对外提供读写，一共有 50g 内存。

因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。

你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。

其实大型的公司，会有基础架构的 team 负责缓存集群的运维。

### 3.Redis 常见性能问题和解决方案：

1、Master 最好不要写内存快照，如果 Master 写内存快照，save 命令调度 rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能 影响是非常大的，会间断性暂停服务 2、如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一3、为了主从复制的速度和连接的稳定性，Master 和 Slave 最好在同一个局域网 4、尽量避免在压力很大的主库上增加从 5、主从复制不要用图状结构，用单向链表结构更为稳定，即：Master <- Slave1<- Slave2 <- Slave3…这样的结构方便解决单点故障问题， 实现 Slave 对 Master的替换。如果 Master 挂了，可以立刻启用 Slave1 做 Master，其他不变。

### 4.如果redis要支撑超过10万+的并发，那应该怎么做？

单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂。

单机在几万。

读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千。

大量的请求都是读，一秒钟二十万次读。

读写分离。

主从架构 -> 读写分离 -> 支撑10万+读QPS的架构。

架构做成主从架构，一主多从，主负责写，并且将数据同步复制到其他的slave节点，从节点负责读。所有的读请求全部走从节点。

可支持水平扩展的读高并发架构。

水平扩容，就是说，如果你的读QPS再增加，也很简单，继续增加redis slave即可。

## 持久化

### 1.Redis的持久化

Redis 如果仅仅只是将数据缓存在内存里面，如果 Redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 Redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。

如果 Redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。

持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 Redis 整个挂了，然后 Redis 就不可用了，你要做的事情就是让 Redis 变得可用，尽快变得可用。

重启 Redis，尽快让它对外提供服务，如果没做数据备份，这时候 Redis 启动了，也不可用啊，数据都没了。

Redis 持久化的两种方式

- RDB：RDB 持久化机制，是对 Redis 中的数据执行**周期性**的持久化。
- AOF：AOF 机制对每条写入命令作为日志，以 `append-only` 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过**回放** AOF 日志中的写入指令来重新构建整个数据集。

通过 RDB 或 AOF，都可以将 Redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。

如果 Redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 Redis，Redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。

如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会使用 **AOF** 来重新构建数据，因为 AOF 中的**数据更加完整**。

RDB 优缺点

- RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 Redis 的数据，这种多个数据文件的方式，**非常适合做冷备**，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 Redis 中的数据。
- RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis **保持高性能**，因为 Redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。
- 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 Redis 进程，更加快速。
- 如果想要在 Redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 Redis 进程宕机，那么会丢失最近 5 分钟的数据。
- RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。

AOF 优缺点

- AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 `fsync` 操作，最多丢失 1 秒钟的数据。
- AOF 日志文件以 `append-only` 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。
- AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 `rewrite` log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。
- AOF 日志文件的命令通过可读较强的方式进行记录，这个特性非常**适合做灾难性的误删除的紧急恢复**。比如某人不小心用 `flushall` 命令清空了所有数据，只要这个时候后台 `rewrite` 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 `flushall` 命令给删了，然后再将该 `AOF` 文件放回去，就可以通过恢复机制，自动恢复所有数据。
- 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。
- AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 `fsync` 一次日志文件，当然，每秒一次 `fsync` ，性能也还是很高的。（如果实时写入，那么 QPS 会大降，Redis 性能会大大降低）
- 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是**基于当时内存中的数据进行指令的重新构建**，这样健壮性会好很多。

RDB 和 AOF 到底该如何选择

- 不要仅仅使用 RDB，因为那样会导致你丢失很多数据；
- 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug；
- Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。

### 2.如何配置RDB持久化机制

redis.conf文件，也就是/etc/redis/6379.conf，去配置持久化。

save 60 1000

每隔60s，如果有超过1000个key发生了变更，那么就生成一个新的dump.rdb文件，就是当前redis内存中完整的数据快照，这个操作也被称之为snapshotting，快照。

也可以手动调用save或者bgsave命令，同步或异步执行rdb快照生成。

save可以设置多个，就是多个snapshotting检查点，每到一个检查点，就会去check一下，是否有指定的key数量发生了变更，如果有，就生成一个新的dump.rdb文件。

### 3.RDB持久化机制的工作流程

（1）redis根据配置自己尝试去生成rdb快照文件。 （2）fork一个子进程出来。 （3）子进程尝试将数据dump到临时的rdb快照文件中。 （4）完成rdb快照文件的生成之后，就替换之前的旧的快照文件。

dump.rdb，每次生成一个新的快照，都会覆盖之前的老快照。

### 4.AOF持久化的配置

AOF持久化，默认是关闭的，默认是打开RDB持久化。

appendonly yes，可以打开AOF持久化机制，在生产环境里面，一般来说AOF都是要打开的，除非你说随便丢个几分钟的数据也无所谓。

打开AOF持久化机制之后，redis每次接收到一条写命令，就会写入日志文件中，当然是先写入os cache的，然后每隔一定时间再fsync一下。

而且即使AOF和RDB都开启了，redis重启的时候，也是优先通过AOF进行数据恢复的，因为aof数据比较完整。

可以配置AOF的fsync策略，有三种策略可以选择，一种是每次写入一条数据就执行一次fsync; 一种是每隔一秒执行一次fsync; 一种是不主动执行fsync。

always: 每次写入一条数据，立即将这个数据对应的写日志fsync到磁盘上去，性能非常非常差，吞吐量很低; 确保说redis里的数据一条都不丢，那就只能这样了。

mysql -> 内存策略，大量磁盘，QPS到多少，一两k。QPS，每秒钟的请求数量。 redis -> 内存，磁盘持久化，QPS到多少，单机，一般来说，上万QPS没问题。

everysec: 每秒将os cache中的数据fsync到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS还是可以上万的。

no: 仅仅redis负责将数据写入os cache就撒手不管了，然后后面os自己会时不时有自己的策略将数据刷入磁盘，不可控了。

### 5.AOF rewrite

redis中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被redis用缓存清除的算法清理掉。

redis中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在redis内存中。

所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在AOF中，AOF日志文件就一个，会不断的膨胀，到很大很大。

所以AOF会自动在后台每隔一定时间做rewrite操作，比如日志里已经存放了针对100w数据的写日志了; redis内存只剩下10万; 基于内存中当前的10万数据构建一套最新的日志，到AOF中; 覆盖之前的老日志; 确保AOF日志文件不会过大，保持跟redis内存数据量一致。

redis 2.4之前，还需要手动，开发一些脚本，crontab，通过BGREWRITEAOF命令去执行AOF rewrite，但是redis 2.4之后，会自动进行rewrite操作。

在redis.conf中，可以配置rewrite策略。

auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb

比如说上一次AOF rewrite之后，是128mb。

然后就会接着128mb继续写AOF的日志，如果发现增长的比例，超过了之前的100%，256mb，就可能会去触发一次rewrite。

但是此时还要去跟min-size，64mb去比较，256mb > 64mb，才会去触发rewrite。

（1）redis fork一个子进程。 （2）子进程基于当前内存中的数据，构建日志，开始往一个新的临时的AOF文件中写入日志。 （3）redis主进程，接收到client新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的AOF文件。 （4）子进程写完新的日志文件之后，redis主进程将内存中的新日志再次追加到新的AOF文件中。 （5）用新的日志文件替换掉旧的日志文件。

### 6.AOF破损文件的修复

如果redis在append数据到AOF文件时，机器宕机了，可能会导致AOF文件破损。

用redis-check-aof --fix命令来修复破损的AOF文件。

### 7.AOF和RDB同时工作

（1）如果RDB在执行snapshotting操作，那么redis不会执行AOF rewrite; 如果redis再执行AOF rewrite，那么就不会执行RDB snapshotting。 （2）如果RDB在执行snapshotting，此时用户执行BGREWRITEAOF命令，那么等RDB快照生成之后，才会去执行AOF rewrite。 （3）同时有RDB snapshot文件和AOF日志文件，那么redis重启的时候，会优先使用AOF进行数据恢复，因为其中的日志更完整。

### 8.企业级的持久化的配置策略

在企业中，RDB的生成策略，用默认的也差不多。

save 60 10000：如果你希望尽可能确保说，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，低峰期，数据量很少，也没必要。

10000->生成RDB，1000->RDB，这个根据你自己的应用和业务的数据量，你自己去决定。

AOF一定要打开，fsync，everysec。

auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍。 auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb。

### 9.企业级的数据备份方案

RDB非常适合做冷备，每次生成之后，就不会再有修改了。

数据备份方案。

（1）写crontab定时调度脚本去做数据备份。 （2）每小时都copy一份rdb的备份，到一个目录中去，仅仅保留最近48小时的备份。 （3）每天都保留一份当日的rdb的备份，到一个目录中去，仅仅保留最近1个月的备份。 （4）每次copy备份的时候，都把太旧的备份给删了。 （5）每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去。

### 10.数据恢复方案

（1）如果是redis进程挂掉，那么重启redis进程即可，直接基于AOF日志文件恢复数据。

fsync everysec，最多就丢一秒的数。

（2）如果是redis进程所在机器挂掉，那么重启机器后，尝试重启redis进程，尝试直接基于AOF日志文件进行数据恢复。

AOF没有破损，也是可以直接基于AOF恢复的。

AOF append-only，顺序写入，如果AOF文件破损，那么用redis-check-aof fix。

（3）如果redis当前最新的AOF和RDB文件出现了丢失/损坏，那么可以尝试基于该机器上当前的某个最新的RDB数据副本进行数据恢复。

当前最新的AOF和RDB文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，人为。

大数据系统，hadoop，有人不小心就把hadoop中存储的大量的数据文件对应的目录，rm -rf一下，我朋友的一个小公司，运维不太靠谱，权限也弄的不太好。

/var/redis/6379下的文件给删除了。

找到RDB最新的一份备份，小时级的备份可以了，小时级的肯定是最新的，copy到redis里面去，就可以恢复到某一个小时的数据。

appendonly.aof + dump.rdb，优先用appendonly.aof去恢复数据，但是我们发现redis自动生成的appendonly.aof是没有数据的。

然后我们自己的dump.rdb是有数据的，但是明显没用我们的数据。

redis启动的时候，自动重新基于内存的数据，生成了一份最新的rdb快照，直接用空的数据，覆盖掉了我们有数据的，拷贝过去的那份dump.rdb。

你停止redis之后，其实应该先删除appendonly.aof，然后将我们的dump.rdb拷贝过去，然后再重启redis。

很简单，就是虽然你删除了appendonly.aof，但是因为打开了aof持久化，redis就一定会优先基于aof去恢复，即使文件不在，那就创建一个新的空的aof文件。

停止redis，暂时在配置中关闭aof，然后拷贝一份rdb过来，再重启redis，数据能不能恢复过来，可以恢复过来。

脑子一热，再关掉redis，手动修改配置文件，打开aof，再重启redis，数据又没了，空的aof文件，所有数据又没了。

在数据安全丢失的情况下，基于rdb冷备，如何完美的恢复数据，同时还保持aof和rdb的双开。

停止redis，关闭aof，拷贝rdb备份，重启redis，确认数据恢复，直接在命令行热修改redis配置，打开aof，这个redis就会将内存中的数据对应的日志，写入aof文件中。

此时aof和rdb两份数据文件的数据就同步了。

redis config set热修改配置参数，可能配置文件中的实际的参数没有被持久化的修改，再次停止redis，手动修改配置文件，打开aof的命令，再次重启redis。

（4）如果当前机器上的所有RDB文件全部损坏，那么从远程的云服务上拉取最新的RDB快照回来恢复数据。

（5）如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了，那么可以选择某个更早的时间点，对数据进行恢复。

举个例子，12点上线了代码，发现代码有bug，导致代码生成的所有的缓存数据，写入redis，全部错了。

找到一份11点的rdb的冷备，然后按照上面的步骤，去恢复到11点的数据，不就可以了吗。

## 高可用

### 1.redis replication的核心机制

（1）redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量。 （2）一个master node是可以配置多个slave node的。 （3）slave node也可以连接其他的slave node。 （4）slave node做复制的时候，是不会block master node的正常工作的。 （5）slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了。 （6）slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量。

slave，高可用性，有很大的关系。

### 2.master持久化对于主从架构的安全保障的意义

如果采用了主从架构，那么建议必须开启master node的持久化！

不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了。

master -> RDB和AOF都关闭了 -> 全部在内存中。

master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己IDE数据是空的。

master就会将空的数据集同步到slave上去，所有slave的数据全部清空。

100%的数据丢失。

master节点，必须要使用持久化机制。

第二个，master的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份rdb去恢复master; 这样才能确保master启动的时候，是有数据的。

### 3.主从架构的核心原理

当启动一个slave node的时候，它会发送一个PSYNC命令给master node。

如果这是slave node重新连接master node，那么master node仅仅会复制给slave部分缺少的数据; 否则如果是slave node第一次连接master node，那么会触发一次full resynchronization。

开始full resynchronization的时候，master会启动一个后台线程，开始生成一份RDB快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB文件生成完毕之后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中。然后master会将内存中缓存的写命令发送给slave，slave也会同步这些数据。

slave node如果跟master node有网络故障，断开了连接，会自动重连。master如果发现有多个slave node都来重新连接，仅仅会启动一个rdb save操作，用一份数据服务所有slave node。

### 4.从复制的断点续传

从redis 2.8开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。

master node会在内存中常见一个backlog，master和slave都会保存一个replica offset还有一个master id，offset就是保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制。

但是如果没有找到对应的offset，那么就会执行一次resynchronization。

### 5.无磁盘化复制

master在内存中直接创建rdb，然后发送给slave，不会在自己本地落地磁盘了。

repl-diskless-sync repl-diskless-sync-delay，等待一定时长再开始复制，因为要等更多slave重新连接过来。

### 6.过期key处理

slave不会过期key，只会等待master过期key。如果master过期了一个key，或者通过LRU淘汰了一个key，那么会模拟一条del命令发送给slave。

### 7.复制的完整流程

（1）slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始。

master host和ip是从哪儿来的，redis.conf里面的slaveof配置的。

（2）slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接。 （3）slave node发送ping命令给master node。 （4）口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证。 （5）master node第一次执行全量复制，将所有数据发给slave node。 （6）master node后续持续将写命令，异步复制给slave node。

### 8.数据同步相关的核心机制

指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制。

（1）master和slave都会维护一个offset

master会在自身不断累加offset，slave也会在自身不断累加offset。 slave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset。

这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况。

（2）backlog

master node有一个backlog，默认是1MB大小。 master node给slave node复制数据时，也会将数据在backlog中同步写一份。 backlog主要是用来做全量复制中断候的增量复制的。

（3）master run id

info server，可以看到master run id。 如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制。 如果需要不更改run id重启redis，可以使用redis-cli debug reload命令。

（4）psync

从节点使用psync从master node进行复制，psync runid offset。 master node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制。

### 9.强制读写分离

基于主从复制架构，实现读写分离。

redis slave node只读，默认开启，slave-read-only。

开启了只读的redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构。

### 10.redis压测

redis自己提供的redis-benchmark压测工具，是最快捷最方便的。

QPS的两个杀手：一个是复杂操作，lrange，挺多的; value很大，2 byte，我之前用redis做大规模的缓存。

做商品详情页的cache，可能是需要把大串数据，拼接在一起，作为一个json串，大小可能都几k，几个byte。

### 11.redis不可用是什么？单实例不可用？主从架构不可用？不可用的后果是什么？

缓存不可用，高并发高性能的缓存不可用了，大量的流量，超过mysql最大承载能力的大并发，大流量，会涌入mysql中，导致mysql宕机。整个系统不可用。

如果master node死掉了会怎么样？

没法写数据了，写缓存的时候全部失效了，slave node没有master给他们复制数据，系统相当于不可用了。

如果slave node死掉了会怎么样？

一个slave挂掉了，是不会影响可用性的，还有其他的slave在提供相同数据下的相同的对外的查询服务。

### 12.redis的集群架构

redis cluster。

支撑N个redis master node，每个master node都可以挂载多个slave node。

读写分离的架构，对于每个master来说，写就写到master，然后读就从mater对应的slave去读。

高可用，因为每个master都有salve节点，那么如果mater挂掉，redis cluster这套机制，就会自动将某个slave切换成master。

redis cluster（多master + 读写分离 + 高可用）。

我们只要基于redis cluster去搭建redis集群即可，不需要手工去搭建replication复制+主从架构+读写分离+哨兵集群+高可用。

### 13.redis cluster vs replication + sentinal

如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个G，单机足够了。

replication，一个mater，多个slave，要几个slave跟你的要求的读吞吐量有关系，然后自己搭建一个sentinal集群，去保证redis主从架构的高可用性，就可以了。

redis cluster，主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用redis cluster。

### 14.redis单master架构的容量的瓶颈问题

缓存清理的算法，将旧的很少使用的数据，给清除出内存，然后保证内存中，就只有固定大小的内存，不可能超过master内存的物理上限的。

master节点的数据和slave节点的数据是一模一样的。

master最大能容纳多大的数据量，那么slave也就只能容纳多大的数据量。

### 15.redis如何通过master横向扩容支撑1T+数据量

如果你要支撑更大的数据量的缓存，那就横向扩容更多的master节点，比如单台服务器是32G，那么30台左右，就差不多可以达到1T.

### 16.redis cluster介绍

redis cluster。

（1）自动将数据进行分片，每个master上放一部分数据。 （2）提供内置的高可用支持，部分master不可用时，还是可以继续工作的。

在redis cluster架构下，每个redis要放开两个端口号，比如一个是6379，另外一个就是加10000的端口号，比如16379。

16379端口号是用来进行节点间通信的，也就是cluster bus的东西，集群总线。cluster bus的通信，用来进行故障检测，配置更新，故障转移授权。

cluster bus用了另外一种二进制的协议，主要用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。

### 17.最老土的hash算法和弊端（大量缓存重建）

来了一个key之后，计算hash值，然后对节点数量（3）取模，取模结果一定在0~2之间，小于节点数量。

最大的问题是，只要任意一个master宕机，所有请求过来，都会基于最新的2个master去取模，尝试去获取数据，导致几乎大部分的请求，全部无法拿到有效的缓存，大量的流量会涌入数据库中，导致数据库被压垮。

### 18.一致性hash算法

一个key过来，同样也会计算hash值，然后会用hash值在圆环对应的各个点上（每个点都有一个hash值）去对比，看hash值应该落在这个圆环的哪个部位。

key落在圆环上以后，就会顺时针旋转去寻找距离自己最近的一个节点。

一致性hash算法，保证任何一个master宕机，只有之前那个master上的数据，会受影响，因为照着顺时针走，找不到先前那个master，会顺时针走到下一个master去。

问题

可能集中在某个hash区间内的值特别多，那么会导致大量的数据都涌入同一个master内，造成master的热点问题，性能出现瓶颈。

### 19.一致性hash算法的虚拟节点实现负载均衡

给每个master都做均匀分布的虚拟节点。

这样的话，在每个区间内，都会均匀分布到不同的节点内，而不是按照顺时针的顺序去走，全部涌入同一个master内。

### 20.redis cluster的重要配置

cluster-enabled <yes/no>

cluster-config-file <filename>：这是指定一个文件，供cluster模式下的redis实例将集群状态保存在那里，包括集群中其他机器的信息，比如节点的上线和下限，故障转移，不是我们去维护的，给它指定一个文件，让redis自己去维护的。

cluster-node-timeout <milliseconds>：节点存活超时时长，超过一定时长，认为节点宕机，master宕机的话就会触发主备切换，slave宕机就不会提供服务。

### 21.多master写入

redis cluster，提供了多个master，数据可以分布式存储在多个master上; 每个master都带着slave，自动就做读写分离; 每个master如果故障，那么就会自动将slave切换成master，高可用。

你在redis cluster写入数据的时候，其实是你可以将请求发送到任意一个master上去执行。

但是，每个master都会计算这个key对应的CRC16值，然后对16384个hashslot取模，找到key对应的hashslot，找到hashslot对应的master。

如果对应的master就在自己本地的话，set mykey1 v1，mykey1这个key对应的hashslot就在自己本地，那么自己就处理掉了。

但是如果计算出来的hashslot在其他master上，那么就会给客户端返回一个moved error，告诉你，你得到哪个master上去执行这条写入的命令。

什么叫做多master的写入，就是每条数据只能存在于一个master上，不同的master负责存储不同的数据，分布式的数据存储。

100w条数据，5个master，每个master就负责存储20w条数据，分布式数据存储。

大型的java系统架构，还专注在大数据系统架构，分布式，分布式存储，hadoop hdfs，分布式资源调度，hadoop yarn，分布式计算，hadoop mapreduce/hive。

分布式的nosql数据库，hbase，分布式的协调，zookeeper，分布式通用计算引擎，spark，分布式的实时计算引擎，storm。

如果你要处理海量数据，就涉及到了一个名词，叫做大数据，只要涉及到大数据，那么其实就会涉及到分布式。

jedis cluster api，就可以自动针对多个master进行写入和读取

### 22.slave的自动迁移

比如现在有10个master，每个有1个slave，然后新增了3个slave作为冗余，有的master就有2个slave了，有的master出现了salve冗余。

如果某个master的slave挂了，那么redis cluster会自动迁移一个冗余的slave给那个master。

只要多加一些冗余的slave就可以了。

为了避免的场景，就是说，如果你每个master只有一个slave，万一说一个slave死了，然后很快，master也死了，那可用性还是降低了。

但是如果你给整个集群挂载了一些冗余slave，那么某个master的slave死了，冗余的slave会被自动迁移过去，作为master的新slave，此时即使那个master也死了。

还是有一个slave会切换成master的。

### 23.节点间的内部通信机制

基础通信原理

（1）redis cluster节点间采取gossip协议进行通信

跟集中式不同，不是将集群元数据（节点信息，故障，等等）集中存储在某个节点上，而是互相之间不断通信，保持整个集群所有节点的数据是完整的。

维护集群的元数据用得，集中式，一种叫做gossip。

集中式：好处在于，元数据的更新和读取，时效性非常好，一旦元数据出现了变更，立即就更新到集中式的存储中，其他节点读取的时候立即就可以感知到; 不好在于，所有的元数据的跟新压力全部集中在一个地方，可能会导致元数据的存储有压力。

gossip：好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，有一定的延时，降低了压力; 缺点，元数据更新有延时，可能导致集群的一些操作会有一些滞后。

我们刚才做reshard，去做另外一个操作，会发现说，configuration error，达成一致。

（2）10000端口

每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如7001，那么用于节点间通信的就是17001端口。

每隔节点每隔一段时间都会往另外几个节点发送ping消息，同时其他几点接收到ping之后返回pong。

（3）交换的信息

故障信息，节点的增加和移除，hash slot信息，等等。

gossip协议

gossip协议包含多种消息，包括ping，pong，meet，fail，等等。

meet: 某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信。

redis-trib.rb add-node

其实内部就是发送了一个gossip meet消息，给新加入的节点，通知那个节点去加入我们的集群。

ping: 每个节点都会频繁给其他节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据。

每个节点每秒都会频繁发送ping给其他的集群，ping，频繁的互相之间交换数据，互相进行元数据的更新。

pong: 返回ping和meet，包含自己的状态和其他信息，也可以用于信息广播和更新。

fail: 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了。

ping消息深入

ping很频繁，而且要携带一些元数据，所以可能会加重网络负担。

每个节点每秒会执行10次ping，每次会选择5个最久没有通信的其他节点。

当然如果发现某个节点通信延时达到了cluster_node_timeout / 2，那么立即发送ping，避免数据交换延时过长，落后的时间太长了。

比如说，两个节点之间都10分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。

所以cluster_node_timeout可以调节，如果调节比较大，那么会降低发送的频率。

每次ping，一个是带上自己节点的信息，还有就是带上1/10其他节点的信息，发送出去，进行数据交换。

至少包含3个其他节点的信息，最多包含总节点-2个其他节点的信息。

### 25.gossip协议，小道留言

所有节点都持有一份元数据，不同的节点如果出现了元数据的变更之后，就不断将元数据发送给其他节点，让其他节点也进行元数据的变更。

### 26.基于重定向的客户端

redis-cli -c，自动重定向。

（1）请求重定向

客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot。

如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向。

cluster keyslot mykey，可以查看一个key对应的hash slot是什么。

用redis-cli的时候，可以加入-c参数，支持自动的请求重定向，redis-cli接收到moved之后，会自动重定向到对应的节点执行命令。

（2）计算hash slot

计算hash slot的算法，就是根据key计算CRC16值，然后对16384取模，拿到对应的hash slot。

用hash tag可以手动指定key对应的slot，同一个hash tag下的key，都会在一个hash slot中，比如set mykey1:{100}和set mykey2:{100}。

（3）hash slot查找

节点间通过gossip协议进行数据交换，就知道每个hash slot在哪个节点上。

### 27.smart jedis

（1）什么是smart jedis

基于重定向的客户端，很消耗网络IO，因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点。

所以大部分的客户端，比如java redis客户端，就是jedis，都是smart的。

本地维护一份hashslot -> node的映射表，缓存，大部分情况下，直接走本地缓存就可以找到hashslot -> node，不需要通过节点进行moved重定向。

（2）JedisCluster的工作原理

在JedisCluster初始化的时候，就会随机选择一个node，初始化hashslot -> node映射表，同时为每个节点创建一个JedisPool连接池。

每次基于JedisCluster执行操作，首先JedisCluster都会在本地计算key的hashslot，然后在本地映射表找到对应的节点。

如果那个node正好还是持有那个hashslot，那么就ok; 如果说进行了reshard这样的操作，可能hashslot已经不在那个node上了，就会返回moved。

如果JedisCluter API发现对应的节点返回moved，那么利用该节点的元数据，更新本地的hashslot -> node映射表缓存。

重复上面几个步骤，直到找到对应的节点，如果重试超过5次，那就报错JedisClusterMaxRedirectionException。

jedis老版本，可能会出现在集群某个节点故障还没完成自动切换恢复时，频繁更新hash slot，频繁ping节点检查活跃，导致大量网络IO开销。

jedis最新版本，对于这些过度的hash slot更新和ping，都进行了优化，避免了类似问题。

（3）hashslot迁移和ask重定向

如果hash slot正在迁移，那么会返回ask重定向给jedis。

jedis接收到ask重定向之后，会重新定位到目标节点去执行，但是因为ask发生在hash slot迁移过程中，所以JedisCluster API收到ask是不会更新hashslot本地缓存。

已经可以确定说，hashslot已经迁移完了，moved是会更新本地hashslot->node映射表缓存的。

### 28.redis cluster的高可用

redis cluster的高可用的原理，几乎跟哨兵是类似的。

1、判断节点宕机

如果一个节点认为另外一个节点宕机，那么就是pfail，主观宕机。

如果多个节点都认为另外一个节点宕机了，那么就是fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown

在cluster-node-timeout内，某个节点一直没有返回pong，那么就被认为pfail。

如果一个节点认为某个节点pfail了，那么会在gossip ping消息中，ping给其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail。

2、从节点过滤

对宕机的master node，从其所有的slave node中，选择一个切换成master node。

检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master。

这个也是跟哨兵是一样的，从节点超时过滤的步骤。

3、从节点选举

哨兵：对所有从节点进行排序，slave priority，offset，run id。

每个从节点，都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成master。

从节点执行主备切换，从节点切换为主节点。

4、与哨兵比较

整个流程跟哨兵相比，非常类似，所以说，redis cluster功能强大，直接集成了replication和sentinal的功能。

## 哨兵

### 1.Redis 哨兵集群实现高可用

哨兵的介绍

sentinel，中文名是哨兵。哨兵是 Redis 集群架构中非常重要的一个组件，主要有以下功能：

- 集群监控：负责监控 Redis master 和 slave 进程是否正常工作。
- 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。
- 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。
- 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。

哨兵用于实现 Redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。

- 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。
- 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。

哨兵的核心知识

- 哨兵至少需要 3 个实例，来保证自己的健壮性。
- 哨兵 + Redis 主从的部署架构，是**不保证数据零丢失**的，只能保证 Redis 集群的高可用性。
- 对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。

哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。

```html
+----+         +----+
| M1 |---------| R1 |
| S1 |         | S2 |
+----+         +----+
```

配置 `quorum=1` ，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。

```html
2 个哨兵，majority=2
3 个哨兵，majority=2
4 个哨兵，majority=2
5 个哨兵，majority=3
...
```

如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。

经典的 3 节点哨兵集群是这样的：

```html
       +----+
       | M1 |
       | S1 |
       +----+
          |
+----+    |    +----+
| R2 |----+----| R3 |
| S2 |         | S3 |
+----+         +----+
```

配置 `quorum=2` ，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。

### 2.Redis 哨兵主备切换的数据丢失问题

导致数据丢失的两种情况

主备切换的过程，可能会导致数据丢失：

- 异步复制导致的数据丢失

因为 master->slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。

- 脑裂导致的数据丢失

脑裂，也就是说，某个 master 所在机器突然**脱离了正常的网络**，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会**认为** master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的**脑裂**。

此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。

数据丢失问题的解决方案

进行如下配置：

```html
min-slaves-to-write 1
min-slaves-max-lag 10
```

表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。

如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。

- 减少异步复制数据的丢失

有了 `min-slaves-max-lag` 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。

- 减少脑裂的数据丢失

如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。

### 3.sdown 和 odown 转换机制

- sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机.
- odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机.

sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 `is-master-down-after-milliseconds` 指定的毫秒数之后，就主观认为 master 宕机了；如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。

### 4.哨兵集群的自动发现机制

哨兵互相之间的发现，是通过 Redis 的 `pub/sub` 系统实现的，每个哨兵都会往 `__sentinel__:hello` 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。

每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 `__sentinel__:hello` channel 里**发送一个消息**，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。

每个哨兵也会去**监听**自己监控的每个 master+slaves 对应的 `__sentinel__:hello` channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。

每个哨兵还会跟其他哨兵交换对 `master` 的监控配置，互相进行监控配置的同步。

slave 配置的自动纠正

哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。

### 5.slave->master 选举算法

如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息：

- 跟 master 断开连接的时长
- slave 优先级
- 复制 offset
- run id

如果一个 slave 跟 master 断开连接的时间已经超过了 `down-after-milliseconds` 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。

```html
(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state
```

接下来会对 slave 进行排序：

- 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。
- 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。
- 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。

### 6.主备切换

quorum 和 majority

每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。

如果 quorum < majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。

但是如果 quorum >= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。

configuration epoch

哨兵会对一套 Redis master+slaves 进行监控，有相应的监控的配置。

执行切换的那个哨兵，会从要切换到的新 master（salve->master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。

如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。

configuration 传播

哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 `pub/sub` 消息机制。

这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。

### 7.哨兵的配置文件

sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1

上面的三个配置，都是针对某个监控的master配置的，给其指定上面分配的名称即可。

这是最小的哨兵配置，如果发生了master-slave故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件

sentinel monitor master-group-name hostname port quorum

quorum的解释如下：

（1）至少多少个哨兵要一致同意，master进程挂掉了，或者slave进程挂掉了，或者要启动一个故障转移操作。 （2）quorum是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作。 （3）假设有5个哨兵，quorum设置了2，那么如果5个哨兵中的2个都认为master挂掉了; 2个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移; 如果5个哨兵中有3个哨兵都是运行的，那么故障转移就会被允许执行。

down-after-milliseconds，超过多少毫秒跟一个redis实例断了连接，哨兵就可能认为这个redis实例挂了。

parallel-syncs，新的master别切换之后，同时有多少个slave被切换到去连接新master，重新做同步，数字越低，花费的时间越多。

假设你的redis是1个master，4个slave。

然后master宕机了，4个slave中有1个切换成了master，剩下3个slave就要挂到新的master上面去。

这个时候，如果parallel-syncs是1，那么3个slave，一个一个地挂接到新的master上面去，1个挂接完，而且从新的master sync完数据之后，再挂接下一个。

如果parallel-syncs是3，那么一次性就会把所有slave挂接到新的master上去。

failover-timeout，执行故障转移的timeout超时时长。

### 8.哨兵节点的增加和删除

增加sentinal，会自动发现。

删除sentinal的步骤。

（1）停止sentinal进程。 （2）SENTINEL RESET *，在所有sentinal上执行，清理所有的master状态。 （3）SENTINEL MASTER mastername，在所有sentinal上执行，查看所有sentinal对数量是否达成了一致。

### 9.slave的永久下线

让master摘除某个已经下线的slave：SENTINEL RESET mastername，在所有的哨兵上面执行。

### 10.slave切换为Master的优先级

slave->master选举优先级：slave-priority，值越小优先级越高。

### 11.基于哨兵集群架构下的安全认证

每个slave都有可能切换成master，所以每个实例都要配置两个指令。

master上启用安全认证，requirepass master连接口令，masterauth

sentinal，sentinel auth-pass <master-group-name> <pass>。

## 优化

### 1.redis在实践中的一些常见问题以及优化思路

1、fork耗时导致高并发请求延时

RDB和AOF的时候，其实会有生成RDB快照，AOF rewrite，耗费磁盘IO的过程，主进程fork子进程。

fork的时候，子进程是需要拷贝父进程的空间内存页表的，也是会耗费一定的时间的。

一般来说，如果父进程内存有1个G的数据，那么fork可能会耗费在20ms左右，如果是10G~30G，那么就会耗费20 * 10，甚至20 * 30，也就是几百毫秒的时间。

info stats中的latest_fork_usec，可以看到最近一次form的时长。

redis单机QPS一般在几万，fork可能一下子就会拖慢几万条操作的请求时长，从几毫秒变成1秒。

优化思路

fork耗时跟redis主进程的内存有关系，一般控制redis的内存在10GB以内，slave -> master，全量复制。

2.AOF的阻塞问题

redis将数据写入AOF缓冲区，单独开一个现场做fsync操作，每秒一次。

但是redis主线程会检查两次fsync的时间，如果距离上次fsync时间超过了2秒，那么写请求就会阻塞。

everysec，最多丢失2秒的数据。

一旦fsync超过2秒的延时，整个redis就被拖慢。

优化思路

优化硬盘写入速度，建议采用SSD，不要用普通的机械硬盘，SSD，大幅度提升磁盘读写的速度。

3.主从复制延迟问题

主从复制可能会超时严重，这个时候需要良好的监控和报警机制。

在info replication中，可以看到master和slave复制的offset，做一个差值就可以看到对应的延迟量。

如果延迟过多，那么就进行报警。

4、主从复制风暴问题

如果一下子让多个slave从master去执行全量复制，一份大的rdb同时发送到多个slave，会导致网络带宽被严重占用。

如果一个master真的要挂载多个slave，那尽量用树状结构，不要用星型结构。

5、vm.overcommit_memory

0: 检查有没有足够内存，没有的话申请内存失败。 1: 允许使用内存直到用完为止。 2: 内存地址空间不能超过swap + 50%。

如果是0的话，可能导致类似fork等操作执行失败，申请不到足够的内存空间。

cat /proc/sys/vm/overcommit_memory echo "vm.overcommit_memory=1" >> /etc/sysctl.conf sysctl vm.overcommit_memory=1

6、swapiness

cat /proc/version，查看linux内核版本。

如果linux内核版本<3.5，那么swapiness设置为0，这样系统宁愿swap也不会oom killer（杀掉进程）。 如果linux内核版本>=3.5，那么swapiness设置为1，这样系统宁愿swap也不会oom killer。

保证redis不会被杀掉。

echo 0 > /proc/sys/vm/swappiness echo vm.swapiness=0 >> /etc/sysctl.conf

7、最大打开文件句柄

ulimit -n 10032 10032

自己去上网搜一下，不同的操作系统，版本，设置的方式都不太一样。

8、tcp backlog

cat /proc/sys/net/core/somaxconn echo 511 > /proc/sys/net/core/somaxconn

### 2.Redis 内存满了怎么办？

通过在Redis安装⽬录下⾯的redis.conf配置⽂件中添加以下配置设置内存⼤⼩

```html
//设置Redis最⼤占⽤内存⼤⼩为100M
maxmemory 100mb
```

Redis可使⽤最⼤内存使⽤完了，是可以使⽤LRU算法进⾏内存淘汰的，那么什么是LRU算法呢？

LRU(Least Recently Used)，即最近最少使⽤，是⼀种缓存置换算法。在使⽤内存作为缓存的时候，缓存的⼤⼩⼀般是固 定的。当缓存被占满，这个时候继续往缓存⾥⾯添加数据，就需要淘汰⼀部分⽼的数据，释放内存空间⽤来存储新的数 据。这个时候就可以使⽤LRU算法了。其核⼼思想是：如果⼀个数据在最近⼀段时间没有被⽤到，那么将来被使⽤到的可能 性也很⼩，所以就可以被淘汰掉。

Redis使⽤的是近似LRU算法，它跟常规的LRU算法还不太⼀样。近似LRU算法通过随机采样法淘汰数据，每次随机出5（默认） 个key，从⾥⾯淘汰掉最近最少使⽤的key。

Redis为了实现近似LRU算法，给每个key增加了⼀个额外增加了⼀个24bit的字段，⽤来存储该key最后⼀次被访问的时间。

Redis3.0对近似LRU的优化

Redis3.0对近似LRU算法进⾏了⼀些优化。新算法会维护⼀个候选池（⼤⼩为16），池中的数据根据访问时间进⾏排序，第⼀次 随机选取的key都会放⼊池中，随后每次随机选取的key只有在访问时间⼩于池中最⼩的时间才会放⼊池中，直到候选池被放 满。当放满后，如果有新的key需要放⼊，则将池中最后访问时间最⼤（最近被访问）的移除。 当需要淘汰的时候，则直接从池中选取最近访问时间最⼩（最久没被访问）的key淘汰掉就⾏。

LFU算法

LFU算法是Redis4.0⾥⾯新加的⼀种淘汰策略。它的全称是Least Frequently Used，它的核⼼思想是根据key的最近被访问的 频率进⾏淘汰，很少被访问的优先被淘汰，被访问的多的则被留下来。 LFU算法能更好的表⽰⼀个key被访问的热度。假如你使⽤的是LRU算法，⼀个key很久没有被访问到，只刚刚是偶尔被访问了⼀ 次，那么它就被认为是热点数据，不会被淘汰，⽽有些key将来是很有可能被访问到的则被淘汰了。如果使⽤LFU算法则不会出现 这种情况，因为使⽤⼀次并不会使⼀个key成为热点数据。 LFU⼀共有两种策略： volatile-lfu：在设置了过期时间的key中使⽤LFU算法淘汰key allkeys-lfu：在所有的key中使⽤LFU算法淘汰数据

## 多级缓存架构

### 1.多级缓存架构

三级缓存：nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构。

时效性要求非常高的数据：库存。

一般来说，显示的库存，都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化。

当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去。

时效性要求不高的数据：商品的基本信息（名称、颜色、版本、规格参数，等等）。

时效性要求不高的数据，就还好，比如说你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也还能接受。

商品价格/库存等时效性要求高的数据，而且种类较少，采取相关的服务系统每次发生了变更的时候，直接采取数据库和redis缓存双写的方案，这样缓存的时效性最高。

商品基本信息等时效性不高的数据，而且种类繁多，来自多种不同的系统，采取MQ异步通知的方式，写一个数据生产服务，监听MQ消息，然后异步拉取服务的数据，更新tomcat jvm缓存+redis缓存。

nginx+lua脚本做页面动态生成的工作，每次请求过来，优先从nginx本地缓存中提取各种数据，结合页面模板，生成需要的页面。

如果nginx本地缓存过期了，那么就从nginx到redis中去拉取数据，更新到nginx本地。

如果redis中也被LRU算法清理掉了，那么就从nginx走http接口到后端的服务中拉取数据，数据生产服务中，先在本地tomcat里的jvm堆缓存中找，ehcache，如果也被LRU清理掉了，那么就重新发送请求到源头的服务中去拉取数据，然后再次更新tomcat堆内存缓存+redis缓存，并返回数据给nginx，nginx缓存到本地。

### 2.多级缓存架构中每一层的意义

nginx本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买iphone、nike、海尔等知名品牌的东西的人，总是比较多的。

这些热数据，利用nginx本地缓存，由于经常被访问，所以可以被锁定在nginx的本地缓存内。

大量的热数据的访问，就是经常会访问的那些数据，就会被保留在nginx本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以了。

那么大量的访问，直接就可以走到nginx就行了，不需要走后续的各种网络开销了。

redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务。

redis缓存最大量的数据，最完整的数据和缓存，1T+数据; 支撑高并发的访问，QPS最高到几十万; 可用性，非常好，提供非常稳定的服务。

nginx本地内存有限，也就能cache住部分热数据，除了各种iphone、nike等热数据，其他相对不那么热的数据，可能流量会经常走到redis那里。

利用redis cluster的多master写入，横向扩容，1T+以上海量数据支持，几十万的读写QPS，99.99%高可用性，那么就可以抗住大量的离散访问请求。

tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔。

同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存。

### 3.最经典的缓存+数据库读写的模式，cache aside pattern

（1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应。

（2）更新的时候，先删除缓存，然后再更新数据库。

### 4.为什么是删除缓存，而不是更新缓存呢？

原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值。

商品详情页的系统，修改库存，只是修改了某个表的某些字段，但是要真正把这个影响的最终的库存计算出来，可能还需要从其他表查询一些数据，然后进行一些复杂的运算，才能最终计算出。

现在最新的库存是多少，然后才能将库存更新到缓存中去。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的。

更新缓存的代价是很高的。

是不是说，每次修改数据库的时候，都一定要将其对应的缓存去更新一份？也许有的场景是这样的，但是对于比较复杂的缓存数据计算的场景，就不是这样了。

如果你频繁修改一个缓存涉及的多个表，那么这个缓存会被频繁的更新，频繁的更新缓存。

但是问题在于，这个缓存到底会不会被频繁访问到？？？

举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存跟新20次，100次; 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据。

28法则，黄金法则，20%的数据，占用了80%的访问量。

实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。

每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存。

其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。

mybatis，hibernate，懒加载，思想。

查询一个部门，部门带了一个员工的list，没有必要说每次查询部门，都里面的1000个员工的数据也同时查出来啊。

80%的情况，查这个部门，就只是要访问这个部门的信息就可以了。

先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询1000个员工。

### 5.最初级的缓存不一致问题以及解决方案

问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致。

解决思路

先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。

因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。

### 6.高并发下的数据不一致问题分析

数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。

一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。

数据变更的程序完成了数据库的修改。

完了，数据库和缓存中的数据不一样了。。。。

只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。

其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。

但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。

### 7.数据库与缓存更新与读取操作进行异步串行化

更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中。

读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中。

一个队列对应一个工作线程。

每个工作线程串行拿到对应的操作，然后一条一条的执行。

这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新。

此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。

待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。

如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。

### 8.高并发的场景下，该解决方案要注意的问题

（1）读请求长时阻塞

由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。

该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。

务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的。

另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。

如果一个内存队列里居然会挤压100个商品的库存修改操作，每个库存修改操作要耗费10ms区完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据。

这个时候就导致读请求的长时阻塞。

一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会hang多少时间，如果读请求在200ms返回，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以的。

如果一个内存队列可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。

其实根据之前的项目经验，一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。

针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了。

一秒，500的写操作，5份，每200ms，就100个写操作。

单机器，20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成。

那么针对每个内存队列中的数据的读请求，也就最多hang一会儿，200ms以内肯定能返回了。

写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列。

大部分的情况下，应该是这样的，大量的读请求过来，都是直接走缓存取到数据的。

少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面。

等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据。

（2）读请求并发量过高

这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值。

但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。

按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作。

如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存。

一般来说，1:1，1:2，1:3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回。

在同一时间最多hang住的可能也就是单机200个读请求，同时hang住。

单机hang200个读请求，还是ok的。

1:20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万。

1万个读请求全部hang在库存服务上，就死定了。

（3）多服务实例部署的请求路由

可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上。

（4）热点商品的路由问题，导致请求的倾斜

万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大。

就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大。

但是的确可能某些机器的负载会高一些。

### 9.在库存服务中实现缓存与数据库双写一致性保障方案

1、线程池+内存队列初始化

```html
@Bean
public ServletListenerRegistrationBean servletListenerRegistrationBean(){
    ServletListenerRegistrationBean servletListenerRegistrationBean = new ServletListenerRegistrationBean();
    servletListenerRegistrationBean.setListener(new InitListener());
    return servletListenerRegistrationBean;
}
```

java web应用，做系统的初始化，一般在哪里做呢？

ServletContextListener里面做，listener，会跟着整个web应用的启动，就初始化，类似于线程池初始化的构建。

spring boot应用，Application，搞一个listener的注册。

2、两种请求对象封装。

3、请求异步执行Service封装。

4、请求处理的工作线程封装。

5、两种请求Controller接口封装。

6、读请求去重优化。

如果一个读请求过来，发现前面已经有一个写请求和一个读请求了，那么这个读请求就不需要压入队列中了。

因为那个写请求肯定会更新数据库，然后那个读请求肯定会从数据库中读取最新数据，然后刷新到缓存中，自己只要hang一会儿就可以从缓存中读到数据了。

7、空数据读请求过滤优化

可能某个数据，在数据库里面压根儿就没有，那么那个读请求是不需要放入内存队列的，而且读请求在controller那一层，直接就可以返回了，不需要等待。

如果数据库里都没有，就说明，内存队列里面如果没有数据库更新的请求的话，一个读请求过来了，就可以认为是数据库里就压根儿没有数据吧。

如果缓存里没数据，就两个情况，第一个是数据库里就没数据，缓存肯定也没数据; 第二个是数据库更新操作过来了，先删除了缓存，此时缓存是空的，但是数据库里是有的。

但是的话呢，我们做了之前的读请求去重优化，用了一个flag map，只要前面有数据库更新操作，flag就肯定是存在的，你只不过可以根据true或false，判断你前面执行的是写请求还是读请求。

但是如果flag压根儿就没有呢，就说明这个数据，无论是写请求，还是读请求，都没有过。

那这个时候过来的读请求，发现flag是null，就可以认为数据库里肯定也是空的，那就不会去读取了。

或者说，我们也可以认为每个商品有一个最最初始的库存，但是因为最初始的库存肯定会同步到缓存中去的，有一种特殊的情况，就是说，商品库存本来在redis中是有缓存的。

但是因为redis内存满了，就给干掉了，但是此时数据库中是有值得。

那么在这种情况下，可能就是之前没有任何的写请求和读请求的flag的值，此时还是需要从数据库中重新加载一次数据到缓存中的。

8、深入的去思考优化代码的漏洞

一个读请求过来，将数据库中的数刷新到了缓存中，flag是false，然后过了一会儿，redis内存满了，自动删除了这个额缓存。

下一次读请求再过来，发现flag是false，就不会去执行刷新缓存的操作了。

而是hang在哪里，反复循环，等一会儿，发现在缓存中始终查询不到数据，然后就去数据库里查询，就直接返回了。

这种代码，就有可能会导致，缓存永远变成null的情况。

最简单的一种，就是在controller这一块，如果在数据库中查询到了，就刷新到缓存里面去，以后的读请求就又可以从缓存里面读了。

### 10.缓存的技术方案，分成两块

第一块，是做实时性比较高的那块数据，比如说库存，销量之类的这种数据，我们采取的实时的缓存+数据库双写的技术方案，双写一致性保障的方案。

第二块，是做实时性要求不高的数据，比如说商品的基本信息，等等，我们采取的是三级缓存架构的技术方案，就是说由一个专门的数据生产的服务，去获取整个商品详情页需要的各种数据，经过处理后，将数据放入各级缓存中，每一级缓存都有自己的作用。

### 11.缓存维度化解决方案

维度：商品维度，商品分类维度，商品店铺维度。

不同的维度，可以看做是不同的角度去观察一个东西，那么每个商品详情页中，都包含了不同的维度数据。

我就跟大家举个例子，如果不维度化，就导致多个维度的数据混合在一个缓存value中。

但是不同维度的数据，可能更新的频率都大不一样。

比如说，现在只是将1000个商品的分类批量调整了一下，但是如果商品分类的数据和商品本身的数据混杂在一起。

那么可能导致需要将包括商品在内的大缓存value取出来，进行更新，再写回去，就会很坑爹，耗费大量的资源，redis压力也很大。

但是如果我们对缓存进行维度化。

维度化：将每个维度的数据都存一份，比如说商品维度的数据存一份，商品分类的数据存一份，商品店铺的数据存一份。

那么在不同的维度数据更新的时候，只要去更新对应的维度就可以了。

### 12.商品详情页缓存数据生产服务的工作流程分析

（1）监听多个kafka topic，每个kafka topic对应一个服务（简化一下，监听一个kafka topic）。 （2）如果一个服务发生了数据变更，那么就发送一个消息到kafka topic中。 （3）缓存数据生产服务监听到了消息以后，就发送请求到对应的服务中调用接口以及拉取数据，此时是从mysql中查询的。 （4）缓存数据生产服务拉取到了数据之后，会将数据在本地缓存中写入一份，就是ehcache中。 （5）同时会将数据在redis中写入一份。

### 13.本地堆缓存

服务本地堆缓存，作用，预防redis层的彻底崩溃，作为缓存的最后一道防线，避免数据库直接裸奔。

服务本地堆缓存，我们用什么来做缓存，难道我们自己手写一个类或者程序去管理内存吗？？？java最流行的缓存的框架，ehcache。

所以我们也是用ehcache来做本地的堆缓存。

### 14.redis的LRU缓存清除算法

1、LRU算法概述

redis默认情况下就是使用LRU策略的，因为内存是有限的，但是如果你不断地往redis里面写入数据，那肯定是没法存放下所有的数据在内存的。

所以redis默认情况下，当内存中写入的数据很满之后，就会使用LRU算法清理掉部分内存中的数据，腾出一些空间来，然后让新的数据写入redis缓存中。

LRU：Least Recently Used，最近最少使用算法。

将最近一段时间内，最少使用的一些数据，给干掉。比如说有一个key，在最近1个小时内，只被访问了一次; 还有一个key在最近1个小时内，被访问了1万次。

这个时候比如你要将部分数据给清理掉，你会选择清理哪些数据啊？肯定是那个在最近小时内被访问了1次的数据。

2、缓存清理设置

redis.conf

maxmemory，设置redis用来存放数据的最大的内存大小，一旦超出这个内存大小之后，就会立即使用LRU算法清理掉部分数据。

如果用LRU，那么就是将最近最少使用的数据从缓存中清除出去。

对于64 bit的机器，如果maxmemory设置为0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止; 但是对于32 bit的机器，有一个隐式的闲置就是3GB。

maxmemory-policy，可以设置内存达到最大闲置后，采取什么策略来处理。

（1）noeviction: 如果内存使用达到了maxmemory，client还要继续写入数据，那么就直接报错给客户端。 （2）allkeys-lru: 就是我们常说的LRU算法，移除掉最近最少使用的那些keys对应的数据。 （3）volatile-lru: 也是采取LRU算法，但是仅仅针对那些设置了指定存活时间（TTL）的key才会清理掉。 （4）allkeys-random: 随机选择一些key来删除掉。 （5）volatile-random: 随机选择一些设置了TTL的key来删除掉。 （6）volatile-ttl: 移除掉部分keys，选择那些TTL时间比较短的keys。

在redis里面，写入key-value对的时候，是可以设置TTL，存活时间，比如你设置了60s。那么一个key-value对，在60s之后就会自动被删除。

redis的使用，各种数据结构，list，set，等等。

allkeys-lru

这边拓展一下思路，对技术的研究，一旦将一些技术研究的比较透彻之后，就喜欢横向对比底层的一些原理。

storm，科普一下

玩儿大数据的人搞得，领域，实时计算领域，storm

storm有很多的流分组的一些策略，按shuffle分组，global全局分组，direct直接分组，fields按字段值hash后分组。

分组策略也很多，但是，真正公司里99%的场景下，使用的也就是shuffle和fields，两种策略。

redis，给了这么多种乱七八糟的缓存清理的算法，其实真正常用的可能也就那么一两种，allkeys-lru是最常用的。

3、缓存清理的流程

（1）客户端执行数据写入操作。 （2）redis server接收到写入操作之后，检查maxmemory的限制，如果超过了限制，那么就根据对应的policy清理掉部分数据。 （3）写入操作完成执行。

4、redis的LRU近似算法

科普一个相对来说稍微高级一丢丢的知识点。

redis采取的是LRU近似算法，也就是对keys进行采样，然后在采样结果中进行数据清理。。

redis 3.0开始，在LRU近似算法中引入了pool机制，表现可以跟真正的LRU算法相当，但是还是有所差距的，不过这样可以减少内存的消耗。

redis LRU算法，是采样之后再做LRU清理的，跟真正的、传统、全量的LRU算法是不太一样的。

maxmemory-samples，比如5，可以设置采样的大小，如果设置为10，那么效果会更好，不过也会耗费更多的CPU资源。

### 15.缓存命中率低

缓存数据生产服务那一层已经搞定了，相当于三层缓存架构中的本地堆缓存+redis分布式缓存都搞定了。

就要来做三级缓存中的nginx那一层的缓存了。

如果一般来说，你默认会部署多个nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的。

流量会被负载均衡，均匀的打到各个nginx服务器上，每个nginx都会发送一次请求到redis上去获取缓存数据，放到自己本地。

缓存命中率低，从而导致redis的压力暴增。

### 16.如何提升缓存命中率

分发层+应用层，双层nginx。

分发层nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则，比如根据productId去进行hash，然后对后端的nginx数量取模。

将某一个商品的访问的请求，就固定路由到一个nginx后端服务器上去，保证说只会从redis中获取一次缓存数据，后面全都是走nginx本地缓存了。

后端的nginx服务器，就称之为应用服务器; 最前端的nginx服务器，被称之为分发服务器。

看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的nginx本地缓存这一层的命中率，大幅度减少redis后端的压力，提升性能。

### 17.定向流量分发策略

1、获取请求参数，比如productId。 2、对productId进行hash。 3、hash值对应用服务器数量取模，获取到一个应用服务器。 4、利用http发送请求到应用层nginx。 5、获取响应后返回。

### 18.基于nginx+lua+java完成多级缓存架构

1、应用nginx的lua脚本接收到请求。

2、获取请求参数中的商品id，以及商品店铺id。

3、根据商品id和商品店铺id，在nginx本地缓存中尝试获取数据。

4、如果在nginx本地缓存中没有获取到数据，那么就到redis分布式缓存中获取数据，如果获取到了数据，还要设置到nginx本地缓存中。

建议不要用nginx+lua直接去获取redis数据。

建议是发送http请求到缓存数据生产服务，由该服务提供一个http接口，缓存数生产服务可以基于redis cluster api从redis中直接获取数据，并返回给nginx。

5、如果缓存数据生产服务没有在redis分布式缓存中没有获取到数据，那么就在自己本地ehcache中获取数据，返回数据给nginx，也要设置到nginx本地缓存中。

6、如果ehcache本地缓存都没有数据，那么就需要去原始的服务中拉去数据，该服务会从mysql中查询，拉去到数据之后，返回给nginx，并重新设置到ehcache和redis中。

7、nginx最终利用获取到的数据，动态渲染网页模板。

8、将渲染后的网页模板作为http响应，返回给分发层nginx。

### 19.分布式重建缓存的并发冲突问题

重建缓存：比如我们这里，数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存。

分布式的重建缓存，在不同的机器上，不同的服务实例中，去做上面的事情，就会出现多个机器分布式重建去读取相同的数据，然后写入缓存中。

1、流量均匀分布到所有缓存服务实例上。

应用层nginx，是将请求流量均匀地打到各个缓存服务实例中的，可能咱们的eshop-cache那个服务，可能会部署多实例在不同的机器上。

2、应用层nginx的hash，固定商品id，走固定的缓存服务实例。

分发层的nginx的lua脚本，是怎么写的，怎么玩儿的，搞一堆应用层nginx的地址列表，对每个商品id做一个hash，然后对应用nginx数量取模。

将每个商品的请求固定分发到同一个应用层nginx上面去。

在应用层nginx里，发现自己本地lua shared dict缓存中没有数据的时候，就采取一样的方式，对product id取模，然后将请求固定分发到同一个缓存服务实例中去。

这样的话，就不会出现说多个缓存服务实例分布式的去更新那个缓存了。

3、源信息服务发送的变更消息，需要按照商品id去分区，固定的商品变更走固定的kafka分区，也就是固定的一个缓存服务实例获取到。

缓存服务，是监听kafka topic的，一个缓存服务实例，作为一个kafka consumer，就消费topic中的一个partition。

所以你有多个缓存服务实例的话，每个缓存服务实例就消费一个kafka partition。

所以这里，一般来说，你的源头信息服务，在发送消息到kafka topic的时候，都需要按照product id去分区。

也就时说，同一个product id变更的消息一定是到同一个kafka partition中去的，也就是说同一个product id的变更消息，一定是同一个缓存服务实例消费到的。

4、自己写的简易的hash分发，与kafka的分区，可能并不一致！！！

我们自己写的简易的hash分发策略，是按照crc32去取hash值，然后再取模的。

关键你又不知道你的kafka producer的hash策略是什么，很可能说跟我们的策略是不一样的。

拿就可能导致说，数据变更的消息所到的缓存服务实例，跟我们的应用层nginx分发到的那个缓存服务实例也许就不在一台机器上了。

这样的话，在高并发，极端的情况下，可能就会出现冲突。

5.分布式的缓存重建并发冲突问题发生了。

6、基于zookeeper分布式锁的解决方案

分布式锁，如果你有多个机器在访问同一个共享资源，那么这个时候，如果你需要加个锁，让多个分布式的机器在访问共享资源的时候串行起来。

那么这个时候，那个锁，多个不同机器上的服务共享的锁，就是分布式锁。

分布式锁当然有很多种不同的实现方案，redis分布式锁，zookeeper分布式锁。

zk，做分布式协调这一块，还是很流行的，大数据应用里面，hadoop，storm，都是基于zk去做分布式协调。

zk分布式锁的解决并发冲突的方案。

（1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁。 （2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新。 （3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁。

### 20.zk分布式锁的原理

创建zk锁，会创建一个临时node，其他请求过来如果再要创建临时node，就会报错，NodeExistsException。

如果临时node创建成功了，那么说明我们成功加锁了，此时就可以去执行对redis立面数据的操作。

如果临时node创建失败了，说明有人已经在拿到锁了，在操作reids中的数据，那么就不断的等待，直到自己可以获取到锁为止。

释放一个分布式锁，去删除掉那个临时node就可以了，就代表释放了一个锁，那么此时其他的机器就可以成功创建临时node，获取到锁。

### 21.缓存冷启动问题

新系统第一次上线，redis中可能没有数据。

系统在线上稳定运行着，但是突然间重要的redis缓存全盘崩溃了，而且不幸的是，数据全都无法找回来。

导致瞬间大量的高并发的请求和流量打向mysql，导致mysql挂掉，系统不可用。

### 22.缓存预热

缓存冷启动，redis启动后，一点数据都没有，直接就对外提供服务了，mysql就裸奔。

（1）提前给redis中灌入部分数据，再提供服务。 （2）肯定不可能将所有数据都写入redis，因为数据量太大了，第一耗费的时间太长了，第二根本redis容纳不下所有的数据。。 （3）需要根据当天的具体访问情况，实时统计出访问频率较高的热数据。 （4）然后将访问频率较高的热数据写入redis中，肯定是热数据也比较多，我们也得多个服务并行读取数据去写，并行的分布式的缓存预热。 （5）然后将灌入了热数据的redis对外提供服务，这样就不至于冷启动，直接让数据库裸奔了。

### 23.缓存预热解决方案：基于storm实时热点统计的分布式并行缓存预热

1、nginx+lua将访问流量上报到kafka中

要统计出来当前最新的实时的热数据是哪些，我们就得将商品详情页访问的请求对应的流浪，日志，实时上报到kafka中。

在nginx这一层，接收到访问请求的时候，就把请求的流量上报发送给kafka。

2、storm从kafka中消费数据，实时统计出每个商品的访问次数，访问次数基于LRU内存数据结构的存储方案。

优先用内存中的一个LRUMap去存放，性能高，而且没有外部依赖。

我之前做过的一些项目，不光是这个项目，还有很多其他的，一些广告计费类的系统，storm。

否则的话，依赖redis，我们就是要防止redis挂掉数据丢失的情况，就不合适了; 用mysql，扛不住高并发读写; 用hbase，hadoop生态系统，维护麻烦，太重了。

其实我们只要统计出最近一段时间访问最频繁的商品，然后对它们进行访问计数，同时维护出一个前N个访问最多的商品list即可。

热数据，最近一段时间，可以拿到最近一段，比如最近1个小时，最近5分钟，1万个商品请求，统计出最近这段时间内每个商品的访问次数，排序，做出一个排名前N的list。

计算好每个task大致要存放的商品访问次数的数量，计算出大小。

然后构建一个LRUMap，apache commons collections有开源的实现，设定好map的最大大小，就会自动根据LRU算法去剔除多余的数据，保证内存使用限制。

即使有部分数据被干掉了，然后下次来重新开始计数，也没关系，因为如果它被LRU算法干掉，那么它就不是热数据，说明最近一段时间都很少访问了。

3、每个storm task启动的时候，基于zk分布式锁，将自己的id写入zk同一个节点中。

4、每个storm task负责完成自己这里的热数据的统计，每隔一段时间，就遍历一下这个map，然后维护一个前3个商品的list，更新这个list。

5、写一个后台线程，每隔一段时间，比如1分钟，都将排名前3的热数据list，同步到zk中去，存储到这个storm task对应的一个znode中去。

6、我们需要一个服务，比如说，代码可以跟缓存数据生产服务放一起，但是也可以放单独的服务。

服务可能部署了很多个实例。

每次服务启动的时候，就会去拿到一个storm task的列表，然后根据taskid，一个一个的去尝试获取taskid对应的znode的zk分布式锁。

如果能获取到分布式锁的话，那么就将那个storm task对应的热数据的list取出来。

然后将数据从mysql中查询出来，写入缓存中，进行缓存的预热，多个服务实例，分布式的并行的去做，基于zk分布式锁做了协调了，分布式并行缓存的预热。

### 24.缓存预热方案

1、服务启动的时候，进行缓存预热。

2、从zk中读取taskid列表。

3、依次遍历每个taskid，尝试获取分布式锁，如果获取不到，快速报错，不要等待，因为说明已经有其他服务实例在预热了。

4、直接尝试获取下一个taskid的分布式锁。

5、即使获取到了分布式锁，也要检查一下这个taskid的预热状态，如果已经被预热过了，就不再预热了。

6、执行预热操作，遍历productid列表，查询数据，然后写ehcache和redis。

7、预热完成后，设置taskid对应的预热状态。

### 25.基于nginx+lua+storm的热点缓存的流量分发策略自动降级解决方案

1、在storm中，实时的计算出瞬间出现的热点。

某个storm task，上面算出了1万个商品的访问次数，LRUMap。

频率高一些，每隔5秒，去遍历一次LRUMap，将其中的访问次数进行排序，统计出往后排的95%的商品访问次数的平均值。

比如说，95%的商品，访问次数的平均值是100。

然后，从最前面开始，往后遍历，去找有没有瞬间出现的热点数据。

1000，95%的平均值（100）的10倍，这个时候要设定一个阈值，比如说超出95%平均值得n倍，5倍。

我们就认为是瞬间出现的热点数据，判断其可能在短时间内继续扩大的访问量，甚至达到平均值几十倍，或者几百倍。

当遍历，发现说第一个商品的访问次数，小于平均值的5倍，就安全了，就break掉这个循环。

热点数据，热数据，不是一个概念。

有100个商品，前10个商品比较热，都访问量在500左右，其他的普通商品，访问量都在200左右，就说前10个商品是热数据。

统计出来。

预热的时候，将这些热数据放在缓存中去预热就可以了。

热点，前面某个商品的访问量，瞬间超出了普通商品的10倍，或者100倍，1000倍，热点。

2、storm这里，会直接发送http请求到nginx上，nginx上用lua脚本去处理这个请求。

storm会将热点本身对应的productId，发送到流量分发的nginx上面去，放在本地缓存中。

storm会将热点对应的完整的缓存数据，发送到所有的应用nginx服务器上去，直接放在本地缓存中。

3、流量分发nginx的分发策略降级。

流量分发nginx，加一个逻辑，就是每次访问一个商品详情页的时候，如果发现它是个热点，那么立即做流量分发策略的降级。

hash策略，同一个productId的访问都同一台应用nginx服务器上。

降级成对这个热点商品，流量分发采取随机负载均衡发送到所有的后端应用nginx服务器上去。

瞬间将热点缓存数据的访问，从hash分发，全部到一台nginx，变成了，负载均衡发送到多台nginx上去。

避免说大量的流量全部集中到一台机器，50万的访问量到一台nginx，5台应用nginx，每台就可以承载10万的访问量。

4、storm还需要保存下来上次识别出来的热点list。

下次去识别的时候，这次的热点list跟上次的热点list做一下diff，看看可能有的商品已经不是热点了。

热点的取消的逻辑，发送http请求到流量分发的nginx上去，取消掉对应的热点数据，从nginx本地缓存中，删除。

### 26.高可用系统架构

资源隔离、限流、熔断、降级、运维监控。

资源隔离：让你的系统里，某一块东西，在故障的情况下，不会耗尽系统所有的资源，比如线程资源。

我实际的项目中的一个case，有一块东西，是要用多线程做一些事情，小伙伴做项目的时候，没有太留神，资源隔离，那块代码，在遇到一些故障的情况下，每个线程在跑的时候，因为那个bug，直接就死循环了，导致那块东西启动了大量的线程，每个线程都死循环。

最终导致我的系统资源耗尽，崩溃，不工作，不可用，废掉了。

资源隔离，那一块代码，最多最多就是用掉10个线程，不能再多了，就废掉了，限定好的一些资源。

限流：高并发的流量涌入进来，比如说突然间一秒钟100万QPS，废掉了，10万QPS进入系统，其他90万QPS被拒绝了。

熔断：系统后端的一些依赖，出了一些故障，比如说mysql挂掉了，每次请求都是报错的，熔断了，后续的请求过来直接不接收了，拒绝访问，10分钟之后再尝试去看看mysql恢复没有。

降级：mysql挂了，系统发现了，自动降级，从内存里存的少量数据中，去提取一些数据出来。

运维监控：监控+报警+优化，各种异常的情况，有问题就及时报警，优化一些系统的配置和参数，或者代码。

### 27.小型电商网站的静态化方案

小型电商网站的页面展示架构的核心思路，全量静态化，页面的全量静态化，nginx取用一个已经静态化好的html页面，直接返回回去，不涉及任何恶的业务逻辑处理。

好处在于：每次用户浏览一个页面，不需要进行任何的跟数据库的交互逻辑，也不需要执行任何的代码，直接返回一个html页面就可以了，速度和性能非常高。

坏处在于：仅仅适用于一些小型的网站，对于大型电商网站的页面，上亿的数据，每次全量静态化不靠谱。

### 28.缓存雪崩

1、redis集群彻底崩溃。

2、缓存服务大量对redis的请求hang住，占用资源。

3、缓存服务大量的请求打到源头服务去查询mysql，直接打死mysql。

4、源头服务因为mysql被打死也崩溃，对源服务的请求也hang住，占用资源。

5、缓存服务大量的资源全部耗费在访问redis和源服务无果，最后自己被拖死，无法提供服务。

6、nginx无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据提供。

7、网站崩溃。

应对缓存雪崩的场景

1、事前解决方案

发生缓存雪崩之前，事情之前，怎么去避免redis彻底挂掉。

redis本身的高可用性，复制，主从架构，操作主节点，读写，数据同步到从节点，一旦主节点挂掉，从节点跟上。

双机房部署，一套redis cluster，部分机器在一个机房，另一部分机器在另外一个机房。

还有一种部署方式，两套redis cluster，两套redis cluster之间做一个数据的同步，redis集群是可以搭建成树状的结构的。

一旦说单个机房出了故障，至少说另外一个机房还能有些redis实例提供服务。

2、事中解决方案

redis cluster已经彻底崩溃了，已经开始大量的访问无法访问到redis了。

（1）ehcache本地缓存

所做的多级缓存架构的作用上了，ehcache的缓存，应对零散的redis中数据被清除掉的现象，另外一个主要是预防redis彻底崩溃。

多台机器上部署的缓存服务实例的内存中，还有一套ehcache的缓存。

ehcache的缓存还能支撑一阵。

（2）对redis访问的资源隔离。

（3）对源服务访问的限流以及资源隔离。

3、事后解决方案

（1）redis数据可以恢复，做了备份，redis数据备份和恢复，redis重新启动起来。

（2）redis数据彻底丢失了，或者数据过旧，快速缓存预热，redis重新启动起来。

redis对外提供服务。

缓存服务里，熔断策略，自动可以恢复，half-open，发现redis可以访问了，自动恢复了，自动就继续去访问redis了。

基于hystrix的高可用服务这块技术之后，先讲解缓存服务如何设计成高可用的架构。

缓存架构应对高并发下的缓存雪崩的解决方案，基于hystrix去做缓存服务的保护。

要带着大家去实现的有什么东西？事前和事后不用了吧。

事中，ehcache本身也做好了。

基于hystrix对redis的访问进行保护，对源服务的访问进行保护，讲解hystrix的时候，也说过对源服务的访问怎么怎么进行这种高可用的保护。

但是站的角度不同，源服务如果自己本身不知道什么原因出了故障，我们怎么去保护，调用商品服务的接口大量的报错、超时。

限流，资源隔离，降级。

### 29.redis集群崩溃

redis集群崩溃的时候，会怎么样？

（1）首先大量的等待，超时，报错。 （2）如果是短时间内报错，会直接走fallback降级，直接返回null。 （3）超时控制，你应该判断说redis访问超过了多长时间，就直接给timeout掉了。

不推荐说用默认的值，一般不太精准，redis的访问你首先自己先统计一下访问时长的百分比，hystrix dashboard，TP90 TP95 TP99。

一般来说，redis访问，假设说TP99在100ms，那么此时，你的timeout稍微多给一些，100ms。

1、timeout超时控制

HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(int value)

意义在于哪里，一旦说redis出现了大面积的故障，此时肯定是访问的时候大量的超过100ms，大量的在等待和超时。

就可以确保说，大量的请求不会hang住过长的时间，比如说hang住个1s，500ms，100ms直接就报timeout，走fallback降级了。

2、熔断策略

（1）circuitBreaker.requestVolumeThreshold

设置一个rolling window，滑动窗口中，最少要有多少个请求时，才触发开启短路。

举例来说，如果设置为20（默认值），那么在一个10秒的滑动窗口内，如果只有19个请求，即使这19个请求都是异常的，也是不会触发开启短路器的。

HystrixCommandProperties.Setter().withCircuitBreakerRequestVolumeThreshold(int value)

我们应该根据我们自己的平时的访问流量去设置，而不是用默认值，比如说，我们认为平时一般的时候，流量也可以在每秒在QPS 100，10秒的滑动窗口就是1000。

一般来说，你可以设置这样的一个值，根据你自己的系统的流量去设置。

假如说，你设置的太少了，或者太多了，都不太合适。

举个例子，你设置一个20，结果在晚上最低峰的时候，刚好是30，可能晚上的时候因为访问不频繁，大量的找不到缓存，可能超时频繁了一些，结果直接就给短路了。

（2）circuitBreaker.errorThresholdPercentage

设置异常请求量的百分比，当异常请求达到这个百分比时，就触发打开短路器，默认是50，也就是50%。

HystrixCommandProperties.Setter().withCircuitBreakerErrorThresholdPercentage(int value)

我们最好还是自己定制，自己设置，你说如果是要50%的时候才短路的话，会有什么情况呢，10%短路，也不太靠谱，90%异常，才短路。

我觉得这个值可以稍微高一些，redis集群彻底崩溃，那么基本上就是所有的请求，100%都会异常，60%，70%。

也有可能偶然出现网络的抖动，导致比如说就这10秒钟，访问延时高了一些，其实可能并不需要立即就短路，可能下10秒马上就恢复了3

金融支付类的接口，可能这个比例就会设置的很低，因为对异常系统必须要很敏感，可能就是10%异常了，就直接短路了，不让继续访问了.

比如金融支付类的接口，正常来说，是很重要的，而且必须是很稳定，我们不能容忍任何的延迟或者是报错。

一旦支付类的接口，有10%的异常的话，我们基本就可以认为这个接口已经出问题了，再继续访问的话，也许访问的就是有问题的接口，可能造成资金的错乱，等给公司造成损失。

熔断，不让访问了，走降级策略。

就是对整个系统，是一个安全性的保障。

（3）circuitBreaker.sleepWindowInMilliseconds

设置在短路之后，需要在多长时间内直接reject请求，然后在这段时间之后，再重新导holf-open状态，尝试允许请求通过以及自动恢复，默认值是5000毫秒

HystrixCommandProperties.Setter().withCircuitBreakerSleepWindowInMilliseconds(int value)

如果redis集群崩溃了，会在5s内就直接恢复，1分钟。

### 30.缓存穿透

大量不存在的key访问reids，导致每次都穿透redis去查询数据库，大量数据涌入数据库导致被打死。

解决方案

我们的缓存穿透的解决方案，其实非常的简单，就是说每次如果从源服务（商品服务）查询到的数据是空，就说明这个数据根本就不存在。

那么如果这个数据不存在的话，我们不要不往redis和ehcache等缓存中写入数据，我们呢，给写入一个空的数据，比如说空的productInfo的json串。

给nginx也是，返回一个空的productInfo的json串咯。

因为我们有一个异步监听数据变更的机制在里面，也就是说，如果数据变更的话，某个数据本来是没有的，可能会导致缓存穿透，所以我们给了个空数据。

但是现在这个数据有了，我们接收到这个变更的消息过后，就可以将数据再次从源服务中查询出来。

然后设置到各级缓存中去了。

### 31.缓存失效

我们在nginx中设置本地的缓存的时候，会给一个过期的时间，比如说10分钟。

10分钟以后自动过期，过期了以后，就会重新从redis中去获取数据。

这个10分钟到期自动过期的事情，就叫做缓存的失效。

如果缓存失效以后，那么实际上此时，就会有大量的请求回到redis中去查询。

如果说同一时间来了1000个请求，都将缓存cache在了nginx自己的本地，缓存失效的时间都设置了10分钟。

那么是不是可能导致10分钟过后，这些数据，就自动全部在同一时间失效了。

如果同一时间全部失效，会不会导致说同一时间大量的请求过来，在nginx里找不到缓存数据，全部高并发走到redis上去了。

加重大量的网络请求，网络负载也会加重。

### 32.商品详情页系统架构演进历程

商品详情页系统架构演进历程

第一个版本 架构设计：

J2EE+Tomcat+MySQL 动态页面，每次请求都要调用多个依赖服务的接口，从数据库里查询数据，然后通过类似JSP的技术渲染到HTML模板中，返回最终HTML页面。 架构缺陷： 每次请求都是要访问数据库的，性能肯定很差。

每次请求都要调用大量的依赖服务，依赖服务不稳定导致商品详情页展示的性能经常抖动。 第二个版本 架构设计： 页面静态化技术。 通过MQ得到商品详情页涉及到的数据的变更消息。 通过Java Worker服务全量调用所有的依赖服务的接口，查询数据库，获取到构成一个商品详情页的完整数据，并通过velocity等模板技术生成静态HTML。 将静态HTML页面通过rsync工具直接推送到多台nginx服务器上，每台nginx服务器上都有全量的HTML静态页面。 nginx对商品详情页的访问请求直接返回本地的静态HTML页面。 在nginx服务器前加一层负载均衡设备，请求打到任何一台应用nginx服务器上，都有全量的HTML静态页面可以返回。

架构缺陷： 全量更新问题。 如果某一个商品分类、商家等信息变更了，那么那个分类、店铺、商家下面所有的商品详情页都需要重新生成静态HTML页面。 更新速度过慢问题。 分类、店铺、商家、商品越来越多，重新生成HTML的负载越来越高，rsync全量同步所有nginx的负载也越来越高。 从数据变更到生成静态HTML，再到全量同步到所有nginx，时间越来越慢。 扩容问题 因为每个商品详情页都要全量同步到所有的nginx上，导致系统无法扩容，无法增加系统容量。 架构优化 解决全量更新问题 每次Java Worker收到某个维度的变更消息，不是拉去全量维度并生成完整HTML，而是按照维度拆分，生成一个变化维度的HTML片段。 nginx对多个HTML片段通过SSI合并html片段然后输出一个完整的html。

解决扩容问题 每个商品详情页不是全量同步到所有的nginx。 而是根据商品id路由到某一台nginx上，同时接入层nginx按照相同的逻辑路由请求。 更新速度过慢问题。 增加更多机器资源。 多机房部署，每个机房部署一套Java Worker+应用Nginx，所有机房用一套负载均衡设备，在每个机房内部完成全流程，不跨机房。 架构优化后的缺陷 更新速度还是不够快的问题。

商品的每个维度都有一个HTML片段，rsync推送大量的HTML片段，负载太高，性能较差。 Nginx基于机械硬盘进行SSI合并，性能太差。 还是存在全量更新的问题 虽然解决了分类、商家、店铺维度的变更，只要增量重新生产较小的HTML片段即可，不用全量重新生成关联的所有商品详情页的HTML。但是如果某个页面模板变更，或者新加入一个页面模板，还是会导致几亿个商品的HTML片段都要重新生成和rsync，要几天时间才能完成，无法响应需求。 还是存在容量问题。 nginx存储有限，不能无限存储几亿，以及增长的商品详情页的HTML文件。 如果nginx存储达到极限，需要删除部分商品详情页的HTML文件，改成nginx找不到HTML，则调用后端接口，回到动态页面的架构。 动态页面架构在高并发访问的情况下，会对依赖系统造成过大的压力，几乎扛不住。

第三个版本 需要支持的需求： 迅速响应各种页面模板的改版和个性化需求的新模板的加入。 页面模块化，页面中的某个区域变化，只要更新这个区域中的数据即可。 支持高性能访问。 支持水平扩容的伸缩性架构。 架构设计： 系统架构设计。 依赖服务有数据变更发送消息到MQ。 数据异构Worker服务监听MQ中的变更消息，调用依赖服务的接口，仅仅拉取有变更的数据即可，然后将数据存储到redis中。 数据异构Worker存储到redis中的，都是原子未加工数据，包括商品基本信息、商品扩展属性、商品其他信息、商品规格参数、商品分类、商家信息。 数据异构Worker发送消息到MQ，数据聚合Worker监听到MQ消息。 数据聚合Worker将原子数据从redis中取出，按照维度聚合后存储到redis中，包括三个维度。 基本信息维度：基本信息、扩展属性。 商品介绍：PC版、移动版。 其他信息：商品分类、商家信息。 nginx+lua，lua从redis读取商品各个维度的数据，通过nginx动态渲染到html模板中，然后输出最终的html。 如何解决所有的问题。 更新问题：不再是生成和推送html片段了，不再需要合成html，直接数据更新到redis，然后走动态渲染，性能大大提升。 全量更新问题：数据和模板分离，数据更新呢就更新数据，模板更新直接推送模板到nginx，不需要重新生成所有html，直接走动态渲染。 容量问题：不需要依赖nginx所在机器的磁盘空间存储大量的html，将数据放redis，html就存放模板，大大减少空间占用，而且redis集群可扩容 。

### 33.商品详情页整体架构组成

动态渲染系统 将页面中静的数据，直接在变更的时候推送到缓存，然后每次请求页面动态渲染新数据。 商品详情页系统（负责静的部分）：被动接收数据，存储redis，nginx+lua动态渲染。 商品详情页动态服务系统（对外提供数据接口）。 提供各种数据接口。 动态调用依赖服务的接口，产生数据并且返回响应。 从商品详情页系统处理出来的redis中，获取数据，并返回响应。 OneService系统 动的部分，都是走ajax异步请求的，不是走动态渲染的。 商品详情页统一服务系统（负责动的部分）。 前端页面 静的部分，直接被动态渲染系统渲染进去了。 动的部分，html一到浏览器，直接走js脚本，ajax异步加载。 商品详情页，分段存储，ajax异步分屏加载。 工程运维 限流，压测，灰度发布。

### 34.多级缓存架构

本地缓存： 使用nginx shared dict作为local cache，http-lua-module的shared dict可以作为缓存，而且reload nginx不会丢失。 也可以使用nginx proxy cache做local cache。 双层nginx部署，一层接入，一层应用，接入层用hash路由策略提升缓存命中率。 比如库存缓存数据的TP99为5s，本地缓存命中率25%，redis命中率28%，回源命中率47%。 一次普通秒杀活动的命中率，本地缓存55%，分布式redis命中率15%，回源命中率27%。 最高可以提升命中率达到10%。 全缓存链路维度化存储，如果有3个维度的数据，只有其中1个过期了，那么只要获取那1个过期的数据即可。nginx local cache的过期时间一般设置为30min，到后端的流量会减少至少3倍。

4级多级缓存 nginx本地缓存，抗热点数据，小内存缓存访问最频繁的数据。 各个机房本地的redis从集群的数据，抗大量离线数据，采用一致性hash策略构建分布式redis缓存集群。 tomcat中的动态服务的本地jvm堆缓存。 支持在一个请求中多次读取一个数据，或者与该数据相关的数据。 作为redis崩溃的备用防线。 固定缓存一些较少访问频繁的数据，比如分类，品牌等数据。 堆缓存过期时间为redis过期时间的一半。 主redis集群。 命中率非常低，小于5%。 防止主从同步延迟导致的数据读取miss。 防止各个机房的从redis集群崩溃之后，全量走依赖服务会导致雪崩，主redis集群是后备防线。 主redis集群，采取多机房一主三从的高可用部署架构。 redis集群部署采取双机房一主三活的架构，机房A部署主集群+一个从集群，机房B部署一个从集群（从机房A主集群）+一个从集群（从机房B从集群）。 双机房一主三活的架构，保证了机房A彻底故障的时候，机房B还有一套备用的集群，可以升级为一主一从。 如果采取机房A部署一主一从，机房B一从，那么机房A故障时，机房B的一从承载所有读写压力，压力过大，很难承受。

### 35.商品详情页动态渲染系统：架构整体设计

（1）依赖服务 -> MQ -> 动态渲染服务 -> 多级缓存 （2）负载均衡 -> 分发层nginx -> 应用层nginx -> 多级缓存 （3）多级缓存 -> 数据直连服务

动态渲染系统

数据闭环 数据闭环架构 依赖服务：商品基本信息，规格参数，商家/店铺，热力图，商品介绍，商品维度，品牌，分类，其他。 发送数据变更消息到MQ。 数据异构Worker集群，监听MQ，将原子数据存储到redis，发送消息到MQ。 数据聚合Worker集群，监听MQ，将原子数据按维度聚合后存储到redis，三个维度（商品基本信息、商品介绍、其他信息）。 数据闭环，就是数据的自我管理，所有数据原样同步后，根据自己的逻辑进行后续的数据加工，走系统流程，以及展示k。 数据形成闭环之后，依赖服务的抖动或者维护，不会影响到整个商品详情页系统的运行。 数据闭环的流程：数据异构（多种异构数据源拉取），数据原子化，数据聚合（按照维度将原子数据进行聚合），数据存储（Redis）。

数据维度化 商品基本信息：标题、扩展属性、特殊属性、图片、颜色尺码、规格参数。 商品介绍。 非商品维度其他信息：分类，商家，店铺，品牌。 商品维度其他信息：采用ajax异步加载，价格，促销，配送至，广告，推荐，最佳组合，等等。 采取ssdb，这种基于磁盘的大容量/高性能的kv存储，保存商品维度、主商品维度、商品维度其他信息，数据量大，不能光靠内存去支撑。 采取redis，纯内存的kv存储，保存少量的数据，比如非商品维度的其他数据，商家数据，分类数据，品牌数据。 一个完整的数据，拆分成多个维度，每个维度独立存储，就避免了一个维度的数据变更就要全量更新所有数据的问题。 不同维度的数据，因为数据量的不一样，可以采取不同的存储策略。

系统拆分 系统拆分更加细：依赖服务、MQ、数据异构Worker、数据同步Worker、Redis、Nginx+Lua。 每个部分的工作专注，影响少，适合团队多人协作。 异构Worker的原子数据，基于原子数据提供的服务更加灵活。 聚合Worker将数据聚合后，减少redis读取次数，提升性能。 前端展示分离为商品详情页前端展示系统和商品介绍前端展示系统，不同特点，分离部署，不同逻辑，互相不影响。

异步化 异步化，提升并发能力，流量削峰。 消息异步化，让各个系统解耦合，如果使用依赖服务调用商品详情页系统接口同步推送，那么就是耦合的。 缓存数据更新异步化，数据异构Worker同步调用依赖服务接口，但是异步更新redis。

动态化 数据获取动态化：nginx+lua获取商品详情页数据的时候，按照维度获取，比如商品基本数据、其他数据（分类、商家）。 模板渲染实时化：支持模板页面随时变化，因为采用的是每次从nginx+redis+ehcache缓存获取数据，渲染到模板的方式，因此模板变更不用重新静态化HTML。 重启应用秒级化：nginx+lua架构，重启在秒级。 需求上线快速化：使用nginx+lua架构开发商品详情页的业务逻辑，非常快速。

多机房多活 Worker无状态，同时部署在各自的机房时采取不同机房的配置，来读取各自机房内部部署的数据集群（redis、mysql等）。 将数据异构Worker和数据聚合Worker设计为无状态化，可以任意水平扩展。 Worker无状态化，但是配置文件有状态，不同的机房有一套自己的配置文件，只读取自己机房的redis、ssdb、mysql等数据。 每个机房配置全链路：接入nginx、商品详情页nginx+商品基本信息redis集群+其他信息redis集群、商品介绍nginx+商品介绍redis集群。 部署统一的CDN以及LVS+KeepAlived负载均衡设备。

### 36.消息队列架构

队列化 任务等待队列。 任务排重队列（异构Worker对一个时间段内的变更消息做排重）。 失败任务队列（失败重试机制）。 优先级队列，刷数据队列（依赖服务洗数据）、高优先级队列（活动商品优先级高）。

### 37.并发化

并发化 数据同步服务做并发化+合并，将多个变更消息合并在一起，调用依赖服务一次接口获取多个数据，采用多线程并发调用。 数据聚合服务做并发化，每次重新聚合数据的时候，对多个原子数据用多线程并发从redis查询。

### 38.高可用设计

读链路多级降级：本机房从集群 -> 主集群 -> 直连。

全链路隔离 基于hystrix的依赖调用隔离，限流，熔断，降级。 普通服务的多机房容灾冗余部署以及隔离。

1.如果缓存从集群挂掉，怎么降级？应用nginx会做一个计数，如果访问缓存从集群最近连续几次都失败，那么就认为挂掉，然后设置一个标志位以及一个时间周期，在那个时间周期内，请求会发现标志位是降级标志位，然后就不走本机房的缓存从集群，直接走数据直连服务。

2.如果数据直连服务，宕机了，怎么办？应用nginx一样，记录好直连服务，如果最近几次http请求都失败，那么也是修改标志位，设置降级时间。然后每次请求直接从应用nginx访问到缓存主集群。

3.如果缓存主集群也宕机了，那么此时再次降级，修改标志位，直接应用nginx，跨机房，去直接调用依赖服务的接口，依赖服务是直接查询mysql的，所以这里的话，其实每个依赖服务都要基于hystrix做限流，熔断等措施，避免自己被打垮。

4.读链路多级降级的策略，控制权掌握在应用层nginx手中，它自己回去检测问题，自动进行整个读链条的自动降级。

### 39.微服务架构设计

1、领域驱动设计：我们需要对这个系统涉及到的领域模型机进行分析，然后进行领域建模，最后的话，设计出我们对应的微服务的模型。 2、spring cloud：微服务的基础技术架构，我们用spring cloud来做。 3、持续交付流水线，jenkins+git+自动化持续集成+自动化测试+自动化部署。 4、docker：大量的微服务的部署与管理。

### 40.redis cluster的问题

twemproxy+redis去做集群，redis部署多个主实例，每个主实例可以挂载一些redis从实例，如果将不同的数据分片，写入不同的redis主实例中，twemproxy这么一个缓存集群的中间件。

redis cluster

（1）不好做读写分离，读写请求全部落到主实例上的，如果要扩展写QPS，或者是扩展读QPS，都是需要扩展主实例的数量，从实例就是一个用做热备+高可用。 （2）不好跟nginx+lua直接整合，lua->redis的client api，但是不太支持redis cluster，中间就要走一个中转的java服务。 （3）不好做树状集群结构，比如redis主集群一主三从双机房架构，redis cluster不太好做成那种树状结构。 （4）方便，相当于是上下线节点，集群扩容，运维工作，高可用自动切换，比较方便。

twemproxy+redis

（1）上线下线节点，有一些手工维护集群的成本。 （2）支持redis集群+读写分离，最基本的多个redis主实例，twemproxy这个中间件来决定的，java/nginx+lua客户端，是连接twemproxy中间件的。每个redis主实例就挂载了多个redis从实例，高可用->哨兵，redis cluster读写都要落到主实例的限制，你自己可以决定写主，读从，等等。 （3）支持redis cli协议，可以直接跟nginx+lua整合。 （4）可以搭建树状集群结构。

### 41.微服务

微服务的强大作用：

（1）迭代速度：你只要管好自己的服务就行了，跟别人没关系，随便你这么玩儿，修改代码，测试，部署，都是你自己的事情，不用考虑其他人，没有任何耦合。 （2）复用性：拆分成一个一个服务之后，就不需要写任何重复的代码了，有一个功能别人做好了，暴露了接口出来，直接调用不就ok了么。 （3）扩展性：独立，扩展，升级版本，重构，更换技术。 （4）完全克服了传统单块应用的缺点。

微服务的缺点

（1）服务太多，难以管理。 （2）微服务 = 分布式系统，你本来是一个系统，现在拆分成多块，部署在不同的服务器上，一个请求要经过不同的服务器上不同的代码逻辑处理，才能完成，这不就是分布式系统。 （3）分布式一致性，分布式事务，故障+容错。

微服务的技术栈

（1）领域驱动设计：微服务建模

你的任何业务系统都有自己独特的复杂的业务，但是这个时候就是有一个问题，怎么拆分服务？拆成哪些服务？拆成多大？每个服务负责哪些功能？

微服务的建模，模型怎么设计。

领域驱动的设计思想，可以去分析系统，完成建模的设计。

（2）Spring Cloud：基础技术架构

各个服务之间怎么知道对方在哪里 -> 服务的注册和发现。

服务之间的调用怎么处理，rpc，负载均衡。

服务故障的容错。

服务调用链条的追踪怎么做。

多个服务依赖的统一的配置如何管理。

（3）DevOps：自动化+持续集成+持续交付+自动化流水线，将迭代速度提升到极致

如果要将微服务的开发效率提升到最高，DevOps，全流程标准化，自动化，大幅度提升你的开发效率。

（4）Docker：容器管理大量服务

微服务，一个大型的系统，可以涉及到几十个，甚至是上百个服务，比较坑，怎么部署，机器怎么管理，怎么运维。