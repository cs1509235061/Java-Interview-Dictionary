# 实战

### 1.线上服务器的cpu使用达到100%了，如何排查、定位和解决该问题？

其实核心思路，就是找到这台服务器上，是哪个进程的哪个线程的哪段代码，导致cpu 100了，主要就是考察你是否熟练运用一些线上的命令。

这里我可以给大家说一个我们线上的经验，就是之前有一个bug，是一个很年轻的同学写的，就是我们当时是定了异常日志是写到es里去的

public void log(String message) {

try {

// 往es去写

} catch(Exception e) {

log(message);

}

}

线上事故，es集群出了点问题，没法写，最后出现线上几十台机器，全部因为这一行代码，全体cpu 100%，卡死了

（1）定位耗费cpu的进程

top -c，就可以显示进程列表，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND

43987 root 20 0 28.2g 4.5g 68m S 99.0 24.0 44333.4 java -Xms。。。

大概类似上面这样，能看到哪个进程，CPU负载最高，还有启动这个进程的命令，比如一般就是java啥啥的。

（2）定位耗费cpu的线程

top -Hp 43987，就是输入那个进程id就好了，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND

16872 root 20 0 28.2g 4.5g 68m S 95.0 12.0 65543.3 java

大概类似上面那样，你就可以看到这个进程里的哪个线程耗费cpu最高

（3）定位哪段代码导致的cpu过高

printf “%x\n” 16872，把线程pid转换成16进制，比如41e8

jstack 43987 | grep ‘0x41e8’ -C5 --color

这个就是用jstack打印进程的堆栈信息，而且通过grep那个线程的16进制的pid，找到那个线程相关的东西，这个时候就可以在打印出的代码里，看到是哪个类的哪个方法导致的这个cpu 100%的问题

### 2.kill进程杀不掉怎么处理？

我们公司有一套自己研发的发布系统，你每次部署，都是走发布系统，告诉他一个git仓库的地址，那个系统会自动从git仓库拉取代码，基于maven来打包，你还可以指定你要用的profile，maven打包的时候会用对应的profile打对应环境的包，打完jar包之后，就会java -jar之类的来启动。

当时那个发布系统，他自己在每台机器上有一个进程，发布和启动的时候，他启动的那个进程，不是直接java -jar来启动的，发布系统的一个进程搞了一个子进程，子进程是我们的系统进程。

这个其实就是线上可能遇到的一个问题，我们之前确实就是遇到过这个问题，kill一个进程死活杀不死，那个进程成了僵尸进程，就是zombie状态。这是因为这个进程释放了资源，但是没有得到父进程的确认。

ps aux，看看STAT那一栏，如果是Z，那么就是zombie状态的僵尸进程

ps -ef | grep 僵尸进程id，可以找到父进程id

然后先kill掉父进程即可

### 3.服务器存储空间快满了（95%），还有一个小时存储就满了，在不影响服务正常运行的情况下，该如何解决？

df -h，先看看磁盘使用的情况

然后就是到你的系统部署的地方，一般就是tomcat下的日志、spring boot的日志，去看看，如果过多，就删除掉一些日志就行了，自己注意让tomcat或者nginx之类的日志输出，按天切割，这样你还可以写个shell脚本，crontab定时，定期删除7天以前的日志

要是不行，那就：find / -size +100M |xargs ls -lh，找找大于100m的文件，但是如果有大量的小文件，那么这样是不行的

或者是用：du -h >fs_du.log，看看各个目录占用的磁盘空间大小，看看是不是哪个目录有大量的小文件

其实面试官无非就是看看是不是知道常见的命令罢了，如果不是。那那个面试官就得再提示多一些细节，到底要考察你什么。但是简单问一个磁盘占用排查，就是常见这几个命令罢了。ok。。。。

### 4.jvm栈在哪些情况下会溢出？java堆在哪些情况下会溢出？做过哪些jvm优化？用了哪些方法？达到了什么样的效果？jvm问题（内存泄露、线程卡死、jvm崩溃、内存溢出、频繁gc），该如何定位和排查（jmap和jstack）？

1.**线上oom问题**

常见的问题是突然之间进程就没了，就是线上的java进程跑着跑着就没了。

有个一个命令：dmesg |grep -E ‘kill|oom|out of memory’，可以查看操作系统启动后的系统日志，这里就是查看跟内存溢出相关联的系统日志。如果你确实是oom导致进程被杀死了，那么系统日志里会出现这样的字眼：Out of memory，kill process 13987（java）。。。。。意思就是说，OOM了，然后就杀了你的那个java进程。那么至少用这个命令查看系统日志，就可以确认是OOM问题发生了。

如果一旦oom，就会导致你的jvm死掉，然后你肯定得重启吧。可能你重启之后，先用ps -aux|grep java命令查看一下你的java进程，就可以找到你的java进程的进程id。然后你可以用top命令看一下，top命令显示的结果列表中，会看到%MEM这一列，这里可以看到你的进程可能对内存的使用率特别高。

接着，可以用jstat -gcutil 20886 1000 10命令，就是用jstat工具，对指定java进程（20886就是进程id，通过ps -aux | grep java命令就能找到），按照指定间隔，看一下统计信息，这里会每隔一段时间显示一下，包括新生代的两个Survivor区、Eden区，以及老年代的内存使用率，还有young gc以及full gc的次数。

看到的东西类似下面那样：

S0 S1 E O YGC FGC

26.80 0.00 10.50 89.90 86 954

其实如果大家了解原理，应该知道，一般来说大量的对象涌入内存，结果始终不能回收，会出现的情况就是，快速撑满年轻代，然后young gc几次，根本回收不了什么对象，导致survivor区根本放不下，然后大量对象涌入老年代。老年代很快也满了，然后就频繁full gc，但是也回收不掉。

然后对象持续增加不就oom了，内存放不下了，爆了呗。

所以jstat先看一下基本情况，马上就能看出来，其实就是大量对象没法回收，一直在内存里占据着，然后就差不多内存快爆了。

接着就是用jmap来一把，jmap -histo:live 20886，看看现在现在java进程里的对象分布情况，就是根据每个对象占用内存从大到小来排列的，你可能会看到下面的东西：

1: 485009 489005612 [B

2: 2609794 98034943 [C

。。。。N多行

其实你一下就可以发现是哪些对象占据内存过多了

接着就是要获取一个堆内存快照了，jmap -dump:format=b,file=文件名 [pid]，就可以把指定java进程的堆内存快照搞到一个指定的文件里去，但是jmap -dump:format其实一般会比较慢一些，也可以用gcore工具来导出内存快照。

先在linux上安装gdb，这个自己百度一下就好，很多的

gdb -q --20886　//启动gdb命令

(gdb) generate-core-file //这里调用命令生成gcore的dump文件

(gdb) gcore /tmp/dump.core　//dump出core文件

(gdb) detach //detach是用来断开与jvm的连接的

(gdb) quit //退出

就用上面的命令即可

jmap -dump:format=b,file=heap.hprof /opt/zhss/java/bin /tmp/dump.core

接着用上面这行命令，将dump.core文件转换成hprof文件即可

接着就是可以用MAT工具，或者是Eclipse MAT的内存分析插件，来对hprof文件进行分析，看看到底是哪个王八蛋对象太多了，导致内存溢出了

用MAT分析一下hprof文件，马上就是出来一个图

byte[] 489005612 489005612 489005612

。。。。。下面一大堆

这个其实也是给你显示每个对象占用的内存大小，大概就是这个

其实到此为止就差不多了，我们没时间带着大家来实战，以后架构班都会带着大家深入学习和实战，如果面试，你能把上面的步骤说出来就不错了，很标准的步骤，一些工具的使用都ok了，你就说看到具体哪个对象之后，就去看代码，解决问题就行了。

一般常见的OOM，要么是短时间内涌入大量的对象，导致你的系统根本支持不住，此时你可以考虑优化代码，或者是加机器；要么是长时间来看，你的很多对象不用了但是还被引用，就是内存泄露了，你也是优化代码就好了；这就会导致大量的对象不断进入老年代，然后频繁full gc之后始终没法回收，就撑爆了

要么是加载的类过多，导致class在永久代理保存的过多，始终无法释放，就会撑爆

我这里可以给大家最后提一点，人家肯定会问你有没有处理过线上的问题，你就说有，最简单的，你说有个小伙子用了本地缓存，就放map里，结果没控制map大小，可以无限扩容，最终导致内存爆了，后来解决方案就是用了一个ehcache框架，自动LRU清理掉旧数据，控制内存占用就好了。

另外，务必提到，线上jvm必须配置-XX:+HeapDumpOnOutOfMemoryError，-XX:HeapDumpPath=/path/heap/dump。因为这样就是说OOM的时候自动导出一份内存快照，你就可以分析发生OOM时的内存快照了，到底是哪里出现的问题。

2.**系统频繁full gc**

比OOM稍微好点的是频繁full gc，如果OOM就是系统自动就挂了，很惨，你绝对是超级大case，但是频繁full gc会好多，其实就是表现为经常请求系统的时候，很卡，一个请求卡半天没响应，就是会觉得系统性能很差。

首先，你必须先加上一些jvm的参数，让线上系统定期打出来gc的日志：

-XX:+PrintGCTimeStamps

-XX:+PrintGCDeatils

-Xloggc:<filename>

如果要是发现每次Full GC过后，ParOldGen就是老年代老是下不去，那就是大量的内存一直占据着老年代，啥事儿不干，回收不掉，所以频繁的full gc，每次full gc肯定会导致一定的stop the world卡顿，这是不可能完全避免的

接着采用跟之前一样的方法，就是dump出来一份内存快照，然后用Eclipse MAT插件分析一下好了，看看哪个对象量太大了

接着其实就是跟具体的业务场景相关了，要看具体是怎么回事，常见的其实要么是内存泄露，要么就是类加载过多导致永久代快满了，此时一般就是针对代码逻辑来优化一下。

给大家还是举个例子吧，我们线上系统的一个真实例子，大家可以用这个例子在面试里来说，比如说当时我们有个系统，在后台运行，每次都会一下子从mysql里加载几十万行数据进来各种处理，类似于定时批量处理，这个时候，如果对几十万数据的处理比较慢，就会导致比如几分钟里面，大量数据囤积在老年代，然后没法回收，就会频繁full gc。

当时我们其实就是根据这个发现了当时两台机器已经不够了，因为我们当时线上用了两台4核8G的虚拟机在跑，明显不够了，就要加机器了，所以增加了机器，每台机器处理更少的数据量，那不就ok了，马上就缓解了频繁full gc的问题了。

面试就先到这里，以后我们架构师课程会走实战派，大量线上系统jvm问题在机器上给大家模拟出来，然后带着大家实战一步一步演练和处理。