# MySQL

### 1.存储引擎

数据库的存储引擎是数据库的底层软件组织，数据库管理系统（DBMS）使用存储引擎创建、查询、更新和删除数据。不同的存储引擎提供了不同的存储机制、索引技巧、锁定水平等功能，都有其特定的功能。现在，许多数据库管理系统都支持多种存储引擎，常用的存储引擎主要有MyISAM、InnoDB、Memory、Archive和Federated。

MyIASM

MyIASM是MySQL默认的存储引擎，不支持数据库事务、行级锁和外键，因此在INSERT（插入）或UPDATE（更新）数据即写操作时需要锁定整个表，效率较低。

MyIASM的特点是执行读取操作的速度快，且占用的内存和存储资源较少。它在设计之初就假设数据被组织成固定长度的记录，并且是按顺序存储的。在查找数据时，MyIASM直接查找文件的OFFSET，定位比InnoDB要快（InnoDB寻址时要先映射到块，再映射到行）。

总体来说，MyIASM的缺点是更新数据慢且不支持事务处理，优点是查询速度快。

InnoDB（B+树）

InnoDB为MySQL提供了事务（Transaction）支持、回滚（Rollback）、崩溃修复能力（Crash Recovery Capabilities）、多版本并发控制（Multi-versioned Concurrency Control）、事务安全（Transaction-safe）的操作。InnoDB的底层存储结构为B+树，B+树的每个节点都对应InnoDB的一个Page，Page大小是固定的，一般被设为16KB。其中，非叶子节点只有键值，叶子节点包含完整的数据

InnoDB适用于有以下需求的场景。

- 经常有数据更新的表，适合处理多重并发更新请求。
- 支持事务。
- 支持灾难恢复（通过bin-log日志等）。
- 支持外键约束，只有InnoDB支持外键。
- 支持自动增加列属性auto_increment。

TokuDB（Fractal Tree-节点带数据）

TokuDB的底层存储结构为Fractal Tree。Fractal Tree的结构与B+树有些类似，只是在Fractal Tree中除了每一个指针（key），都需要指向一个child（孩子）节点，child节点带一个Message Buffer，这个MessageBuffer是一个先进先出队列，用来缓存更新操作，这样，每一次插入操作都只需落在某节点的Message Buffer上，就可以马上返回，并不需要搜索到叶子节点。这些缓存的更新操作会在后台异步合并并更新到对应的节点上。

TokuDB在线添加索引，不影响读写操作，有非常高的写入性能，主要适用于要求写入速度快、访问频率不高的数据或历史数据归档。

Memory

Memory表使用内存空间创建。每个Memory表实际上都对应一个磁盘文件用于持久化。Memory表因为数据是存放在内存中的，因此访问速度非常快，通常使用Hash索引来实现数据索引。Memory表的缺点是一旦服务关闭，表中的数据就会丢失。

Memory还支持散列索引和B树索引。B树索引可以使用部分查询和通配查询，也可以使用不等于和大于等于等操作符方便批量数据访问，散列索引相对于B树索引来说，基于Key的查询效率特别高，但是基于范围的查询效率不是很高。

Archive 存储引擎

正如其名称所示，Archive 非常适合存储归档数据，如日志信息。它只支持 INSERT 和 SELECT 操作，其设计的主要目的是提供高速的插入和压缩功能。

Federated 存储引擎

Federated存储引擎不存放数据，它至少指向一台远程 MySQL 数据库服务器上的表，非常类似于 Oracle 的透明网关。

Maria 存储引擎

Maria存储引擎是新开发的引擎，其设计目标是用来取代原有的 MyISAM 存储引擎，从而成为 MySQL 默认的存储引擎。

### 2.数据库三范式

范式是具有最小冗余的表结构，三范式的概念如下所述。

第一范式(1st NF －列都是不可再分)

如果每列都是不可再分的最小数据单元（也叫作最小的原子单元），则满足第一范式，第一范式的目标是确保每列的原子性。比如，表中的Address列违背了第一范式列不可再分的原则，要满足第一范式，就需要将Address列拆分为Country列和City列。

第二范式(2nd NF－每个表只描述一件事情)

第二范式在第一范式的基础上，规定表中的非主键列不存在对主键的部分依赖，即第二范式要求每个表只描述一件事情。比如，Orders表既包含订单信息，也包含产品信息，需要将其拆分为两个单独的表。

第三范式(3rd NF－不存在对非主键列的传递依赖)

第三范式的定义为：满足第一范式和第二范式，并且表中的列不存在对非主键列的传递依赖。比如，除了主键的订单编号，顾客姓名依赖于非主键的顾客编号，因此需要将该列去除。

### 3.存储过程(特定功能的SQL 语句集)

存储过程指一组用于完成特定功能的SQL语句集，它被存储在数据库中，经过第一次编译后再次调用时不需要被再次编译，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。存储过程是数据库中的一个重要对象，我们可以基于存储过程快速完成复杂的计算操作。

以下为常见的存储过程的优化思路，也是我们编写事务时需要遵守的原则。

- 尽量利用一些SQL语句代替一些小循环，例如聚合函数、求平均函数等。
- 中间结果被存放于临时表中，并加索引。
- 少使用游标（Cursors）：SQL 是种集合语言，对于集合运算有较高的性能，而游标是过程运算。比如，对一个50 万行的数据进行查询时，如果使用游标，则需要对表执行50万次读取请求，将占用大量的数据库资源，影响数据库的性能。
- 事务越短越好：SQL Server支持并发操作，如果事务过长或者隔离级别过高，则都会造成并发操作的阻塞、死锁，导致查询速度极慢、CPU占用率高等。
- 使用try-catch处理异常。
- 尽量不要将查找语句放在循环中，防止出现过度消耗系统资源的情况。

```html
CREATE PROCEDURE getUserInfo_PROC
AS
begin
//过程体
end
GO
```

### 4.触发器(一段能自动执行的程序)

触发器是一段能自动执行的程序，和普通存储过程的区别是“触发器在对某一个表或者数据进行操作时触发”，例如进行UPDATE、INSERT、DELETE操作时，系统会自动调用和执行该表对应的触发器。触发器一般用于数据变化后需要执行一系列操作的情况，比如对系统核心数据的修改需要通过触发器来存储操作日志的信息等。

### 5.MySQL是如何实现Read Repeatable的

当我们使用innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；

同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。

### 6.约束

– 主键约束（Primary Key Constraint）：要求主键列数据唯一，并且不允许为空 – 唯一约束（Unique Constraint）：要求该列唯一，允许为空，但只能出现一个空值。 – 检查约束（Check Constraint）：某列取值范围限制、格式限制等，如有关年龄的约束 – 默认约束（Default Constraint）：某列的默认值，如我们的男性学员较多，性别默认为“男” – 外键约束（Foreign Key Constraint）：用于两表间建立关系，需要指定引用主表的那列

### 7.drop,delete与truncate的区别？

drop 直接删除表；truncate 删除表中数据，再插入时自增长id又从1开始 ；delete 删除表中数据，可以加where字句。

> - **drop table：**

- 属于DDL（Data Definition Language，数据库定义语言）
- 不可回滚
- 不可带 where
- 表内容和结构删除
- 删除速度快

> - **truncate table：**

- 属于DDL（Data Definition Language，数据库定义语言）
- 不可回滚
- 不可带 where
- 表内容删除
- 删除速度快

> - **delete from：**

- 属于DML
- 可回滚
- 可带where
- 表结构在，表内容要看where执行的情况
- 删除速度慢,需要逐行删除

> - **使用简要说明：**

- 不再需要一张表的时候，用drop
- 想删除部分数据行时候，用delete，并且带上where子句
- 保留表而删除所有数据的时候用truncate

### 8.触发器有哪几种

1、DML 触发器

当数据库中表中的数据发生变化时，包括insert,update,delete 任意操作，如果我们对该表写了对应的DML 触发器，那么该触发器自动执行。DML 触发器的主要作用在于强制执行业务规则，以及扩展Sql Server 约束，默认值等。因为我们知道约束只能约束同一个表中的数据，而触发器中则可以执行任意Sql 命令。

2、DDL 触发器

它是Sql Server2005 新增的触发器，主要用于审核与规范对数据库中表，触发器，视图等结构上的操作。比如在修改表，修改列，新增表，新增列等。它在数据库结构发生变化时执行，我们主要用它来记录数据库的修改过程，以及限制程序员对数据库的修改，比如不允许删除某些指定表等。

3、登录触发器

登录触发器将为响应LOGIN 事件而激发存储过程。与SQL Server 实例建立用户会话时将引发此事件。登录触发器将在登录的身份验证阶段完成之后且用户会话实际建立之前激发。因此，来自触发器内部且通常将到达用户的所有消息（例如错误消息和来自PRINT 语句的消息）会传送到SQL Server 错误日志。如果身份验证失败，将不激发登录触发器。

### 9.视图

视图是指计算机数据库中的视图，是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。

### 10.SQL Select语句完整的执行顺序

1、 from 子句组装来自不同数据源的数据； 2、 where 子句基于指定的条件对记录行进行筛选； 3、 group by 子句将数据划分为多个分组； 4、使用聚集函数进行计算； 5、使用 having 子句筛选分组； 6、计算所有的表达式； 7、 select 的字段； 8、使用 order by 对结果集进行排序。

### 11.SQL 之聚合函数

聚合函数是对一组值进行计算并返回单一的值的函数，它经常与select 语句中的 group by 子句一同使用。

a. avg()：返回的是指定组中的平均值，空值被忽略。 b. count()：返回的是指定组中的项目个数。 c. max()：返回指定数据中的最大值。 d. min()：返回指定数据中的最小值。 e. sum()：返回指定数据的和，只能用于数字列，空值忽略。 f. group by()：对数据进行分组，对执行完 group by 之后的组进行聚合函数的运算，计算每一组的值。

最后用 having 去掉不符合条件的组， having 子句中的每一个元素必须出现在 select 列表中（只针对于 mysql ）。

### 12.SQL 之连接查询（左连接和右连接的区别）

- 外连接
  - 左连接（左外连接）：以左表作为基准进行查询，左表数据会全部显示出来，右表如果和左表匹配的数据则显示相应字段的数据，如果不匹配则显示为 null 。
  - 右连接（右外连接）：以右表作为基准进行查询，右表数据会全部显示出来，左表如果和右表匹配的数据则显示相应字段的数据，如果不匹配则显示为 null 。
  - 全连接：先以左表进行左外连接，再以右表进行右外连接。
- 内连接
  - 显示表之间有连接匹配的所有行。

### 13.ACID 是什么？

Atomicity（原子性）：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 14.MySQL的执行过程

1.当我们请求mysql服务器的时候,MySQL前端会有一个监听,请求到了之后,服务器得到相关的SQL语句,执行之前(虚线部分为执行),还会做权限的判断 2.通过权限之后,SQL就到MySQL内部,他会在查询缓存中,看该SQL有没有执行过,如果有查询过,则把缓存结果返回,说明在MySQL内部,也有一个查询缓存.但是这个查询缓存,默认是不开启的,这个查询缓存,和我们的Hibernate，Mybatis的查询缓存是一样的,因为查询缓存要求SQL和参数都要一样,所以这个命中率是非常低的（没什么卵用的意思）。 3.如果我们没有开启查询缓存,或者缓存中没有找到对应的结果,那么就到了解析器,解析器主要对SQL语法进行解析 4.解析结束后就变成一颗解析树,这个解析树其实在Hibernate里面也是有的,大家回忆一下,在以前做过Hibernate项目的时候,是不是有个一个antlr.jar。这个就是专门做语法解析的工具.因为在Hibernate里面有HQL，它就是通过这个工具转换成SQL的,我们编程语言之所以有很多规范、语法,其实就是为了便于这个解析器解析,这个学过编译原理的应该知道. 5.得到解析树之后,不能马上执行,这还需要对这棵树进行预处理,也就是说,这棵树,我没有经过任何优化的树,预处理器会这这棵树进行一些预处理,比如常量放在什么地方,如果有计算的东西,把计算的结果算出来等等... 6.预处理完毕之后,此时得到一棵比较规范的树,这棵树就是要拿去马上做执行的树,比起之前的那棵树,这棵得到了一些优化 7.查询优化器，是MySQL里面最关键的东西,我们写任何一条SQL,比如SELECT * FROM USER WHERE USERNAME = toby AND PASSWORD = 1,它会怎么去执行?它是先执行username = toby还是password= 1?每一条SQL的执行顺序查询优化器就是根据MySQL对数据统计表的一些信息,比如索引,比如表一共有多少数据,MySQL都是有缓存起来的,在真正执行SQL之前,他会根据自己的这些数据,进行一个综合的判定,判断这一次在多种执行方式里面,到底选哪一种执行方式,可能运行的最快.这一步是MySQL性能中,最关键的核心点,也是我们的优化原则.我们平时所讲的优化SQL,其实说白了,就是想让查询优化器,按照我们的想法,帮我们选择最优的执行方案,因为我们比MySQL更懂我们的数据.MySQL看数据,仅仅只是自己收集到的信息,这些信息可能是不准确的,MySQL根据这些信息选了一个它自认为最优的方案,但是这个方案可能和我们想象的不一样. 8.这里的查询执行计划,也就是MySQL查询中的执行计划,比如要先执行username = toby还是password= 1 9.这个执行计划会传给查询执行引擎,执行引擎选择存储引擎来执行这一份传过来的计划,到磁盘中的文件中去查询,这个时候重点来了,影响这个查询性能最根本的原因是什么?就是硬盘的机械运动,也就是我们平时熟悉的IO,所以一条查询语句是快还是慢,就是根据这个时间的IO来确定的.那怎么执行IO又是什么来确定的?就是传过来的这一份执行计划. 10.如果开了查询缓存,则返回结果给客户端,并且查询缓存也放一份。

### 15.百万级别或以上的数据如何删除

关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。 所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。

1. 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
2. 然后删除其中无用数据（此过程需要不到两分钟）
3. 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
4. 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

### 16.表的设计及优化

优化①：创建规范化表，消除数据冗余

第一范式：属性(字段)的原子性约束，要求属性具有原子性，不可再分割； 第二范式：记录的惟一性约束，要求记录有惟一标识，每条记录需要有一个属性来做为实体的唯一标识。 第三范式：属性(字段)冗余性的约束，即任何字段不能由其他字段派生出来，在通俗点就是：主键没有直接关系的数据列必须消除(消除的办法就是再创建一个表来存放他们，当然外键除外)

优化②：合适的字段属性

0）数值型字段的比较比字符串的比较效率高得多，所以字段类型尽量使用最小、最简单的数据类型。如IP地址可以使用int类型。 1）建议不要使用DOUBLE，不仅仅只是存储长度的问题，同时还会存在精确性的问题。 2）对于整数的存储，在数据量较大的情况下，建议区分开 TINYINT / INT / BIGINT 的选择 3）char是固定长度，所以它的处理速度比varchar快得多，但缺点是浪费存储空间，不能在行尾保存空格。在MySQL中，MyISAM建议使用固定长度代替可变长度列；InnoDB建议使用varchar类型，因为在InnoDB中，内部行存储格式没有区分固定长度和可变长度。

4） 尽量不要允许NULL，除非必要，可以用NOT NULL+DEFAULT代替。 5）text与blob区别：blob保存二进制数据；text保存字符数据，有字符集。text和blob不能有默认值。 实际场景：text与blob主要区别是text用来保存字符数据（如文章，日记等），blob用来保存二进制数据（如照片等）。blob与text在执行了大量删除操作时候，有性能问题（产生大量的“空洞“），为提高性能建议定期optimize table 对这类表进行碎片整理。 6） 自增字段要慎用，不利于数据迁移 7）强烈反对在数据库中存放 LOB 类型数据，虽然数据库提供了这样的功能，但这不是他所擅长的，我们更应该让合适的工具做他擅长的事情，才能将其发挥到极致。（反正我么碰到过LOB类型数据） 8）尽量将表字段定义为NOT NULL约束，这时由于在MySQL中含有空值的列很难进行查询优化，NULL值会使索引以及索引的统计信息变得很复杂，可以使用0或者空字符串来代替。 9）尽量使用TIMESTAMP类型，因为其存储空间只需要 DATETIME 类型的一半，且日期类型中只有它能够和实际时区相对应。对于只需要精确到某一天的数据类型，建议使用DATE类型，因为他的存储空间只需要3个字节，比TIMESTAMP还少。（真的是技术文，欢迎补充）

优化③：索引 索引是一个表优化的重要指标，在表优化中占有极其重要的成分

优化④：表的拆分（大表拆小表） 1、垂直拆分（其实就是列的拆分将原来的一个有很多列的表拆分成多张表） 注意：垂直拆分应该在数据表设计之初就执行的步骤，然后查询的时候用jion关键起来即可; 通常我们按以下原则进行垂直拆分:

1. 把不常用的字段单独放在一张表;
2. 把text，blob等大字段拆分出来放在附表中;
3. 经常组合查询的列放在一张表中;

缺点也很明显，需要使用冗余字段，而且需要join操作。 2、水平拆分（ 如果你发现某个表的记录太多，例如超过一千万条，则要对该表进行水平分割。水平分割的做法是，以该表主键的某个值为界线，将该表的记录水平分割为两个表。）

### 17.MYSQL储存过程

储存过程是一个可编程的函数，它在数据库中创建并保存。它可以有SQL语句和一些特殊的控制结构组成。当希望在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟。它允许控制数据的访问方式。存储过程通常有以下优点：

1)存储过程能实现较快的执行速度。

如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。

2)存储过程允许标准组件式编程

3)存储过程可以用流控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。

4)存储过程可被作为一种安全机制来充分利用。 系统管理员通过执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。

5)存储过程能过减少网络流量。 针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织程存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大增加了网络流量并降低了网络负载。

存储函数

封装一段sql代码，完成一种特定的功能，必须返回结果。其余特性基本跟存储过程相同。

存储函数与存储过程的区别

1） 存储函数有且只有一个返回值，而存储过程不能有返回值。就是说能不能使用return。（函数可返回返回值或者表对象，绝对不能返回结果集）

2） 函数只能有输入参数，而且不能带in, 而存储过程可以有多个in,out,inout参数。 3） 存储过程中的语句功能更强大，存储过程可以实现很复杂的业务逻辑，而函数有很多限制，如不能在函数中使用insert,update,delete,create等语句；存储函数只完成查询的工作，可接受输入参数并返回一个结果，也就是函数实现的功能针对性比较强。比如：工期计算、价格计算。 4）存储过程可以调用存储函数。但函数不能调用存储过程。 5）存储过程一般是作为一个独立的部分来执行(call调用)。而函数可以作为查询语句的一个部分来调用。

MySQL 创建一个最简单的存储过程

```html
drop procedure if exists pr_add;//如果存储过程pr_add存在，则删去，如果不存在，则什么事也不做
//计算两个数之和
create PROCEDURE pr_add
(
in a int,
in b int
)
begin
declare c int,//declare是申明变量的关键字
if a is null THEN
    set a =1;
end if;
if b is null THEN
    set b =1;
end if;
set c=a+bl
select c as sum;
end;

call pr_add(null,null);//调用存储过程
```

MySQL 存储过程特点

创建 MySQL 存储过程的简单语法为：

```html
create procedure 存储过程名字()
(
[in|out|inout] 参数 datatype
)
begin
MySQL 语句;
end;
```

MySQL 存储过程参数如果不显式指定“in”、“out”、“inout”，则默认为“in”。习惯上，对于是“in” 的参数，我们都不会显式指定。 1 MySQL 存储过程名字后面的“()”是必须的，即使没有一个参数，也需要“()” 2 MySQL 存储过程参数，不能在参数名称前加“@”，如：“@a int”。下面的创建存储过程语法在 MySQL中是错误的（在 SQL Server 中是正确的）。 MySQL 存储过程中的变量，不需要在变量名字前加“@”，虽然 MySQL 客户端用户变量要加个“@”。

3 MySQL 存储过程的参数不能指定默认值。 4 MySQL 存储过程不需要在 procedure body 前面加 “as”。而 SQL Server 存储过程必须加 “as” 关键字。

5 如果 MySQL 存储过程中包含多条 MySQL 语句，则需要 begin end 关键字。

6 MySQL 存储过程中的每条语句的末尾，都要加上分号 “;”

7 不能在 MySQL 存储过程中使用 “return” 关键字。

8 调用 MySQL 存储过程时候，需要在过程名字后面加“()”，即使没有一个参数，也需要“()”，调用out及inout参数格式为@arguments_name形式。

9 因为 MySQL 存储过程参数没有默认值，所以在调用 MySQL 存储过程时候，不能省略参数。可以用null 来替代。

### 18.视图

什么是视图？

视图是基于 SQL 语句的结果集的可视化的表。 视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。视图并不在数据库中以存储的数据值集形式存在，而是存在于实际引用的数据库表中，视图的构成可以是单表查询，多表联合查询，分组查询以及计算(表达式)查询等。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。

视图的优点

a、简化查询语句（视图机制使用户可以将注意力集中在所关心地数据上。如果这些数据不是直接来自基本表，则可以通过定义视图，使数据库看起来结构简单、清晰，并且可以简化用户的的数据查询操作。） b、可以进行权限控制 把表的权限封闭，但是开放相应的视图权限，视图里只开放部分数据列等。 c、大数据表分表的时候，比如某张表的数据有100万条，那么可以将这张表分成四个视图。按照对id取余计算 d、用户能以多种角度看待同一数据： 使不同的用户以不同的方式看待同一数据，当许多不同种类的用户共享同一个数据库时，这种灵活性是非常必要的。 e、对重构数据库提供了一定程度的逻辑独立性： 视图可以使应用程序和数据库表在一定程度上独立。

视图的缺点

1）性能差： 把视图查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，sql server也要把它变成一个复杂的结合体，需要花费一定的时间。 2）修改限制： 当用户试图修改试图的某些信息时，数据库必须把它转化为对基本表的某些信息的修改，对于简单的试图来说，这是很方便的，但是，对于比较复杂的试图，可能是不可修改的。

视图使用场景

1） 需要权限控制的时候。 2）如果某个查询结果出现的非常频繁，就是要经常拿这个查询结果来做子查询，使用视图会更加方便。 3）关键信息来源于多个复杂关联表，可以创建视图提取我们需要的信息，简化操作；

视图的分类

1）关系视图： 它属于数据库对象的一种，也就是最常见的一种关联查询； 2）内嵌视图： 它不属于任何用户，也不是对象，创建方式与普通视图完全不同，不具有可复用性，不能通过数据字典获取数据； 3）对象视图： 它是基于表对象类型的视图，特性是继承、封装等可根据需要构建对象类型封装复杂查询(官方:为了迎合对象类型而重建数据表是不实现的)； 4）物化视图： 它主要用于数据库的容灾(备份)，实体化的视图可存储和查询，通过DBLink连接在主数据库物化视图中复制，当主库异常备库接管实现容灾；

视图的使用

1、创建视图

1. create or replace view v_test asselect * fromuser; 加上OR REPLACE表示该语句还能替换已有的视图 2、调取视图
2. select * from v_test; 3、修改视图
3. alter view v_test asselect * from user1; 4、删除视图
4. drop view if exists v_test; 5、查看视图
5. show tables; 视图放在information_schema数据库下的views表里
6. 查看视图的定义 show table status from companys like'v_test';

增删改最终都是修改到基础表。且视图中虽然可以更新数据，但是有很多的限制。一般情况下，最好将视图作为查询数据的虚拟表，而不要通过视图更新数据。因为，使用视图更新数据时，如果没有全面考虑在视图中更新数据的限制，就可能会造成数据更新失败。

视图的算法

a、Merge：合并的执行方式，每当执行的时候，先将我们的视图的sql语句与外部查询视图的sql语句，混合在一起，最终执行。 b、Temptable：临时表模式，每当查询的时候，将视图所使用的select语句生成一个结果的临时表，再在当当前临时表内进行查询。

视图使用注意点

（1）修改操作时要非常非常小心，不然不经意间你已经修改了基本表里的多条数据； （2）视图中的查询语句性能要调到最优； （3）虽说上面讲到，视图有些是可以修改的。但是更多的是禁止修改视图。 对于可更新的视图，在视图中的行和基表中的行之间必须具有一对一的关系或者特殊的没有约束的一对多字段。还有一些特定的其他结构，这类结构会使得视图不可更新。 不可更改的情况如下：视图中含有以下的都不可被修改了。 （一）聚合函数（SUM(), MIN(), MAX(), COUNT()等）。 （二）DISTINCT。如下错误。 （三）GROUP BY （四）HAVING （五）UNION或UNION ALL （六）位于选择列表中的子查询 （八）FROM子句中的不可更新视图 （九）WHERE子句中的子查询，引用FROM子句中的表。

（十）ALGORITHM = TEMPTABLE（使用临时表总会使视图成为不可更新的）。

### 19.sql_mode

sql_mode 决定了哪些 SQL 语句可以被执行，哪些不能被执行

1、 ONLY_FULL_GROUP_BY

对于 `GROUP BY` 聚合操作，如果 `SELECT中` 的列，没有在 `GROUP BY` 中出现，那么将认为这个SQL 是不合法的

2、 STRICT_TRANS_TABLES

在该模式下，如果一个值不能插入到一个事务表中，则中断当前的操作，对非事务表不做任何限制

3、 NO_ZERO_IN_DATE

在严格模式，不接受月或日部分为0的日期。 如果使用 `IGNORE` 选项或非严格模式，则可以插入类似的日期 '0000-00-00'，但会生成警告

4、 NO_ZERO_DATE

在严格模式，不要将 '0000-00-00' 做为合法日期 如果使用 `IGNORE` 选项或非严格模式，则可以插入类似的日期 '0000-00-00'，但会生成警告

5、 ERROR_FOR_DIVISION_BY_ZERO

严格模式下，在 `INSERT` 或 `UPDATE` 过程中，如果除零(或 `MOD(X，0)` )，则产生错误(否则为警告) 如果未给出该模式，除零时MySQL返回 `NULL` 如果设置了 `INSERT IGNORE` 或 `UPDATE IGNORE` 中，MySQL 则生成除零警告，但操作结果为NULL

6、 NO_AUTO_CREATE_USER

防止 `GRANT` 自动创建新用户 但如果通知指定了密码，则有可能会创建新用户

7、 NO_ENGINE_SUBSTITUTION

如果需要的存储引擎被禁用或未编译，那么抛出错误 不设置此值时，会默认的存储引擎替代，并抛出一个异常

在 5.0 以后的版本中新增加了三个特殊的 sql_mode

- ANSI：宽松模式，对于大部分的 sql 语句按照古老的模式执行成功，但会抛出一个警告
- TRADITIONAL：算是标准模式吧，这种模式下有些 sql 语句就会直接出错
- STRICT_TRANS_TABLES：严格模式，这种模式下只要一条 SQL 语句出错就会直接中断事务的执行

### 20.大小写敏感

只要在创建表的时候指定collate为utf8_bin，就可以实现大小写敏感，如果建表时未指定，则可修改字段的校对规则，也可以实现大小写敏感。

### 21.命令

USE 命令

USE 命令用于选择要操作的 MySQL 数据库 使用该命令后所有 MySQL 命令都只针对该数据库

```html
USE **数据库名**
```

SHOW DATABASE 命令

SHOW DATABASES 命令用于列出 MySQL 数据库管理系统的数据库

SHOW TABLES 命令

SHOW TABLES 命令用于列出指定数据库中的所有表 使用该命令前需要使用 USE 命令来选择要操作的 数据库

SHOW COLUMNS FROM 命令

SHOW COLUMNS FROM table_name 命令用于显示数据表的属性，属性类型，主键信息 ，是否为NULL ，默认值等等信息

SHOW INDEX FROM 命令

SHOW INDEX FROM table_name 命令用于显示数据表的详细索引信息，包括 PRIMARY KEY(主键)

SHOW TABLE STATUS LIKE LIKE 命令

SHOW TABLE STATUS LIKE [FROM db_name] [LIKE 'pattern'] 命令用于输出 MySQL 数据库管理系统的性能及统计信息

使用 mysql 命令连接

```html
mysql -u [用户名] -p [密码,可以不输入]
```

使用 mysqladmin 创建数据库

可以使用 mysqladmin 命令来创建数据库

```html
mysqladmin [OPTIONS] command [command-option] command ...
```

使用 CREATE DATABASE 语句创建数据库

使用 CREATE DATABASE 语句创建数据库前先要连接到 MySQL 数据库服务器

使用 mysqladmin drop 命令删除数据库

```html
mysqladmin [OPTIONS] command [command-option] command ...
```

使用 DROP DATABASE 语句创建数据库

使用 DROP DATABASE 语句删除数据库前先要连接到 MySQL 数据库服务器

切换 MySQL 数据库

使用 USE SQL命令来选择指定的数据库

DROP TABLE SQL 语句

使用 DROP TABLE SQL 语句删除数据表

```html
DROP TABLE table_name ;
```

INSERT INTO SQL

使用 INSERT INTO SQL 语句往表中插入数据的语法格式如下

```html
INSERT INTO table_name ( field1, field2,...fieldN ) VALUES ( value1, value2,...valueN );
```

SELECT FROM SQL

使用 SELECT FROM SQL 语句查询表中数据的语法格式如下

```html
SELECT column_name,column_name
FROM table_name
[WHERE Clause]
[LIMIT N,M]
```

UPDATE SQL

UPDATE SQL 语句修改数据的通用语法格式如下

```html
UPDATE table_name SET field1=new-value1, field2=new-value2 [WHERE Clause]
```

DELETE FROM 语句

DELETE FROM SQL 语句删除数据的通用语法格式如下

```html
DELETE FROM table_name [WHERE Clause]
```

ORDER BY 子句语法

SQL SELECT 中语句使用 ORDER BY 子句对查询数据进行排序的语法格式如下

```html
SELECT field1, field2,...fieldN table_name1, table_name2...
ORDER BY field1, [field2...] [ASC [DESC]]
```

GROUP BY

SQL SELECT 中语句使用 GROUP BY 子句对查询数据进行分组的语法格式如下

```html
SELECT column_name, function(column_name)
FROM table_name
WHERE column_name operator value
GROUP BY column_name;
```

删除表字段

ALTER TABLE tablename DROP fieldname 命令可以用来删除表中的某个字段

如果表中只剩余一个字段，那么无法使用 DROP 来删除字段

```html
ALTER TABLE tbl_language DROP founded_at;
```

添加表字段

ALTER TABLE tablename ADD field 命令可以给一张表添加字段

```html
ALTER TABLE tbl_language ADD founded_at DATE;
```

指定位置添加字段

默认情况下字段会添加到数据表字段的末尾 如果需要指定新增字段的位置，可以使用 MySQL 提供的关键字 FIRST (设定位第一列)， AFTER 字段名 (设定位于某个字段之后)

```html
ALTER TABLE tbl_language ADD lid int FIRST;
ALTER TABLE tbl_language ADD llid int AFTER url;
ALTER TABLE tbl_language ADD lllid int AFTER lid;
```

修改字段类型和其它字段元数据

ALTER TABLE tablename MODIFY field 命令可以用来修改字段类型

```html
ALTER TABLE tbl_language MODIFY url VARCHAR(255);
```

重命名字段

ALTER TABLE tablename MODIFY old_field new_field 命令可以用来重命名字段 和 MODIFY 不一样， CHANGE 后面紧跟着的是要修改的字段名，然后指定新字段名及类型

```html
ALTER TABLE tbl_language CHANGE url home VARCHAR(128);
```

修改字段默认值

ALTER TABLE tablename ALTER field SET DEFAULT default_value 可以修改字段的默认值

```html
ALTER TABLE tbl_language ALTER url SET DEFAULT '';
```

删除字段默认值

ALTER TABLE tablename ALTER field DROP DEFAULT 可以删除字段的默认值

```html
ALTER TABLE tbl_language ALTER url DROP DEFAULT;
```

修改数据表引擎

ALTER TABLE tablename ENGINE = engine 可以修改表的类型(数据库引擎)

```html
ALTER TABLE tbl_language ENGINE = MYISAM;
```

修改表名

如果需要修改数据表的名称，可以在 ALTER TABLE 语句中使用 RENAME 子句来实现

```html
ALTER TABLE tbl_language RENAME TO tbl_lang;
```

显示索引信息

SHOW INDEX FROM tablename; 命令可以列出某个表中的相关的索引信息

```html
SHOW INDEX FROM tbl_language;
```

普通索引

1、 创建索引

```html
CREATE INDEX indexName ON mytable(username(length));
```

2、 修改表结构(添加索引)

```html
ALTER table tableName ADD INDEX indexName(columnName)
```

3、 创建表的时候直接指定

删除索引

```html
DROP INDEX [indexName] ON tablename;
```

唯一索引

1、 创建索引

```html
CREATE UNIQUE INDEX indexName ON mytable(username(length))
```

2、 修改表结构

```html
ALTER table mytable ADD UNIQUE [indexName] (username(length))
```

3、 创建表的时候直接指定

使用 ALTER 命令添加索引

1、 ALTER TABLE tbl_name ADD PRIMARY KEY (column_list);

添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。

2、 ALTER TABLE tbl_name ADD UNIQUE index_name (column_list);

创建索引的值必须是唯一的 ( 除了 NULL 外，NULL 可能会出现多次)

3、 ALTER TABLE tbl_name ADD INDEX index_name (column_list);

添加普通索引，索引值可出现多次

4、 ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list);

指定了索引为 FULLTEXT ，用于全文索引

创建临时表

命令 CREATE TEMPORARY TABLE tablename 命令用来创建临时表

复制表

1、 使用 SHOW CREATE TABLE 命令获取创建数据表 ( CREATE TABLE ) 语句，该语句包含了原数据表的结构，索引等 2、 复制 1 中的 SQL 语句，修改数据表名，并执行 SQL 语句，完整的克隆出一个数据表结构一模一样的表 3、 如果需要复制表的内容，可以使用 INSERT INTO ... SELECT 语句来实现

SELECT VERSION()

返回服务器版本信息

SELECT DATABASE()

返回当前数据库名 (或者返回空)

SELECT USER()

返回当前用户名

SHOW STATUS

返回服务器状态

SHOW VARIABLES

返回服务器配置变量

INSERT IGNORE INTO

INSERT IGNORE INTO 与 INSERT INTO 的区别就是 INSERT IGNORE 会忽略数据库中已经存在的数据 如果数据库没有数据，就插入新的数据，如果有数据的话就跳过这条数据 这样就可以保留数据库中已经存在数据，达到在间隙中插入数据的目的

REPLACE INTO

设置了记录的主键或者唯一性后, INSERT IGNORE INTO 插入数据时如果插入重复数据，将不返回错误，只以警告形式返回 REPLACE INTO 如果存在相同的记录，则先删除掉，再插入新记录

统计重复数据

当表中存在重复数据时，可以使用 GROUP BY 子句对重复数据的关键字段做分组，然后使用COUNT(*) 来统计重复了多少条

```html
SELECT COUNT(*) as cnt, name FROM tbl_language GROUP BY name HAVING cnt > 1;
```

过滤重复数据

如果需要读取不重复的数据，可以在 SELECT 语句中使用 DISTINCT 关键字来过滤重复数据

```html
SELECT DISTINCT name, url, founded_at FROM tbl_language;
```

### 22.可以使用多少列创建索引？

任何标准表最多可以创建16 个索引列。

### 23.NOW（）和CURRENT_DATE（）有什么区别？

NOW（） 命令用于显示当前年份，月份， 日期，小时， 分钟和秒。

CURRENT_DATE（） 仅显示当前年份， 月份和日期

### 24.简单说一说drop、delete与truncate的区别

SQL中的drop、delete、truncate都表示删除，但是三者有一些差别 delete和truncate只删除表的数据不删除表的结构 速度,一般来说: drop> truncate >delete delete语句是dml,这个操作会放到rollback segement中,事务提交之后才生效; 如果有相应的trigger,执行的时候将被触发. truncate,drop是ddl, 操作立即生效,原数据不放到rollbacksegment中,不能回滚. 操作不触发trigger

### 25.什么是通用 SQL 函数？

1、CONCAT(A, B) – 连接两个字符串值以创建单个字符串输出。通常用于将两个或多个字段合并为一个字段。 SELECT*FROM LIMIT 0,50; 2、FORMAT(X, D)- 格式化数字 X 到 D 有效数字。 3、CURRDATE(), CURRTIME()- 返回当前日期或时间。 4、NOW（） – 将当前日期和时间作为一个值返回。 5、MONTH（），DAY（），YEAR（），WEEK（），WEEKDAY（） – 从日期值中提取给定数据。 6、HOUR（），MINUTE（），SECOND（） – 从时间值中提取给定数据。 7、DATEDIFF（A，B） – 确定两个日期之间的差异，通常用于计算年龄 8、SUBTIMES（A，B） – 确定两次之间的差异。 9、FROMDAYS（INT） – 将整数天数转换为日期值

### 26.MySQL 有关权限的表都有哪几个？

MySQL 服务器通过权限表来控制用户对数据库的访问，权限表存放在 MySQL 数据库里，由 MySQL_install_db 脚本初始化。这些权限表分别 user，db，table_priv，columns_priv 和 host。

### 27.MySQL 数据库作发布系统的存储，一天五万条以上的增量，预计运维三年,怎么优化？

1、设计良好的数据库结构，允许部分数据冗余，尽量避免 join 查询，提高效率。 2、选择合适的表字段数据类型和存储引擎，适当的添加索引。 3、MySQL 库主从读写分离。 4、找规律分表，减少单表中的数据量提高查询速度。 5、添加缓存机制，比如 memcached，apc 等。 6、不经常改动的页面，生成静态页面。 7、书写高效率的 SQL。比如 SELECT * FROM TABEL 改为 SELECT field_1,field_2, field_3 FROM TABLE.

### 28.、SQL 注入漏洞产生的原因？如何防止？

SQL 注入产生的原因：程序开发过程中不注意规范书写 sql 语句和对特殊字符进行过滤，导致客户端可以通过全局变量 POST 和 GET 提交 一些 sql 语句正常执行。 防止 SQL 注入的方式： 开启配置文件中的 magic_quotes_gpc 和 magic_quotes_runtime 设置 执行 sql 语句时使用 addslashes 进行 sql 语句转换 Sql 语句书写尽量不要省略双引号和单引号。 过滤掉 sql 语句中的一些关键词：update、insert、delete、select、 * 。 提高数据库表和字段的命名技巧，对一些重要的字段根据程序的特点命名，取不易被猜到的。

### 29.完整性约束包括哪些？

数据完整性(Data Integrity)是指数据的精确(Accuracy)和可靠性(Reliability)。 分为以下四类： 1、实体完整性：规定表的每一行在表中是惟一的实体。 2、域完整性：是指表中的列必须满足某种特定的数据类型约束，其中约束又包括取值范围、精度等规定。 3、参照完整性：是指两个表的主关键字和外关键字的数据应一致，保证了表之间的数据的一致性，防止了数据丢失或无意义的数据在数据库中扩散。 4、用户定义的完整性：不同的关系数据库系统根据其应用环境的不同，往往还需要一些特殊的约束条件。用户定义的完整性即是针对某个特定关系数据库的约束条件，它反映某一具体应用必须满足的语义要求。与表有关的约束：包括列约束(NOT NULL（非空约束）)和表约束(PRIMARY KEY、foreign key、check、UNIQUE) 。

### 30.什么是时间戳

时间戳就是在数据库表中单独加一列时间戳，比如“TimeStamp”， 每次读出来的时候，把该字段也读出来，当写回去的时候，把该字段加1，提交之前 ，跟数据库的该字段比较一次，如果比数据库的值大的话，就允许保存，否则不允许保存，这种处理方法虽然不使用数据库系统提供的锁机制，但是这种方法可以大大提高数据库处理的并发量，以上悲观锁所说的加“锁”，其实分为几种锁，分别是： 排它锁（写锁）和共享锁（读锁） 。

### 31.数据库实现缓存最终一致性的一些方法

缓存是什么

存储的速度是有区别的。缓存就是把低速存储的结果，临时保存在高速存储的技术。

为什么需要缓存

存储如mysql通常支持完整的ACID特性，因为可靠性，持久性等因素，性能普遍不高，高并发的查询会给mysql带来压力，造成数据库系统的不稳定。同时也容易产生延迟。根据局部性原理，80%请求会落到20%的热点数据上，在读多写少场景，增加一层缓存非常有助提升系统吞吐量和健壮性。

存在问题

存储的数据随着时间可能会发生变化，而缓存中的数据就会不一致。具体能容忍的不一致时间，需要具体业务具体分析，但是通常的业务，都需要做到最终一致。

redis作为mysql缓存

通常的开发模式中，都会使用mysql作为存储，而redis作为缓存，加速和保护mysql。但是，当mysql数据更新之后，redis怎么保持同步呢。

强一致性同步成本太高，如果追求强一致，那么没必要用缓存了，直接用mysql即可。通常考虑的，都是最终一致性。

方案一

通过key的过期时间，mysql更新时，redis不更新。这种方式实现简单，但不一致的时间会很长。如果读请求非常频繁，且过期时间比较长，则会产生很多长期的脏数据。

优点：

- 开发成本低，易于实现；
- 管理成本低，出问题的概率会比较小。

不足

- 完全依赖过期时间，时间太短容易缓存频繁失效，太长容易有长时间更新延迟（不一致）

方案二

在方案一的基础上扩展，通过key的过期时间兜底，并且，在更新mysql时，同时更新redis。

优点

- 相对方案一，更新延迟更小。

不足

- 如果更新mysql成功，更新redis却失败，就退化到了方案一；
- 在高并发场景，业务server需要和mysql,redis同时进行连接。这样是损耗双倍的连接资源，容易造成连接数过多的问题。

方案三

针对方案二的同步写redis进行优化，增加消息队列，将redis更新操作交给kafka，由消息队列保证可靠性，再搭建一个消费服务，来异步更新redis。

优点

- 消息队列可以用一个句柄，很多消息队列客户端还支持本地缓存发送，有效解决了方案二连接数过多的问题；
- 使用消息队列，实现了逻辑上的解耦；
- 消息队列本身具有可靠性，通过手动提交等手段，可以至少一次消费到redis。

不足

- 依旧解决不了时序性问题，如果多台业务服务器分别处理针对同一行数据的两条请求，举个栗子，a = 1；a = 5;，如果mysql中是第一条先执行，而进入kafka的顺序是第二条先执行，那么数据就会产生不一致。
- 引入了消息队列，同时要增加服务消费消息，成本较高。

方案四

通过订阅binlog来更新redis，把我们搭建的消费服务，作为mysql的一个slave，订阅binlog，解析出更新内容，再更新到redis。

优点

- 在mysql压力不大情况下，延迟较低；
- 和业务完全解耦；
- 解决了时序性问题。

缺点

- 要单独搭建一个同步服务，并且引入binlog同步机制，成本较大。

总结

方案选型

1. 首先确认产品上对延迟性的要求，如果要求极高，且数据有可能变化，别用缓存。
2. 通常来说，方案1就够了，笔者咨询过4，5个团队，基本都是用方案1，因为能用缓存方案，通常是读多写少场景，同时业务上对延迟具有一定的包容性。方案1没有开发成本，其实比较实用。
3. 如果想增加更新时的即时性，就选择方案2，不过没必要做重试保证之类的。
4. 方案3，方案4针对于对延时要求比较高业务，一个是推模式，一个是拉模式，而方案4具备更强的可靠性，既然都愿意花功夫做处理消息的逻辑，不如一步到位，用方案4。

### 32.SQL 约束有哪几种？

- NOT NULL: 用于控制字段的内容一定不能为空（NULL）。
- UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。
- PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。
- FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。
- CHECK: 用于控制字段的值范围。

### 33.MySQL存储引擎中的MyISAM和InnoDB区别详解

在MySQL 5.5之前，MyISAM是mysql的默认数据库引擎，其由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然MyISAM性能极佳，但却有一个显著的缺点： **不支持事务处理**。不过，MySQL也导入了另一种数据库引擎InnoDB，以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。

InnoDB是MySQL的数据库引擎之一，其由Innobase oy公司所开发，2006年五月由甲骨文公司并购。与传统的ISAM、MyISAM相比，**InnoDB的最大特色就是支持ACID兼容的事务功能**，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。具体地，MyISAM与InnoDB作为MySQL的两大存储引擎的差异主要包括：

- **存储结构**：每个MyISAM在磁盘上存储成三个文件：第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义，数据文件的扩展名为.MYD (MYData)，索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。
- **存储空间**：MyISAM可被压缩，占据的存储空间较小，支持静态表、动态表、压缩表三种不同的存储格式。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。
- **可移植性、备份及恢复**：MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便，同时在备份和恢复时也可单独针对某个表进行操作。InnoDB免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。
- **事务支持**：MyISAM强调的是性能，每次查询具有原子性，其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB提供事务、外键等高级数据库功能，具有事务提交、回滚和崩溃修复能力。
- **AUTO_INCREMENT**：在MyISAM中，可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，它可以根据前面几列进行排序后递增。InnoDB中必须包含只有该字段的索引，并且引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。
- **表锁差异**：MyISAM只支持表级锁，用户在操作MyISAM表时，select、update、delete和insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB支持事务和行级锁。行锁大幅度提高了多用户并发操作的新能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。
- **全文索引**：MyISAM支持 FULLTEXT类型的全文索引；InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。
- **表主键**：MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。
- **表的具体行数**：MyISAM保存表的总行数，select count() from table;会直接取出出该值；而InnoDB没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。
- **CURD操作**：在MyISAM中，如果执行大量的SELECT，MyISAM是更好的选择。对于InnoDB，如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。
- **外键**：MyISAM不支持外键，而InnoDB支持外键。

通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁、外键等等。尤其在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，必须需要注意的是，任何一种表都不是万能的，合适的才是最好的，才能最大的发挥MySQL的性能优势。如果是不复杂的、非关键的Web应用，还是可以继续考虑MyISAM的，这个具体情况具体考虑。

### 34.主键、自增主键、主键索引与唯一索引概念区别

1. 主键：指字段 **唯一、不为空值** 的列；
2. 主键索引：指的就是主键，主键是索引的一种，是唯一索引的特殊类型。创建主键的时候，数据库默认会为主键创建一个唯一索引；
3. 自增主键：字段类型为数字、自增、并且是主键；
4. 唯一索引：索引列的值必须唯一，但允许有空值。**主键是唯一索引，这样说没错；但反过来说，唯一索引也是主键就错误了，因为唯一索引允许空值，主键不允许有空值，所以不能说唯一索引也是主键。**

### 35.**SQL 的 select 语句完整的执行顺序？**

1、**from** 子句组装来自不同数据源的数据； 2、**where** 子句基于指定的条件对记录行进行筛选； 3、**group** **by** 子句将数据划分为多个分组； 4、使用**聚集函数**进行计算； 5、使用 **having** 子句筛选分组； 6、计算所有的表达式； 7、**select** 的字段； 8、使用 **order by** 对结果集进行排序。

### 36.**什么是sql注入？如何防止sql注入？**

**sql注入** 通过在 Web 表单中输入（恶意）SQL 语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行 SQL 语句。 举例：当执行的 sql 为 `select * from user where username = "admin" or "a"="a"`时，sql 语句恒成立，参数 admin 毫无意义。

**防止 sql 注入的方式：**

1. 预编译语句：如，select * from user where username = ？，sql 语句语义不会发生改变，sql 语句中变量用？表示，即使传递参数时为"admin or 'a'= 'a'"，也会把这整体当做一个字符创去查询。
2. Mybatis 框架中的 mapper 方式中的 # 也能很大程度的防止 sql 注入（$无法防止 sql 注入）。

### 37.**Mysql 存储引擎有哪些？**

1.`InnoDB 存储引擎` InnoDB 是事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键，InnoDB 是默认的MySQL引擎。 2.`MyISAM 存储引擎` MyISAM 基于 ISAM 存储引擎，并对其进行扩展。它是在 Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事物。 3.`MEMORY 存储引擎` MEMORY 存储引擎将表中的数据存储到内存中，未查询和引用其他表数据提供快速访问。 4.`NDB 存储引擎` DB 存储引擎是一个集群存储引擎，类似于 Oracle 的 RAC，但它是 Share Nothing 的架构，因此能提供更高级别的高可用性和可扩展性。NDB 的特点是数据全部放在内存中，因此通过主键查找非常快。 关于 NDB，有一个问题需要注意，它的连接(join)操作是在 MySQL 数据库层完成，不是在存储引擎层完成，这意味着，复杂的 join 操作需要巨大的网络开销，查询速度会很慢。 5.`Memory (Heap) 存储引擎` Memory 存储引擎（之前称为 Heap）将表中数据存放在内存中，如果数据库重启或崩溃，数据丢失，因此它非常适合存储临时数据。 6.`Archive 存储引擎` 正如其名称所示，Archive 非常适合存储归档数据，如日志信息。它只支持 INSERT 和 SELECT 操作，其设计的主要目的是提供高速的插入和压缩功能。 7.`Federated 存储引擎` Federated 存储引擎不存放数据，它至少指向一台远程 MySQL 数据库服务器上的表，非常类似于 Oracle 的透明网关。 8.`Maria 存储引擎` Maria 存储引擎是新开发的引擎，其设计目标是用来取代原有的 MyISAM 存储引擎，从而成为 MySQL 默认的存储引擎。

### 38.**MySQL 索引的“使用”要注意什么？**

1.避免在 WHERE 子句中使用`!=`或`<>`操作符，否则将引擎放弃使用索引而进行全表扫描。优化器将无法通过索引来确定将要命中的行数,因此需要搜索该表的所有行。

2.避免在 WHERE 子句中使用 OR 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：`SELECT id FROM t WHERE num = 10 OR num = 20` 。

3.避免在 WHERE 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。

4.避免在 WHERE 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。

5.LIKE 查询，`%` 不能在前，因为无法使用索引。如果需要模糊匹配，可以使用全文索引。

### 39.**为什么mysql建议使用自增主键？**

1、如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引。 如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引。 如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。

2、数据记录本身被存于主索引（一颗B+Tree）的叶子节点上，这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放。 因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）

3、如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。

4、如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过`OPTIMIZE TABLE`来重建表并优化填充页面。

### 40.**mysql rr级别如何解决幻读问题？**

该隔离级别是 MySQL `默认的隔离级别`，在同一个事务里，select 的结果是事务开始时时间点的状态，因此，同样的 select 操作读到的结果会是一致的，但是，会有幻读现象。 MySQL 的 InnoDB 引擎可以通过 `next-key locks` 机制来避免幻读。InnoDB 存储引擎使用三种行锁的算法用来满足相关事务隔离级别的要求: **Record Locks** 该锁为索引记录上的锁，如果表中没有定义索引，InnoDB 会默认为该表创建一个隐藏的聚簇索引，并使用该索引锁定记录。

**Gap Locks** 该锁会锁定一个范围，但是不括记录本身。可以通过修改隔离级别为 `READ COMMITTED` 或者配置 `innodb_locks_unsafe_for_binlog` 参数为 `ON`。

**Next-key Locks** 该锁就是 `Record Locks` 和 `Gap Locks` 的组合，即锁定一个范围并且锁定该记录本身。InnoDB 使用 `Next-key Locks` 解决幻读问题。需要注意的是，如果索引有唯一属性，则 InnnoDB 会自动将 Next-key Locks 降级为 Record Locks。

> 举例：如果一个索引有 1, 3, 5 三个值，则该索引锁定的区间为 (-∞,1], (1,3], (3,5], (5,+ ∞)。

### 41.**MVCC的流程?**

mvcc根据**undo log**来实现 RR级别下，事务中的`第一个SELECT`请求才开始创建`read view`； RC级别下，事务中`每次SELECT`请求都会重新创建`read view`；

ReadView 中是当前活跃的事务 ID 列表，称之为 `m_ids`，其中最小值为 `up_limit_id`，最大值为 `low_limit_id`，事务 ID 是事务开启时 InnoDB 分配的，其大小决定了事务开启的先后顺序，因此我们可以通过 ID 的大小关系来决定版本记录的可见性，具体判断流程如下：

- 如果被访问版本的 `trx_id 小于 m_ids 中的最小值 up_limit_id`，说明生成该版本的事务在 ReadView 生成前就已经提交了，所以该版本可以被当前事务访问。
- 如果被访问版本的 `trx_id 大于 m_ids 列表中的最大值 low_limit_id`，说明生成该版本的事务在生成 ReadView 后才生成，所以该版本不可以被当前事务访问。需要根据 Undo Log 链找到前一个版本，然后根据该版本的 DB_TRX_ID 重新判断可见性。
- 如果被访问版本的 `trx_id 属性值在 m_ids 列表中最大值和最小值之间（包含）`，那就需要判断一下 trx_id 的值是不是在 m_ids 列表中。如果在，说明创建 ReadView 时生成该版本所属事务还是活跃的，因此该版本不可以被访问，需要查找 Undo Log 链得到上一个版本，然后根据该版本的 DB_TRX_ID 再从头计算一次可见性；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。

此时经过一系列判断我们已经得到了这条记录相对 ReadView 来说的可见结果。此时，如果这条记录的 delete_flag 为 true，说明这条记录已被删除，不返回。否则说明此记录可以安全返回给客户端。

### 42.**mysql什么时候会出现数据页预读?**

1、有一个参数是`innodb_read_ahead_threshold`，他的默认值是`56`，意思就是如果顺序的访问了一个区里的多个数据页，访问的数据页的数量超过了这个阈值，此时就会触发预读机制，把下一个相邻区中的所有数据页都加载到缓存里去

2、如果Buffer Pool里缓存了一个区里的13个连续的数据页，而且这些数据页都是比较频繁会被访问的，此时就会 直接触发预读机制，把这个区里的其他的数据页都加载到缓存里去 这个机制是通过参数`innodb_random_read_ahead`来控制的，他默认是OFF，也就是这个规则是关闭的

3、全表扫描

### 43.**mysql有哪些binlog录入格式？**

- `statement`，statement模式下，记录单元为语句。即每一个sql造成的影响会记录。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
- `row`，row级别下，记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
- `mixed`，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。

### 44.**备份计划，mysqldump以及xtranbackup的实现原理，**

（1） 备份计划

（2）备份恢复时间

（3）备份恢复失败如何处理

**原理：**

mysqldump

mysqldump属于逻辑备份。加入--single-transaction选项可以进行一致性备份。后台进程会先设置session的事务隔离级别为RR(SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上--master-data=1的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时binlog的位置(show master status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务。

Xtrabackup:

xtrabackup属于物理备份，直接拷贝表空间文件，同时不断扫描产生的redo日志并保存下来。最后完成innodb的备份后，会做一个 flush engine logs的操作(老版本在有bug，在5.6上不做此操作会丢数据)，确保所有的redo log都已经落盘(涉及到事务的两阶段提交概念，因为xtrabackup并不拷贝binlog，所以必须保证所有的redo log都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是innodb完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的redo就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要flush tables with read lock，把myisam等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。

**备份计划：**

视库的大小来定，一般来说100G内的库，可以考虑使用mysqldump来做，因为mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump备份出来的文件比较小，压缩之后更小)。

100G以上的库，可以考虑用xtranbackup来做，备份速度明显要比mysqldump要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。

**备份恢复时间：**

物理备份恢复快，逻辑备份恢复慢

**备份恢复失败如何处理：**

首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。

### 45、封锁

封锁粒度

MySQL 中提供了两种封锁粒度：行级锁以及表级锁。

应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。

但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。

在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。

封锁类型

\1. 读写锁

- 互斥锁（Exclusive），简写为 X 锁，又称写锁。
- 共享锁（Shared），简写为 S 锁，又称读锁。

有以下两个规定：

- 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。
- 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。

锁的兼容关系如下：

|      | X    | S    |
| ---- | ---- | ---- |
| X    | ×    | ×    |
| S    | ×    |      |

\2. 意向锁

使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。

在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。

意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定：

- 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁；
- 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。

通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。

各种锁的兼容关系如下：

|      | X    | IX   | S    | IS   |
| ---- | ---- | ---- | ---- | ---- |
| X    | ×    | ×    | ×    | ×    |
| IX   | ×    |      | ×    |      |
| S    | ×    | ×    |      |      |
| IX   | ×    |      |      |      |

解释如下：

- 任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁；
- 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）

封锁协议

\1. 三级封锁协议

**一级封锁协议**

事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。

可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。

**二级封锁协议**

在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。

可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。

**三级封锁协议**

在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。

可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。

\2. 两段锁协议

加锁和解锁分为两个阶段进行。

可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。串行执行的事务互不干扰，不会出现并发一致性问题。

事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。

```html
lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B)
```

但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度。

```html
lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C)
```

MySQL 隐式与显示锁定

MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。

InnoDB 也可以使用特定的语句进行显示锁定：

```html
SELECT ... LOCK In SHARE MODE;
SELECT ... FOR UPDATE;
```

### 46、多版本并发控制

多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。

基本思想

在封锁一节中提到，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。

在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。

脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。

版本号

- 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。
- 事务版本号 TRX_ID ：事务开始时的系统版本号。

Undo 日志

MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。

例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。

```html
INSERT INTO t(id, x) VALUES(1, "a");
UPDATE t SET x="b" WHERE id=1;
UPDATE t SET x="c" WHERE id=1;
```

因为没有使用 `START TRANSACTION` 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。

INSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。

ReadView

MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, ...}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。

在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：

- TRX_ID < TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。
- TRX_ID > TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。
- TRX_ID_MIN <= TRX_ID <= TRX_ID_MAX，需要根据隔离级别再进行判断：
  - 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。
  - 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。

在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。

快照读与当前读

\1. 快照读

MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。

```html
SELECT * FROM table ...;
```

\2. 当前读

MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。

```html
INSERT;
UPDATE;
DELETE;
```

在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。

```html
SELECT * FROM table WHERE ? lock in share mode;
SELECT * FROM table WHERE ? for update;
```

### 47、Next-Key Locks

Next-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现。

MVCC 不能解决幻影读问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。

Record Locks

锁定一个记录上的索引，而不是记录本身。

如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。

Gap Locks

锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。

```html
SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;
```

Next-Key Locks

它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：

```html
(-∞, 10]
(10, 11]
(11, 13]
(13, 20]
(20, +∞)
```

### 2.一条SQL语句在MySQL中如何执行的

查询语句

```html
select * from tb_student  A where A.age='18' and A.name=' 张三 ';
```

- 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。

- 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id='1'。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。

- 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案：

  - 先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。
  - 先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。

  那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。

- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

更新语句

```html
update tb_student A set A.age='19' where A.name=' 张三 ';
```

MySQL 自带的日志模块式 **binlog（归档日志）** ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 **redo log（重做日志）**，我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下：

- 先查询到张三这一条数据，如果有缓存，也是会用到缓存。
- 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。
- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。
- 更新完成。

总结

- MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。
- 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。
- SQL 等执行过程分为两类，一类对于查询等过程如下：权限校验---》查询缓存---》分析器---》优化器---》权限校验---》执行器---》引擎
- 对于更新等语句执行流程如下：分析器----》权限校验----》执行器---》引擎---redo log prepare---》binlog---》redo log commit

### 3.一条SQL语句执行得很慢的原因有哪些？

分类讨论

**1、大多数情况是正常的，只是偶尔会出现很慢的情况。**

**2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。**

针对偶尔很慢的情况

1、数据库在刷新脏页（flush）

当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在**内存**中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到**磁盘**中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到**磁盘**中去。

> 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。

**刷脏页有下面4种场景（后两种不用太关注“性能”问题）：**

- **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，**就会导致我们平时正常的SQL语句突然执行的很慢**，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。
- **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。
- **MySQL 认为系统“空闲”的时候：**这时系统没什么压力。
- **MySQL 正常关闭的时候：**这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

2、拿不到锁我能怎么办

这个就比较容易想到了，我们要执行的这条语句，刚好这条语句涉及到的**表**，别人在用，并且加锁了，我们拿不到锁，只能慢慢等待别人释放锁了。或者，表没有加锁，但要使用到的某个一行被加锁了，这个时候，我也没办法啊。

如果要判断是否真的在等待锁，我们可以用 **show processlist**这个命令来查看当前的状态哦，这里我要提醒一下，有些命令最好记录一下，反正，我被问了好几个命令，都不知道怎么写，呵呵。

针对一直都这么慢的情况

1、扎心了，没用到索引

**（1）、字段没有索引**

刚好你的 c 字段上没有索引，那么抱歉，只能走全表扫描了，你就体验不会索引带来的乐趣了，所以，这回导致这条查询语句很慢。

**（2）、字段有索引，但却没有用索引**

好吧，这个时候你给 c 这个字段加上了索引，然后又查询了一条语句

```html
select * from t where c - 1 = 1000;
```

我想问大家一个问题，这样子在查询的时候会用索引查询吗？

答是不会，如果我们在字段的左边做了运算，那么很抱歉，在查询的时候，就不会用上索引了，所以呢，大家要注意这种**字段上有索引，但由于自己的疏忽，导致系统没有使用索引**的情况了。

正确的查询应该如下

```html
select * from t where c = 1000 + 1;
```

有人可能会说，右边有运算就能用上索引？难道数据库就不会自动帮我们优化一下，自动把 c - 1=1000 自动转换为 c = 1000+1。

**（3）、函数操作导致没有用上索引**

如果我们在查询的时候，对字段进行了函数操作，也是会导致没有用上索引的，例如

```html
select * from t where pow(c,2) = 1000;
```

这里我只是做一个例子，假设函数 pow 是求 c 的 n 次方，实际上可能并没有 pow(c,2)这个函数。其实这个和上面在左边做运算也是很类似的。

所以呢，一条语句执行都很慢的时候，可能是该语句没有用上索引了，不过具体是啥原因导致没有用上索引的呢，你就要会分析了，我上面列举的三个原因，应该是出现的比较多的吧。

2、呵呵，数据库自己选错索引了

我们在进行查询操作的时候，例如

```html
select * from t where 100 < c and c < 100000;
```

我们知道，主键索引和非主键索引是有区别的，主键索引存放的值是**整行字段的数据**，而非主键索引上存放的值不是整行字段的数据，而且存放**主键字段的值**。

也就是说，我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后，再根据主键的值走主键索引，查询到整行数据返回。

好吧扯了这么多，其实我就是想告诉你，就算你在 c 字段上有索引，系统也并不一定会走 c 这个字段上的索引，而是有可能会直接扫描扫描全表，找出所有符合 100 < c and c < 100000 的数据。

**为什么会这样呢？**

其实是这样的，系统在执行这条语句的时候，会进行预测：究竟是走 c 索引扫描的行数少，还是直接扫描全表扫描的行数少呢？显然，扫描行数越少当然越好了，因为扫描行数越少，意味着I/O操作的次数越少。

如果是扫描全表的话，那么扫描的次数就是这个表的总行数了，假设为 n；而如果走索引 c 的话，我们通过索引 c 找到主键之后，还得再通过主键索引来找我们整行的数据，也就是说，需要走两次索引。而且，我们也不知道符合 100 c < and c < 10000 这个条件的数据有多少行，万一这个表是全部数据都符合呢？这个时候意味着，走 c 索引不仅扫描的行数是 n，同时还得每行数据走两次索引。

**所以呢，系统是有可能走全表扫描而不走索引的。那系统是怎么判断呢？**

判断来源于系统的预测，也就是说，如果要走 c 字段索引的话，系统会预测走 c 字段索引大概需要扫描多少行。如果预测到要扫描的行数很多，它可能就不走索引而直接扫描全表了。

那么问题来了，**系统是怎么预测判断的呢？**这里我给你讲下系统是怎么判断的吧，虽然这个时候我已经写到脖子有点酸了。

系统是通过**索引的区分度**来判断的，一个索引上不同的值越多，意味着出现相同数值的索引越少，意味着索引的区分度越高。我们也把区分度称之为**基数**，即区分度越高，基数越大。所以呢，基数越大，意味着符合 100 < c and c < 10000 这个条件的行数越少。

所以呢，一个索引的基数越大，意味着走索引查询越有优势。

**那么问题来了，怎么知道这个索引的基数呢？**

系统当然是不会遍历全部来获得一个索引的基数的，代价太大了，索引系统是通过遍历部分数据，也就是通过**采样**的方式，来预测索引的基数的。

**扯了这么多，重点的来了**，居然是采样，那就有可能出现**失误**的情况，也就是说，c 这个索引的基数实际上是很大的，但是采样的时候，却很不幸，把这个索引的基数预测成很小。例如你采样的那一部分数据刚好基数很小，然后就误以为索引的基数很小。**然后就呵呵，系统就不走 c 索引了，直接走全部扫描了**。

所以呢，说了这么多，得出结论：**由于统计的失误，导致系统没有走索引，而是走了全表扫描**，而这，也是导致我们 SQL 语句执行的很慢的原因。

> 这里我声明一下，系统判断是否走索引，扫描行数的预测其实只是原因之一，这条查询语句是否需要使用使用临时表、是否需要排序等也是会影响系统的选择的。

不过呢，我们有时候也可以通过强制走索引的方式来查询，例如

```html
select * from t force index(a) where c < 100 and c < 100000;
```

我们也可以通过

```html
show index from t;
```

来查询索引的基数和实际是否符合，如果和实际很不符合的话，我们可以重新来统计索引的基数，可以用这条命令

```html
analyze table t;
```

来重新统计分析。

**既然会预测错索引的基数，这也意味着，当我们的查询语句有多个索引的时候，系统有可能也会选错索引哦**，这也可能是 SQL 执行的很慢的一个原因。

**总结**

以上是我的总结与理解，最后一个部分，我怕很多人不大懂**数据库居然会选错索引**，所以我详细解释了一下，下面我对以上做一个总结。

一个 SQL 执行的很慢，我们要分两种情况讨论：

1、大多数情况下很正常，偶尔很慢，则有如下原因

(1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。

(2)、执行的时候，遇到锁，如表锁、行锁。

2、这条 SQL 语句一直执行的很慢，则有如下原因。

(1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。

(2)、数据库选错了索引。

### 4.数据类型

1. 数值类型 -- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数
   - 默认存在符号位，unsigned 属性修改
   - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数'123'，补填后为'00123'
   - 在满足要求的情况下，越小越好。
   - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。 -- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。 -- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。
2. 字符串类型 -- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2= 65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3 -- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值 -- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.
3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155 datetime YYYY-MM-DD hh:mm:ss timestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmss date YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDD time hh:mm:ss hhmmss hhmmss year YYYY YY YYYY YY
4. 枚举和集合 -- 枚举(enum) ---------- enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。 -- 集合（set） ---------- set(val1, val2, val3...) create table tab ( gender set('男', '女', '无') ); insert into tab values ('男, 女'); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。

### 5.列属性(列约束)

1. PRIMARY 主键
   - 能唯一标识记录的字段，可以作为主键。
   - 一个表只能有一个主键。
   - 主键具有唯一性。
   - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id));
   - 主键字段的值不能为null。
   - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));
2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。
3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, 'val'); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null
4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, 'val'); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time
5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;
6. COMMENT 注释 例：create table tab ( id int ) comment '注释内容';
7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择：
   1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。
   2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。
   3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。

### 6.建表规范

```html
/* 建表规范 */ ------------------
    -- Normal Format, NF
        - 每个表保存一个实体信息
        - 每个具有一个ID字段作为主键
        - ID主键 + 原子表
    -- 1NF, 第一范式        
        字段不能再分，就满足第一范式。
    -- 2NF, 第二范式
        满足第一范式的前提下，不能出现部分依赖。
        消除符合主键就可以避免部分依赖。增加单列关键字。
    -- 3NF, 第三范式
        满足第二范式的前提下，不能出现传递依赖。
        某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。
        将一个实体信息的数据放在一个表内实现。
```

### 7.SELECT

```html
/* SELECT */ ------------------
SELECT [ALL|DISTINCT] select_expr FROM -> WHERE -> GROUP BY [合计函数] -> HAVING -> ORDER BY -> LIMIT
a. select_expr
    -- 可以用 * 表示所有字段。
        select * from tb;
    -- 可以使用表达式（计算公式、函数调用、字段也是个表达式）
        select stu, 29+25, now() from tb;
    -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。
        - 使用 as 关键字，也可省略 as.
        select stu+10 as add10 from tb;
b. FROM 子句    
    用于标识查询来源。
    -- 可以为表起别名。使用as关键字。
        SELECT * FROM tb1 AS tt, tb2 AS bb;
    -- from子句后，可以同时出现多个表。
        -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。
        SELECT * FROM tb1, tb2;
    -- 向优化符提示如何选择索引
        USE INDEX、IGNORE INDEX、FORCE INDEX
        SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3;
        SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;
c. WHERE 子句
    -- 从from获得的数据源中进行筛选。
    -- 整型1表示真，0表示假。
    -- 表达式由运算符和运算数组成。
        -- 运算数：变量（字段）、值、函数返回值
        -- 运算符：
            =, <=>, <>, !=, <=, <, >=, >, !, &&, ||,
            in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor
            is/is not 加上ture/false/unknown，检验某个值的真假
            <=>与<>功能相同，<=>可用于null比较
d. GROUP BY 子句, 分组子句
    GROUP BY 字段/别名 [排序方式]
    分组后会进行排序。升序：ASC，降序：DESC
    以下[合计函数]需配合 GROUP BY 使用：
    count 返回不同的非NULL值数目  count(*)、count(字段)
    sum 求和
    max 求最大值
    min 求最小值
    avg 求平均值
    group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。
e. HAVING 子句，条件子句
    与 where 功能、用法相同，执行时机不同。
    where 在开始时执行检测数据，对原数据进行过滤。
    having 对筛选出的结果再次进行过滤。
    having 字段必须是查询出来的，where 字段必须是数据表存在的。
    where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。
    where 不可以使用合计函数。一般需用合计函数才会用 having
    SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。
f. ORDER BY 子句，排序子句
    order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]...
    升序：ASC，降序：DESC
    支持多个字段的排序。
g. LIMIT 子句，限制结果数量子句
    仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。
    limit 起始位置, 获取条数
    省略第一个参数，表示从索引0开始。limit 获取条数
h. DISTINCT, ALL 选项
    distinct 去除重复记录
    默认为 all, 全部记录
```

### 8.UNION

```html
/* UNION */ ------------------
    将多个select查询的结果组合成一个结果集合。
    SELECT ... UNION [ALL|DISTINCT] SELECT ...
    默认 DISTINCT 方式，即所有返回的行都是唯一的
    建议，对每个SELECT查询加上小括号包裹。
    ORDER BY 排序时，需加上 LIMIT 进行结合。
    需要各select查询的字段数量一样。
    每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。
```

### 9.子查询

```html
/* 子查询 */ ------------------
    - 子查询需用括号包裹。
-- from型
    from后要求是一个表，必须给子查询结果取个别名。
    - 简化每个查询内的条件。
    - from型需将结果生成一个临时表格，可用以原表的锁定的释放。
    - 子查询返回一个表，表型子查询。
    select * from (select * from tb where id>0) as subfrom where id>1;
-- where型
    - 子查询返回一个值，标量子查询。
    - 不需要给子查询取别名。
    - where子查询内的表，不能直接用以更新。
    select * from tb where money = (select max(money) from tb);
    -- 列子查询
        如果子查询结果返回的是一列。
        使用 in 或 not in 完成查询
        exists 和 not exists 条件
            如果子查询返回数据，则返回1或0。常用于判断条件。
            select column1 from t1 where exists (select * from t2);
    -- 行子查询
        查询条件是一个行。
        select * from t1 where (id, gender) in (select id, gender from t2);
        行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...)
        行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。
    -- 特殊运算符
    != all()    相当于 not in
    = some()    相当于 in。any 是 some 的别名
    != some()   不等同于 not in，不等于其中某一个。
    all, some 可以配合其他运算符一起使用。
```

### 10.连接查询(join)

```html
/* 连接查询(join) */ ------------------
    将多个表的字段进行连接，可以指定连接条件。
-- 内连接(inner join)
    - 默认就是内连接，可省略inner。
    - 只有数据存在时才能发送连接。即连接结果不能出现空行。
    on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真）
    也可用where表示连接条件。
    还有 using, 但需字段名相同。 using(字段名)
    -- 交叉连接 cross join
        即，没有条件的内连接。
        select * from tb1 cross join tb2;
-- 外连接(outer join)
    - 如果数据不存在，也会出现在连接结果中。
    -- 左外连接 left join
        如果数据不存在，左表记录会出现，而右表为null填充
    -- 右外连接 right join
        如果数据不存在，右表记录会出现，而左表为null填充
-- 自然连接(natural join)
    自动判断连接条件完成连接。
    相当于省略了using，会自动查找相同字段名。
    natural join
    natural left join
    natural right join
select info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info
.stu_id;
```

### 11.TRUNCATE

```html
/* TRUNCATE */ ------------------
TRUNCATE [TABLE] tbl_name
清空数据
删除重建表
区别：
1，truncate 是删除表再创建，delete 是逐条删除
2，truncate 重置auto_increment的值。而delete不会
3，truncate 不知道删除了几条，而delete知道。
4，当被用于带分区的表时，truncate 会保留分区
```

### 12.备份与还原

```html
/* 备份与还原 */ ------------------
备份，将数据的结构与表内数据保存起来。
利用 mysqldump 指令完成。
-- 导出
mysqldump [options] db_name [tables]
mysqldump [options] ---database DB1 [DB2 DB3...]
mysqldump [options] --all--database
1. 导出一张表
　　mysqldump -u用户名 -p密码 库名 表名 > 文件名(D:/a.sql)
2. 导出多张表
　　mysqldump -u用户名 -p密码 库名 表1 表2 表3 > 文件名(D:/a.sql)
3. 导出所有表
　　mysqldump -u用户名 -p密码 库名 > 文件名(D:/a.sql)
4. 导出一个库
　　mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 > 文件名(D:/a.sql)
可以-w携带WHERE条件
-- 导入
1. 在登录mysql的情况下：
　　source  备份文件
2. 在不登录的情况下
　　mysql -u用户名 -p密码 库名 < 备份文件
```

### 13.视图

```html
什么是视图：
    视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。
    视图具有表结构文件，但不存在数据文件。
    对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。
    视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。
-- 创建视图
CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement
    - 视图名必须唯一，同时不能与表重名。
    - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。
    - 可以指定视图执行的算法，通过ALGORITHM指定。
    - column_list如果存在，则数目必须等于SELECT语句检索的列数
-- 查看结构
    SHOW CREATE VIEW view_name
-- 删除视图
    - 删除视图后，数据依然存在。
    - 可同时删除多个视图。
    DROP VIEW [IF EXISTS] view_name ...
-- 修改视图结构
    - 一般不修改视图，因为不是所有的更新视图都会映射到表上。
    ALTER VIEW view_name [(column_list)] AS select_statement
-- 视图作用
    1. 简化业务逻辑
    2. 对客户端隐藏真实的表结构
-- 视图算法(ALGORITHM)
    MERGE       合并
        将视图的查询语句，与外部查询需要先合并再执行！
    TEMPTABLE   临时表
        将视图执行完毕后，形成临时表，再做外层查询！
    UNDEFINED   未定义(默认)，指的是MySQL自主去选择相应的算法。
```

### 14.事务(transaction)

```html
事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。
    - 支持连续SQL的集体成功或集体撤销。
    - 事务是数据库在数据晚自习方面的一个功能。
    - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。
    - InnoDB被称为事务安全型引擎。
-- 事务开启
    START TRANSACTION; 或者 BEGIN;
    开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。
-- 事务提交
    COMMIT;
-- 事务回滚
    ROLLBACK;
    如果部分操作发生问题，映射到事务开启前。
-- 事务的特性
    1. 原子性（Atomicity）
        事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。
    2. 一致性（Consistency）
        事务前后数据的完整性必须保持一致。
        - 事务开始和结束时，外部数据一致
        - 在整个事务过程中，操作是连续的
    3. 隔离性（Isolation）
        多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。
    4. 持久性（Durability）
        一个事务一旦被提交，它对数据库中的数据改变就是永久性的。
-- 事务的实现
    1. 要求是事务支持的表类型
    2. 执行一组相关的操作前开启事务
    3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。
-- 事务的原理
    利用InnoDB的自动提交(autocommit)特性完成。
    普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。
    而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。
-- 注意
    1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。
    2. 事务不能被嵌套
-- 保存点
    SAVEPOINT 保存点名称 -- 设置一个事务保存点
    ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点
    RELEASE SAVEPOINT 保存点名称 -- 删除保存点
-- InnoDB自动提交特性设置
    SET autocommit = 0|1;    0表示关闭自动提交，1表示开启自动提交。
    - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。
    - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是，
        SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接)
        而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务)
```

### 15.锁表

```html
/* 锁表 */
表锁定只用于防止其它客户端进行不正当地读取和写入
MyISAM 支持表锁，InnoDB 支持行锁
-- 锁定
    LOCK TABLES tbl_name [AS alias]
-- 解锁
    UNLOCK TABLES
```

### 16.触发器

```html
------------------
    触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象
    监听：记录的增加、修改、删除。
-- 创建触发器
CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt    
参数：
    trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。
    trigger_event指明了激活触发程序的语句的类型
        INSERT：将新行插入表时激活触发程序
        UPDATE：更改某一行时激活触发程序
        DELETE：从表中删除某一行时激活触发程序
    tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。
    trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构
-- 删除
DROP TRIGGER [schema_name.]trigger_name
可以使用old和new代替旧的和新的数据
    更新操作，更新前是old，更新后是new.
    删除操作，只有old.
    增加操作，只有new.
-- 注意
    1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。
-- 字符连接函数
concat(str1,str2,...])
concat_ws(separator,str1,str2,...)
-- 分支语句
if 条件 then
    执行语句
else if 条件 then
    执行语句
else
    执行语句
end if;
-- 修改最外层语句结束符
delimiter 自定义结束符号
    SQL语句
自定义结束符号
delimiter ;     -- 修改回原来的分号
-- 语句块包裹
begin
    语句块
end
-- 特殊的执行
1. 只要添加记录，就会触发程序。
2. Insert into on duplicate key update 语法会触发：
    如果没有重复记录，会触发 before insert, after insert;
    如果有重复记录并更新，会触发 before insert, before update, after update;
    如果有重复记录但是没有发生更新，则触发 before insert, before update
3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert
```

### 17.SQL编程

```html
/* SQL编程 */ ------------------
--// 局部变量 ----------
-- 变量声明
    declare var_name[,...] type [default value]
    这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。
-- 赋值
    使用 set 和 select into 语句为变量赋值。
    - 注意：在函数内是可以使用全局变量（用户自定义的变量）
--// 全局变量 ----------
-- 定义、赋值
set 语句可以定义并为变量赋值。
set @var = value;
也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。
还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。
select @var:=20;
select @v1:=id, @v2=name from t1 limit 1;
select * from tbl_name where @var:=30;
select into 可以将表中查询获得的数据赋给变量。    
    -| select max(height) into @max_height from tb;
-- 自定义变量名
为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。
@var=10;
    - 变量被定义后，在整个会话周期都有效（登录到退出）
--// 控制结构 ----------
-- if语句
if search_condition then
    statement_list   
[elseif search_condition then
    statement_list]
...
[else
    statement_list]
end if;
-- case语句
CASE value WHEN [compare-value] THEN result
[WHEN [compare-value] THEN result ...]
[ELSE result]
END
-- while循环
[begin_label:] while search_condition do
    statement_list
end while [end_label];
- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。
    -- 退出循环
        退出整个循环 leave
        退出当前循环 iterate
        通过退出的标签决定退出哪个循环
--// 内置函数 ----------
-- 数值函数
abs(x)          -- 绝对值 abs(-10.9) = 10
format(x, d)    -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46
ceil(x)         -- 向上取整 ceil(10.1) = 11
floor(x)        -- 向下取整 floor (10.1) = 10
round(x)        -- 四舍五入去整
mod(m, n)       -- m%n m mod n 求余 10%3=1
pi()            -- 获得圆周率
pow(m, n)       -- m^n
sqrt(x)         -- 算术平方根
rand()          -- 随机数
truncate(x, d)  -- 截取d位小数
-- 时间日期函数
now(), current_timestamp();     -- 当前日期时间
current_date();                 -- 当前日期
current_time();                 --当前时间
date('yyyy-mm-dd hh:ii:ss');    -- 获取日期部分
time('yyyy-mm-dd hh:ii:ss');    -- 获取时间部分
date_format('yyyy-mm-dd hh:ii:ss', '%d %y %a %d %m %b %j'); -- 格式化时间
unix_timestamp();               -- 获得unix时间戳
from_unixtime();                -- 从时间戳获得时间
-- 字符串函数
length(string)          -- string长度，字节
char_length(string)     -- string的字符个数
substring(str, position [,length])      -- 从str的position开始,取length个字符
replace(str ,search_str ,replace_str)   -- 在str中用replace_str替换search_str
instr(string ,substring)    -- 返回substring首次在string中出现的位置
concat(string [,...])   -- 连接字串
charset(str)            -- 返回字串字符集
lcase(string)           -- 转换成小写
left(string, length)    -- 从string2中的左边起取length个字符
load_file(file_name)    -- 从文件读取内容
locate(substring, string [,start_position]) -- 同instr,但可指定开始位置
lpad(string, length, pad)   -- 重复用pad加在string开头,直到字串长度为length
ltrim(string)           -- 去除前端空格
repeat(string, count)   -- 重复count次
rpad(string, length, pad)   --在str后用pad补充,直到长度为length
rtrim(string)           -- 去除后端空格
strcmp(string1 ,string2)    -- 逐字符比较两字串大小
-- 流程函数
case when [condition] then result [when [condition] then result ...] [else result] end   多分支
if(expr1,expr2,expr3)  双分支。
-- 聚合函数
count()
sum();
max();
min();
avg();
group_concat()
-- 其他常用函数
md5();
default();
--// 存储函数，自定义函数 ----------
-- 新建
    CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型
        函数体
    - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。
    - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。
    - 参数部分，由"参数名"和"参数类型"组成。多个参数用逗号隔开。
    - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。
    - 多条语句应该使用 begin...end 语句块包含。
    - 一定要有 return 返回值语句。
-- 删除
    DROP FUNCTION [IF EXISTS] function_name;
-- 查看
    SHOW FUNCTION STATUS LIKE 'partten'
    SHOW CREATE FUNCTION function_name;
-- 修改
    ALTER FUNCTION function_name 函数选项
--// 存储过程，自定义功能 ----------
-- 定义
存储存储过程 是一段代码（过程），存储在数据库中的sql组成。
一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。
而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。
-- 创建
CREATE PROCEDURE sp_name (参数列表)
    过程体
参数列表：不同于函数的参数列表，需要指明参数类型
IN，表示输入型
OUT，表示输出型
INOUT，表示混合型
注意，没有返回值。
```

### 18.存储过程

```html
/* 存储过程 */ ------------------
存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。
调用：CALL 过程名
-- 注意
- 没有返回值。
- 只能单独调用，不可夹杂在其他语句中
-- 参数
IN|OUT|INOUT 参数名数据类型
IN      输入：在调用过程中，将数据输入到过程体内部的参数
OUT     输出：在调用过程中，将过程体处理完的结果返回到客户端
INOUT   输入输出：既可输入，也可输出
-- 语法
CREATE PROCEDURE 过程名 (参数列表)
BEGIN    
    过程体
END
```

### 19.用户和权限管理

```html
/* 用户和权限管理 */ ------------------
-- root密码重置
1. 停止MySQL服务
2.  [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &
    [Windows] mysqld --skip-grant-tables
3. use mysql;
4. UPDATE `user` SET PASSWORD=PASSWORD("密码") WHERE `user` = "root";
5. FLUSH PRIVILEGES;
用户信息表：mysql.user
-- 刷新权限
FLUSH PRIVILEGES;
-- 增加用户
CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串)
    - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。
    - 只能创建用户，不能赋予权限。
    - 用户名，注意引号：如 'user_name'@'192.168.1.1'
    - 密码也需引号，纯数字密码也要加引号
    - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD
-- 重命名用户
RENAME USER old_user TO new_user
-- 设置密码
SET PASSWORD = PASSWORD('密码')  -- 为当前用户设置密码
SET PASSWORD FOR 用户名 = PASSWORD('密码') -- 为指定用户设置密码
-- 删除用户
DROP USER 用户名
-- 分配权限/添加用户
GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] 'password']
    - all privileges 表示所有权限
    - *.* 表示所有库的所有表
    - 库名.表名 表示某库下面的某表
    GRANT ALL PRIVILEGES ON `pms`.* TO 'pms'@'%' IDENTIFIED BY 'pms0817';
-- 查看权限
SHOW GRANTS FOR 用户名
    -- 查看当前用户权限
    SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();
-- 撤消权限
REVOKE 权限列表 ON 表名 FROM 用户名
REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名   -- 撤销所有权限
-- 权限层级
-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。
全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user
    GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。
数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host
    GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。
表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv
    GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。
列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv
    当使用REVOKE时，您必须指定与被授权列相同的列。
-- 权限列表
ALL [PRIVILEGES]    -- 设置除GRANT OPTION之外的所有简单权限
ALTER   -- 允许使用ALTER TABLE
ALTER ROUTINE   -- 更改或取消已存储的子程序
CREATE  -- 允许使用CREATE TABLE
CREATE ROUTINE  -- 创建已存储的子程序
CREATE TEMPORARY TABLES     -- 允许使用CREATE TEMPORARY TABLE
CREATE USER     -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。
CREATE VIEW     -- 允许使用CREATE VIEW
DELETE  -- 允许使用DELETE
DROP    -- 允许使用DROP TABLE
EXECUTE     -- 允许用户运行已存储的子程序
FILE    -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILE
INDEX   -- 允许使用CREATE INDEX和DROP INDEX
INSERT  -- 允许使用INSERT
LOCK TABLES     -- 允许对您拥有SELECT权限的表使用LOCK TABLES
PROCESS     -- 允许使用SHOW FULL PROCESSLIST
REFERENCES  -- 未被实施
RELOAD  -- 允许使用FLUSH
REPLICATION CLIENT  -- 允许用户询问从属服务器或主服务器的地址
REPLICATION SLAVE   -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）
SELECT  -- 允许使用SELECT
SHOW DATABASES  --显示所有数据库
SHOW VIEW   -- 允许使用SHOW CREATE VIEW
SHUTDOWN    -- 允许使用mysqladmin shutdown
SUPER   -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到
max_connections。
UPDATE  -- 允许使用UPDATE
USAGE   -- “无权限”的同义词
GRANT OPTION    -- 允许授予权限
```

### 20.表维护

```html
/* 表维护 */
-- 分析和存储表的关键字分布
ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...
-- 检查一个或多个表是否有错误
CHECK TABLE tbl_name [, tbl_name] ... [option] ...
option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED}
-- 整理数据文件的碎片
OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...
```

### 21.杂项

```html
/* 杂项 */ ------------------
1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！
2. 每个库目录存在一个保存当前数据库的选项文件db.opt。
3. 注释：
    单行注释 # 注释内容
    多行注释 /* 注释内容 */
    单行注释 -- 注释内容     (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)
4. 模式通配符：
    _   任意单个字符
    %   任意多个字符，甚至包括零字符
    单引号需要进行转义 \'
5. CMD命令行内的语句结束符可以为 ";", "\G", "\g"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。
6. SQL对大小写不敏感
7. 清除已有语句：\c
```

### 22.实践总结

模糊查询

对于模糊查询阿里巴巴开发手册这样说到：

> 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。
>
> 说明:索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。

外键和级联

对于外键和级联，阿里巴巴开发手册这样说到：

> 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。
>
> 说明:以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风 险;外键影响数据库的插入速度

为什么不要用外键呢？大部分人可能会这样回答：

> 1. **增加了复杂性：** a.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。
> 2. **增加了额外工作**： 数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。）
> 3. 外键还会因为需要请求对其他表内部加锁而容易出现死锁情况；
> 4. **对分库分表不友好** ：因为分库分表下外键是无法生效的。
> 5. ......

关于@Transactional注解

对于`@Transactional`事务注解，阿里巴巴开发手册这样说到：

> 【参考】@Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。

### 23.**数据库存储时间**

1.切记不要用字符串存储日期

我记得我在大学的时候就这样干过，而且现在很多对数据库不太了解的新手也会这样干，可见，这种存储日期的方式的优点还是有的，就是简单直白，容易上手。

但是，这是不正确的做法，主要会有下面两个问题：

1. 字符串占用的空间更大！
2. 字符串存储的日期比较效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。

2.Datetime 和 Timestamp 之间抉择

Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？

**通常我们都会首选 Timestamp。** 下面说一下为什么这样做!

2.1 DateTime 类型没有时区信息的

**DateTime 类型是没有时区信息的（时区无关）** ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。不要小看这个问题，很多系统就是因为这个问题闹出了很多笑话。

**Timestamp 和时区有关**。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。

2.2 DateTime 类型耗费空间更大

Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。

- DateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59
- Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59

> Timestamp 在不同版本的 MySQL 中有细微差别。

4.数值型时间戳是更好的选择吗？

很多时候，我们也会使用 int 或者 bigint 类型的数值也就是时间戳来表示时间。

这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便，毕竟只是存放的数值。缺点也很明显，就是数据的可读性太差了，你无法直观的看到具体时间。

时间戳的定义如下：

> 时间戳的定义是从一个基准时间开始算起，这个基准时间是「1970-1-1 00:00:00 +0:00」，从这个时间开始，用整数表示，以秒计时，随着时间的流逝这个时间整数不断增加。这样一来，我只需要一个数值，就可以完美地表示时间了，而且这个数值是一个绝对数值，即无论的身处地球的任何角落，这个表示时间的时间戳，都是一样的，生成的数值都是一样的，并且没有时区的概念，所以在系统的中时间的传输中，都不需要进行额外的转换了，只有在显示给用户的时候，才转换为字符串格式的本地时间。

### 24.存储过程

存储过程指一组用于完成特定功能的SQL语句集，它被存储在数据库中，经过第一次编译后再次调用时不需要被再次编译，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。存储过程是数据库中的一个重要对象，我们可以基于存储过程快速完成复杂的计算操作。以下为常见的存储过程的优化思路，也是我们编写事务时需要遵守的原则。

- 尽量利用一些SQL语句代替一些小循环，例如聚合函数、求平均函数等。
- 中间结果被存放于临时表中，并加索引。
- 少使用游标（Cursors）：SQL 是种集合语言，对于集合运算有较高的性能，而游标是过程运算。比如，对一个 50 万行的数据进行查询时，如果使用游标，则需要对表执行50万次读取请求，将占用大量的数据库资源，影响数据库的性能。
- 事务越短越好：SQL Server支持并发操作，如果事务过长或者隔离级别过高，则都会造成并发操作的阻塞、死锁，导致查询速度极慢、CPU占用率高等。
- 使用try-catch处理异常。
- 尽量不要将查找语句放在循环中，防止出现过度消耗系统资源的情况。

### 25.触发器

触发器是一段能自动执行的程序，和普通存储过程的区别是“触发器在对某一个表或者数据进行操作时触发”，例如进行UPDATE、INSERT、DELETE操作时，系统会自动调用和执行该表对应的触发器。触发器一般用于数据变化后需要执行一系列操作的情况，比如对系统核心数据的修改需要通过触发器来存储操作日志的信息等。

### 6.查询缓存的使用

> 执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用

my.cnf加入以下配置，重启MySQL开启查询缓存

```html
query_cache_type=1
query_cache_size=600000
```

MySQL执行以下命令也可以开启查询缓存

```html
set global  query_cache_type=1;
set global  query_cache_size=600000;
```

如上，**开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果**。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL库中的系统表，其查询结果也不会被缓存。

缓存建立之后，MySQL的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。

**缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。** 因此，开启查询缓存要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，**还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存：**

```html
select sql_no_cache count(*) from usr;
```

### 21.什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?

池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。就好比你去食堂打饭，打饭的大妈会先把饭盛好几份放那里，你来了就直接拿着饭盒加菜即可，不用再临时又盛饭又打菜，效率就高了。除了初始化资源，池化设计还包括如下这些特征：池子的初始值、池子的活跃值、池子的最大值等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。这篇文章对[池化设计思想](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485679&idx=1&sn=57dbca8c9ad49e1f3968ecff04a4f735&chksm=cea24724f9d5ce3212292fac291234a760c99c0960b5430d714269efe33554730b5f71208582&token=1141994790&lang=zh_CN#rd)介绍的还不错，直接复制过来，避免重复造轮子了。

数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。**在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中**。 连接池还减少了用户必须等待建立与数据库的连接的时间。

### 22.主键使⽤⾃增ID还是UUID?

推荐使⽤⾃增ID,不要使⽤UUID. 因为在InnoDB存储引擎中,主键索引是作为聚簇索引存在的,也就是说,主键索引的B+树叶⼦节点上存储了主键索引以及全部的数 据(按照顺序),如果主键索引是⾃增ID,那么只需要不断向后排列即可,如果是UUID,由于到来的ID与原来的⼤⼩不确定,会造成⾮常 多的数据插⼊,数据移动,然后导致产⽣很多的内存碎⽚,进⽽造成插⼊性能的下降.

### 23.MySQL的binlog有有⼏种录⼊格式?分别有什么区别?

有三种格式,statement,row和mixed. statement模式下,记录单元为语句.即每⼀个sql造成的影响会记录.由于sql的执⾏是有上下⽂的,因此在保存的时候需要保存 相关的信息,同时还有⼀些使⽤了函数之类的语句⽆法被记录复制. row级别下,记录单元为每⼀⾏的改动,基本是可以全部记下来但是由于很多操作,会导致⼤量⾏的改动(⽐如alter table),因此 这种模式的⽂件保存的信息太多,⽇志量太⼤. mixed. ⼀种折中的⽅案,普通操作使⽤statement记录,当⽆法使⽤statement的时候使⽤row. 此外,新版的MySQL中对row级别也做了⼀些优化,当表结构发⽣变化的时候,会记录语句⽽不是逐⾏记录.

### 24.超⼤分⻚怎么处理?

超⼤的分⻚⼀般从两个⽅向上来解决. 数据库层⾯,这也是我们主要集中关注的(虽然收效没那么⼤),类似于select * from table where age > 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然⽐ 较慢. 当时我们可以修改为select * from table where id in (select id from table where age > 20 limit 1000000,10).这样 虽然也load了⼀百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还 可以select * from table where id > 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核⼼思想都⼀样,就是减 少load的数据. 从需求的⻆度减少这种请求....主要是不做类似的需求(直接跳转到⼏百万⻚之后的具体某⼀⻚.只允许逐⻚查看或者按照给定 的路线⾛,这样可预测,可缓存)以及防⽌ID泄漏且连续被⼈恶意攻击. 解决超⼤分⻚,其实主要是靠缓存,可预测性的提前查到内容,缓存⾄redis等k-V数据库中,直接返回即可.

## 54.实践总结

### 模糊查询

对于模糊查询阿里巴巴开发手册这样说到：

> 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。
>
> 说明:索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。

### 外键和级联

对于外键和级联，阿里巴巴开发手册这样说到：

> 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。
>
> 说明:以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风 险;外键影响数据库的插入速度

为什么不要用外键呢？大部分人可能会这样回答：

> 1. **增加了复杂性：** a.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。
> 2. **增加了额外工作**： 数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。）
> 3. 外键还会因为需要请求对其他表内部加锁而容易出现死锁情况；
> 4. **对分库分表不友好** ：因为分库分表下外键是无法生效的。
> 5. ......

### 关于@Transactional注解

对于`@Transactional`事务注解，阿里巴巴开发手册这样说到：

> 【参考】@Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。

## 55.为什么要用索引?索引的优缺点分析

### 索引的优点

**可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。毕竟大部分系统的读请求总是大于写请求的。** 另外，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

### 索引的缺点

1. **创建索引和维护索引需要耗费许多时间**：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。
2. **占用物理存储空间** ：索引需要使用物理文件存储，也会耗费一定空间。

## 56.B 树和 B+树区别

- B 树的所有节点既存放 键(key) 也存放 数据(data);而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。

## 57.Hash 索引和 B+树索引优劣分析

**Hash 索引定位快**

Hash 索引指的就是 Hash 表，最大的优点就是能够在很短的时间内，根据 Hash 函数定位到数据所在的位置，这是 B+树所不能比的。

**Hash 冲突问题**

知道 HashMap 或 HashTable 的同学，相信都知道它们最大的缺点就是 Hash 冲突了。不过对于数据库来说这还不算最大的缺点。

**Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点。**

试想一种情况:

```html
SELECT * FROM tb1 WHERE id < 500;
```

B+树是有序的，在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。

## 58.索引类型

### 主键索引(Primary Key)

**数据表的主键列使用的就是主键索引。**

**一张数据表有只能有一个主键，并且主键不能为 null，不能重复。**

**在 mysql 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。**

### 二级索引(辅助索引)

**二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

**PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。**

1. **唯一索引(Unique Key)** ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
2. **普通索引(Index)** ：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**
3. **前缀索引(Prefix)** ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
4. **全文索引(Full Text)** ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

## 59.聚集索引与非聚集索引

### 聚集索引

**聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。**

在 Mysql 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

聚集索引的优点

聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

聚集索引的缺点

1. **依赖于有序的数据** ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
2. **更新代价大** ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

### 非聚集索引

**非聚集索引即索引结构和数据分开存放的索引。**

**二级索引属于非聚集索引。**

> MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。
>
> **非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。**

非聚集索引的优点

**更新代价比聚集索引要小** 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的

非聚集索引的缺点

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. **可能会二次查询(回表)** :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

## 60.非聚集索引一定回表查询吗(覆盖索引)?

**非聚集索引不一定回表查询。**

> 试想一种情况，用户准备使用 SQL 查询用户名，而用户名字段正好建立了索引。

```html
 SELECT name FROM table WHERE name='guang19';
```

> 那么这个索引的 key 本身就是 name，查到对应的 name 直接返回就行了，无需回表查询。

**即使是 MYISAM 也是这样，虽然 MYISAM 的主键索引确实需要回表， 因为它的主键索引的叶子节点存放的是指针。但是如果 SQL 查的就是主键呢?**

```html
SELECT id FROM table WHERE id=1;
```

主键索引本身的 key 就是主键，查到返回就行了。这种情况就称之为覆盖索引了。

## 61.覆盖索引

如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！

**覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。**

> 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。
>
> 再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。

## 62.索引创建原则

### 单列索引

单列索引即由一列属性组成的索引。

### 联合索引(多列索引)

联合索引即由多列属性组成索引。

### 最左前缀原则

假设创建的联合索引由三个字段组成:

```html
ALTER TABLE table ADD INDEX index_name (num,name,age)
```

那么当查询的条件有为:num / (num AND name) / (num AND name AND age)时，索引才生效。所以在创建联合索引时，尽量把查询最频繁的那个字段作为最左(第一个)字段。查询的时候也尽量以这个字段为第一条件。

## 63.索引创建注意点

### 最左前缀原则

虽然我目前的 Mysql 版本较高，好像不遵守最左前缀原则，索引也会生效。 但是我们仍应遵守最左前缀原则，以免版本更迭带来的麻烦。

### 选择合适的字段

1.不为 NULL 的字段

索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。

2.被频繁查询的字段

我们创建索引的字段应该是查询操作非常频繁的字段。

3.被作为条件查询的字段

被作为 WHERE 条件查询的字段，应该被考虑建立索引。

4.被经常频繁用于连接的字段

经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。

### 不合适创建索引的字段

1.被频繁更新的字段应该慎重建立索引

虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。

2.不被经常查询的字段没有必要建立索引

3.尽可能的考虑建立联合索引而不是单列索引

因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。

4.注意避免冗余索引

冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。

5.考虑在字符串类型的字段上使用前缀索引代替普通索引

前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。

### 使用索引一定能提高查询性能吗?

大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。

## 64.**数据库存储时间**

### 1.切记不要用字符串存储日期

我记得我在大学的时候就这样干过，而且现在很多对数据库不太了解的新手也会这样干，可见，这种存储日期的方式的优点还是有的，就是简单直白，容易上手。

但是，这是不正确的做法，主要会有下面两个问题：

1. 字符串占用的空间更大！
2. 字符串存储的日期比较效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。

### 2.Datetime 和 Timestamp 之间抉择

Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？

**通常我们都会首选 Timestamp。** 下面说一下为什么这样做!

2.1 DateTime 类型没有时区信息的

**DateTime 类型是没有时区信息的（时区无关）** ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。不要小看这个问题，很多系统就是因为这个问题闹出了很多笑话。

**Timestamp 和时区有关**。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。

2.2 DateTime 类型耗费空间更大

Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。

- DateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59
- Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59

> Timestamp 在不同版本的 MySQL 中有细微差别。

### 4.数值型时间戳是更好的选择吗？

很多时候，我们也会使用 int 或者 bigint 类型的数值也就是时间戳来表示时间。

这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便，毕竟只是存放的数值。缺点也很明显，就是数据的可读性太差了，你无法直观的看到具体时间。

时间戳的定义如下：

> 时间戳的定义是从一个基准时间开始算起，这个基准时间是「1970-1-1 00:00:00 +0:00」，从这个时间开始，用整数表示，以秒计时，随着时间的流逝这个时间整数不断增加。这样一来，我只需要一个数值，就可以完美地表示时间了，而且这个数值是一个绝对数值，即无论的身处地球的任何角落，这个表示时间的时间戳，都是一样的，生成的数值都是一样的，并且没有时区的概念，所以在系统的中时间的传输中，都不需要进行额外的转换了，只有在显示给用户的时候，才转换为字符串格式的本地时间。

# Mysql二

### 1.为什么使⽤ B+树 ⽽不使⽤⼆叉树或者B树？

⾸先，我们知道访问磁盘需要访问到指定块中，⽽访问指定块是需要 盘⽚旋转 和 磁臂移动 的，这是⼀个⽐较耗时的过程，如 果增加树⾼那么就意味着你需要进⾏更多次的磁盘访问，所以会采⽤n叉树。 ⽽使⽤B+树是因为如果使⽤B树在进⾏⼀个范围查找的时候每次都会进⾏重新检索，⽽在B+树中可以充分利⽤叶⼦结点的链表。 在建表的时候你可能会添加多个索引，⽽ InnDB 会为每个索引建⽴⼀个 B+树 进⾏存储索引。

创建一个表，这个时候 InnDB 就会为我们建⽴两个 B+索引树，⼀个是 主键 的 聚簇索引，另⼀个是 普通索引 的 辅助索引。

辅助索引上⾯的叶⼦节点的值只是存了主键的值，⽽在主键的聚簇索引上的叶⼦节点才是存上了整条记录的值。

回表

⽐如这个时候我们进⾏⼀个查询操作

```html
select name from test where a = 30;
```

我们知道因为条件 MySQL 是会⾛ a 的索引的，但是 a 索引上并没有存储 name 的值，此时我们就需要拿到相应 a 上的主键值， 然后通过这个主键值去⾛ 聚簇索引 最终拿到其中的name值，这个过程就叫回表。 我们来总结⼀下回表是什么？MySQL在辅助索引上找到对应的主键值并通过主键值在聚簇索引上查找所要的数据就叫回表。

索引维护

我们知道索引是需要占⽤空间的，索引虽能提升我们的查询速度但是也是不能滥⽤。 ⽐如我们在⽤⼾表⾥⽤⾝份证号做主键，那么每个⼆级索引的叶⼦节点占⽤约20个字节，⽽如果⽤整型做主键，则只要4个字 节，如果是⻓整型（bigint）则是8个字节。也就是说如果我⽤整型后⾯维护了4个g的索引列表，那么⽤⾝份证将会是20个g。 所以我们可以通过缩减索引的⼤⼩来减少索引所占空间。 当然B+树为了维护索引的有序性会在删除，插⼊的时候进⾏⼀些必要的维护(在InnoDB中删除会将节点标记为“可复⽤”以减少 对结构的变动)。 ⽐如在增加⼀个节点的时候可能会遇到数据⻚满了的情况，这个时候就需要做⻚的分裂，这是⼀个⽐较耗时的⼯作，⽽且⻚的分 裂还会导致数据⻚的利⽤率变低，⽐如原来存放三个数据的数据⻚再次添加⼀个数据的时候需要做⻚分裂，这个时候就会将现有 的四个数据分配到两个数据⻚中，这样就减少了数据⻚利⽤率。

覆盖索引

上⾯提到了 回表，⽽有时候我们查辅助索引的时候就已经满⾜了我们需要查的数据，这个时候 InnoDB 就会进⾏⼀个叫 覆盖索引 的操作来提升效率，减少回表。 ⽐如这个时候我们进⾏⼀个 select 操作 select id from test where a = 1; 这个时候很明显我们⾛了 a 的索引直接能获取到 id 的值，这个时候就不需要进⾏回表，我们这个时候就使⽤了 覆盖索引。 简单来说 覆盖索引 就是当我们⾛辅助索引的时候能获取到我们所需要的数据的时候不需要再次进⾏回表操作的操作。

联合索引最左匹配原则

全值匹配的时候优化器会改变顺序，也就是说你全值匹配时的顺序和原先的联合索引顺序不⼀致没有关系，优化器会帮你调好。 索引匹配从最左边的地⽅开始，如果没有则会进⾏全表扫描，⽐如你设计了⼀个(a,b,c)的联合索引，然后你可以使⽤(a),(a,b),(a,b,c) ⽽你使⽤ (b),(b,c),(c)就⽤不到索引了。 遇到范围匹配会取消索引。

索引下推

如果最左匹配原则因为范围查询终⽌了，InnoDB还是会索引下推来优化性能。

⼀些最佳实践 哪些情况需要创建索引？ 频繁作为查询条件的字段应创建索引。 多表关联查询的时候，关联字段应该创建索引。 查询中的排序字段，应该创建索引。 统计或者分组字段需要创建索引。 哪些情况不需要创建索引 表记录少。 经常增删改查的表。 频繁更新的字段。 where 条件使⽤不⾼的字段。 字段很⼤的时候。 其他 尽量选择区分度⾼的列作为索引。 不要对索引进⾏⼀些函数操作，还应注意隐式的类型转换和字符编码转换。 尽可能的扩展索引，不要新建⽴索引。⽐如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引 即可。 多考虑覆盖索引，索引下推，最左匹配。

### 2.锁

全局锁

MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候， 可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结 构等）和更新类事务的提交语句。 一般会在进行全库逻辑备份 的时候使用，这样就能确保 其他线程不能对该数据库做更新操作。

表锁

MDL(Meta Data Lock)元数据锁 MDL锁⽤来保证只有⼀个线程能对该表进⾏表结构更改。 怎么说呢？MDL分为 MDL写锁 和 MDL读锁，加锁规则是这样的 当线程对⼀个表进⾏ CRUD 操作的时候会加 MDL读锁 当线程对⼀个表进⾏ 表结构更改 操作的时候会加 MDL写锁 写锁和读锁，写锁和写锁互斥，读锁之间不互斥 lock tables xxx read/write; 这是给⼀个表设置读锁和写锁的命令，如果在某个线程A中执⾏lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写 t2的语句都会被阻塞。同时，线程A在执⾏unlock tables之前，也只能执⾏读t1、读写t2的操作。连写t1都不允许，⾃然也不能 访问其他表。 这种表锁是⼀种处理并发的⽅式，但是在InnoDB中常⽤的是⾏锁。

⾏锁

我们知道在5.5版本以前 MySQL 的默认存储引擎是 MyISAM，⽽ MyISAM 和 InnoDB 最⼤的区别就是两个 事务 ⾏锁 其中⾏锁是我们今天的主题，如果不了解事务可以去补习⼀下。 其实⾏锁就是两个锁，你可以理解为 写锁(排他锁 X锁)和读锁(共享锁 S锁) 共享锁（S锁）：允许⼀个事务去读⼀⾏，阻⽌其他事务获得相同数据集的排他锁。也叫做读锁：读锁是共享的，多个客⼾ 可以同时读取同⼀个资源，但不允许其他客⼾修改。 排他锁（X锁)：允许获得排他锁的事务更新数据，阻⽌其他事务取得相同数据集的共享读锁和排他写锁。也叫做写锁：写锁 是排他的，写锁会阻塞其他的写锁和读锁。 ⽽⾏锁还会引起⼀个⼀个很头疼的问题，那就是死锁。 如果事务A对⾏100加了写锁，事务B对⾏101加了写锁，此时事务A想要修改⾏101⽽事务B⼜想修改⾏100，这样占有且等待就导 致了死锁问题，⽽⾯对死锁问题就只有检测和预防了。

next-key锁

MVCC 和⾏锁是⽆法解决 幻读 问题的，这个时候 InnoDB 使⽤了 ⼀个叫 GAP锁(间隙锁) 的东西，它配合 ⾏锁 形成了 next-key 锁，解决了幻读的问题。 但是因为它的加锁规则，⼜导致了扩⼤了⼀些加锁范围从⽽减少数据库并发能⼒。具体的加锁规则如下： 加锁的基本单位是next-key lock 就是⾏锁和GAP锁结合。 查找过程中访问到的对象就会加锁。 索引上的等值查询，给唯⼀索引加锁的时候，next-key lock退化为⾏锁。 索引上的等值查询，向右遍历时且最后⼀个值不满⾜等值条件的时候，next-key lock退化为间隙锁。 唯⼀索引上的范围查询会访问到不满⾜条件的第⼀个值为⽌。

### 3.MySQL性能

最⼤数据量

MySQL没有限制单表最⼤记录数，它取决于操作系统对⽂件⼤⼩的限制。

单表⾏数超过500万⾏或者单表容量超过2GB，才推荐分库分表。

最⼤并发数

并发数是指同⼀时刻数据库能处理多少个请求，由maxconnections和max userconnections决定。 maxconnections是指MySQL实例的最⼤连接数，上限值是16384，maxuserconnections是指每个数据库⽤⼾的最 ⼤连接数。MySQL会为每个连接提供缓冲区，意味着消耗更多的内存。如果连接数设置太⾼硬件吃不消，太低⼜不能充 分利⽤硬件。⼀般要求两者⽐值超过10%

查询耗时0.5秒

响应时间=客⼾端UI渲染耗时+⽹络请求耗时+应⽤程序处理耗时+查询数据库耗时，0.5秒就是留给数 据库1/6的处理时间。

实施原则

充分利⽤但不滥⽤索引，须知索引也消耗磁盘和CPU。 不推荐使⽤数据库函数格式化数据，交给应⽤程序处理。 不推荐使⽤外键约束，⽤应⽤程序保证数据准确性。 写多读少的场景，不推荐使⽤唯⼀索引，⽤应⽤程序保证唯⼀性。 适当冗余字段，尝试创建中间表，⽤应⽤程序计算中间结果，⽤空间换时间。 不允许执⾏极度耗时的事务，配合应⽤程序拆分成更⼩的事务。 预估重要数据表（⽐如订单表）的负载和数据增⻓态势，提前优化。

数据表设计

数据类型

数据类型的选择原则：更简单或者占⽤空间更⼩。 如果⻓度能够满⾜，整型尽量使⽤tinyint、smallint、medium_int⽽⾮int。 如果字符串⻓度确定，采⽤char类型。 如果varchar能够满⾜，不采⽤text类型。 精度要求较⾼的使⽤decimal类型，也可以使⽤BIGINT，⽐如精确两位⼩数就乘以100后保存。 尽量采⽤timestamp⽽⾮datetime。

相⽐datetime，timestamp占⽤更少的空间，以UTC的格式储存⾃动转换时区。

避免空值 MySQL中字段为NULL时依然占⽤空间，会使索引、索引统计更加复杂。从NULL值更新到⾮NULL⽆法做到原地更新， 容易发⽣索引分裂影响性能。尽可能将NULL值⽤有意义的值代替，也能避免SQL语句⾥⾯包含 is not null 的判断。 text类型优化 由于text字段储存⼤量数据，表容量会很早涨上去，影响其他字段的查询性能。建议抽取出来放在⼦表⾥，⽤业务主键 关联。 索引优化 索引分类

1. 普通索引：最基本的索引。
2. 组合索引：多个字段上建⽴的索引，能够加速复合查询条件的检索。
3. 唯⼀索引：与普通索引类似，但索引列的值必须唯⼀，允许有空值。
4. 组合唯⼀索引：列值的组合必须唯⼀。
5. 主键索引：特殊的唯⼀索引，⽤于唯⼀标识数据表中的某⼀条记录，不允许有空值，⼀般⽤primary key约束。
6. 全⽂索引：⽤于海量⽂本的查询，MySQL5.6之后的InnoDB和MyISAM均⽀持全⽂索引。由于查询精度以及扩展性 不佳，更多的企业选择Elasticsearch。

索引优化

1. 分⻚查询很重要，如果查询数据量超过30%，MYSQL不会使⽤索引。
2. 单表索引数不超过5个、单个索引字段数不超过5个。
3. 字符串可使⽤前缀索引，前缀⻓度控制在5-8个字符。
4. 字段唯⼀性太低，增加索引没有意义，如：是否删除、性别。
5. 合理使⽤覆盖索引，如下所⽰： select loginname, nickname from member where login_name = ? loginname, nickname两个字段建⽴组合索引，⽐login_name简单索引要更快

SQL优化 分批处理

操作符<>优化

通常<>操作符⽆法使⽤索引，采⽤union聚合搜索结果

OR优化 在Innodb引擎下or⽆法使⽤组合索引，

IN优化

1. IN适合主表⼤⼦表⼩，EXIST适合主表⼩⼦表⼤。由于查询优化器的不断升级，很多场景这两者性能差不多⼀样了。
2. 尝试改为join查询，举例如下： select id from orders where user_id in (select id from user where level = 'VIP');

不做列运算 通常在查询条件列运算会导致索引失效

避免Select all 如果不查询表中所有的列，避免使⽤ SELECT * ，它会进⾏全表扫描，不能有效利⽤索引。

Like优化

去除了前⾯的%查询将会命中索引，但是产品经理⼀定要前后模糊匹配呢？全⽂索引fulltext可以尝试⼀下，但 Elasticsearch才是终极武器。

Join优化 join的实现是采⽤Nested Loop Join算法，就是通过驱动表的结果集作为基础数据，通过该结数据作为过滤条件到下⼀ 个表中循环查询数据，然后合并结果。如果有多个join，则将前⾯的结果集作为循环数据，再次到后⼀个表中查询数 据。

1. 驱动表和被驱动表尽可能增加查询条件，满⾜ON的条件⽽少⽤Where，⽤⼩结果集驱动⼤结果集。
2. 被驱动表的join字段上加上索引，⽆法建⽴索引的时候，设置⾜够的Join Buffer Size。
3. 禁⽌join连接三个以上的表，尝试增加冗余字段。

Limit优化 limit⽤于分⻚查询时越往后翻性能越差，解决的原则：缩⼩扫描范围 ，

先筛选出ID缩⼩查询范围

### 4.MySQL 数据库开发规范

1. 所有的数据库对象名称必须使⽤⼩写字⺟并⽤下划线分割（MySQL⼤⼩写敏感，名称要⻅名知意，最好不超过32字符）
2. 所有的数据库对象名称禁⽌使⽤MySQL保留关键字（如 desc、range、match、delayed 等，请参考 MySQL官⽅保留字 ）
3. 临时库表必须以tmp为前缀并以⽇期为后缀（tmp_）
4. 备份库和库必须以bak为前缀并以⽇期为后缀(bak_) 5.所有存储相同数据的列名和列类型必须⼀致。（在多个表中的字段如user_id，它们类型必须⼀致）
5. mysql5.5之前默认的存储的引擎是myisam，没有特殊要求，所有的表必须使⽤innodb（innodb好处⽀持失误，⾏级锁，⾼ 并发下性能更好，对多核，⼤内存，ssd等硬件⽀持更好）
6. 数据库和表的字符集尽量统⼀使⽤utf8（字符集必须统⼀，避免由于字符集转换产⽣的乱码，汉字utf8下占3个字节）
7. 所有表和字段都要添加注释COMMENT，从⼀开始就进⾏数据字典的维护
8. 尽量控制单表数据量的⼤⼩在500w以内，超过500w可以使⽤历史数据归档，分库分表来实现（500万⾏并不是MySQL数据库 的限制。过⼤对于修改表结构，备份，恢复都会有很⼤问题。MySQL没有对存储有限制，取决于存储设置和⽂件系统）
9. 谨慎使⽤mysql分区表（分区表在物理上表现为多个⽂件，在逻辑上表现为⼀个表）
10. 谨慎选择分区键，跨分区查询效率可能更低
11. 建议使⽤物理分表的⽅式管理⼤数据
12. 尽量做到冷热数据分离，减⼩表的宽度（mysql限制最多存储4096列，⾏数没有限制，但是每⼀⾏的字节总数不能超过 65535。列限制好处：减少磁盘io，保证热数据的内存缓存命中率，避免读⼊⽆⽤的冷数据）
13. 禁⽌在表中建⽴预留字段（⽆法确认存储的数据类型，对预留字段类型进⾏修改，会对表进⾏锁定）
14. 禁⽌在数据中存储图⽚，⽂件⼆进制数据（使⽤⽂件服务器）
15. 禁⽌在线上做数据库压⼒测试
16. 禁⽌从开发环境，测试环境直接连⽣产环境数据库
17. 限制每张表上的索引数量，建议单表索引不超过5个（索引会增加查询效率，但是会降低插⼊和更新的速度）
18. 避免建⽴冗余索引和重复索引（冗余：index（a,b,c) index(a,b) index(a)）
19. 禁⽌给表中的每⼀列都建⽴单独的索引
20. 每个innodb表必须有⼀个主键，选择⾃增id（不能使⽤更新频繁的列作为主键，不适⽤UUID,MD5,HASH,字符串列作为主 键）
21. 区分度最⾼的列放在联合索引的最左侧
22. 尽量把字段⻓度⼩的列放在联合索引的最左侧
23. 尽量避免使⽤外键（禁⽌使⽤物理外键，建议使⽤逻辑外键）
24. 优先选择符合存储需要的最⼩数据类型
25. 优先使⽤⽆符号的整形来存储
26. 优先选择存储最⼩的数据类型（varchar(N),N代表的是字符数，⽽不是字节数，N代表能存储多少个汉字）
27. 避免使⽤Text或是Blob类型
28. 避免使⽤ENUM数据类型（修改ENUM值需要使⽤ALTER语句，ENUM类型的ORDER BY操作效率低，需要额外操作，禁⽌使 ⽤书值作为ENUM的枚举值
29. 尽量把所有的字段定义为NOT NULL（索引NULL需要额外的空间来保存，所以需要暂⽤更多的内存，进⾏⽐较和计算要对 NULL值做特别的处理）
30. 使⽤timestamp或datetime类型来存储时间
31. 同财务相关的⾦额数据，采⽤decimal类型（不丢失精度，禁⽌使⽤ float 和 double）
32. 避免使⽤双%号和like，搜索严禁左模糊或者全模糊（如果需要请⽤搜索引擎来解决。索引⽂件具有 B-Tree 的最左前缀匹配 特性，如果左边的值未确定，那么⽆法使⽤此索）
33. 建议使⽤预编译语句进⾏数据库操作
34. 禁⽌跨库查询（为数据迁移和分库分表留出余地，降低耦合度，降低⻛险）
35. 禁⽌select * 查询（消耗更多的cpu和io及⽹络带宽资源，⽆法使⽤覆盖索引）
36. 禁⽌使⽤不含字段列表的insert语句（不允许insert into t values（‘a’，‘b’，‘c’）不允许）
37. in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内
38. 禁⽌使⽤order by rand（）进⾏随机排序
39. 禁⽌where从句中对列进⾏函数转换和计算（例如：where date（createtime）=‘20160901’ 会⽆法使⽤createtime列 上索引。改成 where createtime>='20160901' and createtime <'20160902'）
40. 尽量使⽤ union all 代替 union
41. 拆分复杂的⼤SQL为多个⼩SQL（ MySQL⼀个SQL只能使⽤⼀个CPU进⾏计算）
42. 尽量避免使⽤⼦查询，可以把⼦查询优化为join操作（⼦查询的结果集⽆法使⽤索引，⼦查询会产⽣临时表操作，如果⼦查询 数据量⼤会影响效率，消耗过多的CPU及IO资源）
43. 超过100万⾏的批量写操作，要分批多次进⾏操作（⼤批量操作可能会造成严重的主从延迟，binlog⽇志为row格式会产⽣⼤ 量的⽇志，避免产⽣⼤事务操作）
44. 对于⼤表使⽤pt—online-schema-change修改表结构（避免⼤表修改产⽣的主从延迟，避免在对表字段进⾏修改时进⾏锁 表）
45. 对于程序连接数据库账号，遵循权限最⼩原则
46. 超过三个表禁⽌ join。（需要 join 的字段，数据类型必须绝对⼀致；多表关联查询时，保证被关联的字段需要有索引。即使 双表 join 也要注意表索引、SQL 性能。）
47. 在varchar字段上建⽴索引时，必须指定索引⻓度，没必要对全字段建⽴索引，根据实际⽂本区分度决定索引⻓度即可。
48. SQL 性能优化的⽬标：⾄少要达到 range 级别，要求是 ref 级别，如果可以是 consts最好
49. 使⽤ ISNULL()来判断是否为 NULL 值。
50. 尽量不要使⽤物理删除（即直接删除，如果要删除的话提前做好备份），⽽是使⽤逻辑删除，使⽤字段delete_flag做逻辑删 除，类型为tinyint，0表⽰未删除，1表⽰已删除
51. 如果有 order by 的场景，请注意利⽤索引的有序性。order by 最后的字段是组合,索引的⼀部分，并且放在索引组合顺序的 最后，避免出现 file_sort 的情况，影响查询性能。
52. 在代码中写分⻚查询逻辑时，若 count 为 0 应直接返回，避免执⾏后⾯的分⻚语句

### 5.索引

索引的分类

从存储结构上来划分：BTree索引（B-Tree或B+Tree索引），Hash索引，full-index全⽂索引，R-Tree索引。 从应⽤层次来分：普通索引，唯⼀索引，复合索引 根据中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，⾮聚集索引。

普通索引：即⼀个索引只包含单个列，⼀个表可以有多个单列索引 唯⼀索引：索引列的值必须唯⼀，但允许有空值 复合索引：即⼀个索引包含多个列 聚簇索引(聚集索引)：并不是⼀种单独的索引类型，⽽是⼀种数据存储⽅式。具体细节取决于不同的实现，InnoDB的聚簇索引其 实就是在同⼀个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据⾏。 ⾮聚簇索引：不是聚簇索引，就是⾮聚簇索引（认真脸）。

索引的底层实现

mysql默认存储引擎innodb只显式⽀持B-Tree( 从技术上来说是B+Tree)索引，对于频繁访问的表，innodb会透明建⽴⾃适应 hash索引，即在B树索引基础上建⽴hash索引，可以显著提⾼查找效率，对于客⼾端是透明的，不可控制的，隐式的。

Hash索引

基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每⼀⾏数据，存储引擎都会对所有的索引列计算⼀个哈希码 （hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据⾏的指针。

B-Tree索引（MySQL使⽤B+Tree）

B-Tree能加快数据的访问速度，因为存储引擎不再需要进⾏全表扫描来获取数据，数据分布在各个节点之中。

B+Tree索引

是B-Tree的改进版本，同时也是数据库索引索引所采⽤的存储结构。数据都在叶⼦节点上，并且增加了顺序访问指针，每个叶⼦ 节点都指向相邻的叶⼦节点的地址。相⽐B-Tree来说，进⾏范围查找时只需要查找两个节点，进⾏遍历即可。⽽B-Tree需要获取 所有节点，相⽐之下B+Tree效率更⾼。

为什么索引结构默认使⽤B-Tree，⽽不是hash，⼆叉树，红⿊树？

hash：虽然可以快速定位，但是没有顺序，IO复杂度⾼。 ⼆叉树：树的⾼度不均匀，不能⾃平衡，查找效率跟数据有关（树的⾼度），并且IO代价⾼。 红⿊树：树的⾼度随着数据量增加⽽增加，IO代价⾼。

为什么官⽅建议使⽤⾃增⻓主键作为索引

结合B+Tree的特点，⾃增主键是连续的，在插⼊过程中尽量减少⻚分裂，即使要进⾏⻚分裂，也只会分裂很少⼀部分。并且能减 少数据的移动，每次插⼊都是插⼊到最后。总之就是减少分裂和移动的频率。

### 6.联合索引是什么?为什么需要注意联合索引中的顺序?

MySQL可以使⽤多个字段同时建⽴⼀个索引,叫做联合索引.在联合索引中,如果想要命中索引,需要按照建⽴索引时的字段顺序挨个 使⽤,否则⽆法命中索引. 具体原因为: MySQL使⽤索引时需要索引有序,假设现在建⽴了"name,age,school"的联合索引 那么索引的排序为: 先按照name排序,如果name相同,则按照age排序,如果age的值也相等,则按照school进⾏排序. 当进⾏查询时,此时索引仅仅按照name严格有序,因此必须⾸先使⽤name字段进⾏等值查询,之后对于匹配到的列⽽⾔,其按照age 字段严格有序,此时可以使⽤age字段⽤做索引查找,以此类推. 因此在建⽴联合索引的时候应该注意索引列的顺序,⼀般情况下,将查询需求频繁或者字段选择性⾼的列放在前⾯.此外可以根据特 例的查询或者表结构进⾏单独的调整.

### 7.在哪些情况下会发⽣针对该列创建了索引但是在查询的时候并没有使⽤呢?

使⽤不等于查询 列参与了数学运算或者函数 在字符串like时左边是通配符.类似于'%aaa'. 当mysql分析全表扫描⽐使⽤索引快的时候不使⽤索引. 当使⽤联合索引,前⾯⼀个条件为范围查询,后⾯的即使符合最左前缀原则,也⽆法使⽤索引.

### 8.为什么要尽量设定⼀个主键?

主键是数据库确保数据⾏在整张表唯⼀性的保障,即使业务上本张表没有主键,也建议添加⼀个⾃增⻓的ID列作为主键. 设定了主键之后,在后续的删改查的时候可能更加快速以及确保操作数据范围安全.

### 9.字段为什么要求定义为not null?

null值会占⽤更多的字节,且会在程序中造成很多与预期不符的情况.

### 10.如果要存储⽤⼾的密码散列,应该使⽤什么字段进⾏存储?

密码散列,盐,⽤⼾⾝份证号等固定⻓度的字符串应该使⽤char⽽不是varchar来存储,这样可以节省空间且提⾼检索效率.

### 11.如何实现 MySQL 的读写分离？

其实很简单，就是基于主从复制架构，简单来说，就搞⼀个主库，挂多个从库，然后我们就单单只是写主库，然后主库会⾃动把 数据给同步到从库上去。

MySQL 主从复制原理

主库将变更写⼊ binlog ⽇志，然后从库连接到主库之后，从库有⼀个 IO 线程，将主库的 binlog ⽇志拷⻉到⾃⼰本地，写⼊⼀个 relay 中继⽇志中。接着从库中有⼀个 SQL 线程会从中继⽇志读取 binlog，然后执⾏ binlog ⽇志中的内容，也就是在⾃⼰本地 再次执⾏⼀遍 SQL，这样就可以保证⾃⼰跟主库的数据是⼀样的。

从库同步主库数据的过程是串⾏化的，也就是说主库上并⾏的操作，在从库上会串⾏执⾏。由于从库从主库拷⻉⽇志以及串⾏执⾏ SQL 的特点，在⾼并发场景下，从库的数据⼀定会⽐主库慢⼀些，是有延时的。所以经常出现，刚写⼊主库的数据可能是读不到的，要过⼏⼗毫秒，甚⾄⼏百毫秒才能读取到。还有另外⼀个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

MySQL 实际上在这⼀块有两个机制，⼀个是半同步复制，⽤来解决主库数据丢失问题；⼀个是并⾏复制，⽤来解决主从同步延时问题。

这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写⼊ binlog ⽇志之后，就会将强制此时⽴即将数据同步到从库，从库将⽇志写⼊⾃⼰本地的 relay log 之后，接着会返回⼀个 ack 给主库，主库接收到⾄少⼀个从库的 ack 之后才会认为写操作完成了。 所谓并⾏复制，指的是从库开启多个线程，并⾏读取 relay log 中不同库的⽇志，然后并⾏重放不同库的⽇志，这是库级别的并⾏。

### 12.表分区

表分区，是指根据⼀定规则，将数据库中的⼀张表分解成多个更⼩的，容易管理的部分。从逻辑上看，只有⼀张表，但是底层却 是由多个物理分区组成。

表分区有什么好处？

1、存储更多数据。分区表的数据可以分布在不同的物理设备上，从⽽⾼效地利⽤多个硬件设备。和单个磁盘或者⽂件系统相 ⽐，可以存储更多数据 2、优化查询。在where语句中包含分区条件时，可以只扫描⼀个或多个分区表来提⾼查询效率；涉及sum和count语句时，也可 以在多个分区上并⾏处理，最后汇总结果。 3、分区表更容易维护。例如：想批量删除⼤量数据可以清除整个分区。 4、避免某些特殊的瓶颈，例如InnoDB的单个索引的互斥访问，ext3问价你系统的inode锁竞争等。

分区表的限制因素

1、⼀个表最多只能有1024个分区 2、MySQL5.1中，分区表达式必须是整数，或者返回整数的表达式。在MySQL5.5中提供了⾮整数表达式分区的⽀持。 3、如果分区字段中有主键或者唯⼀索引的列，那么多有主键列和唯⼀索引列都必须包含进来。即：分区字段要么不包含主键或 者索引列，要么包含全部主键和索引列。 4、分区表中⽆法使⽤外键约束 5、MySQL的分区适⽤于⼀个表的所有数据和索引，不能只对表数据分区⽽不对索引分区，也不能只对索引分区⽽不对表分区， 也不能只对表的⼀部分数据分区。

MySQL⽀持的分区类型有哪些？

RANGE分区：这种模式允许将数据划分不同范围。例如可以将⼀个表通过年份划分成若⼲个分区 LIST分区：这种模式允许系统通过预定义的列表的值来对数据进⾏分割。按照List中的值分区，与RANGE的区别是，range分区 的区间范围值是连续的。 HASH分区 ：这中模式允许通过对表的⼀个或多个列的Hash Key进⾏计算，最后通过这个Hash码不同数值对应的数据区域进⾏ 分区。例如可以建⽴⼀个对表主键进⾏分区的表。 KEY分区 ：上⾯Hash模式的⼀种延伸，这⾥的Hash Key是MySQL系统产⽣的。

## 主从复制

### 1.如何实现 MySQL 的读写分离？

基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。

### 2.MySQL 主从复制原理的是啥？

主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。

有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。

而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。

这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。

所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。

### 3.MySQL 主从同步延时问题

我们通过 MySQL 命令：

show status

查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。

一般来说，如果主从延迟较为严重，有以下解决方案：

- 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
- 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
- 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
- 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。

### 4.MySQL的主从复制架构

MySQL自己在执行增删改的时候会记录binlog日志，这个binlog日志里就记录了所有数据增删改的操作，然后从库上有一个IO线程，这个IO线程会负责跟主库建立一个TCP连接，接着请求主库传输binlog日志给自己，这个时候主库上有一个IO dump线程，就会负责通过这个TCP连接把binlog日志传输给从库的IO线程。接着从库的IO线程会把读取到的binlog日志数据写入到自己本地的relay日志文件中去，然后从库上另外有一个SQL线程会读取relay日志里的内容，进行日志重做，把所有在主库执行过的增删改操作，在从库上做一遍，达到一个还原数据的过程。

异步复制

主库把日志写入binlog文件，接着自己就提交事务返回了，他也不管从库到底收到日志没有。

半同步复制

半同步的意思，就是说，你主库写入数据，日志进入binlog之后，起码得确保 binlog日志复制到从库了，你再告诉客户端说本次写入事务成功。

第一种叫做AFTER_COMMIT方式，他不是默认的，他的意思是说，主库写入日志到binlog，等待binlog复制到从库了，主库就提交自己的本地事务，接着等待从库返回给自己一个成功的响应，然后主库返回提交事务成功的响应给客户端。

另外一种是现在MySQL 5.7默认的方式，主库把日志写入binlog，并且复制给从库，然后开始等待从库的响应，从库返回说成功给主库了，主库再提交事务，接着返回提交事务成功的响应给客户端。

在主库上检查一下半同步复制是否正常运行：show global status like '%semi%';，如果看到了Rpl_semi_sync_master_status的状态是ON，那么就可以了。

### 5.主从复制架构中的数据延迟问题，应该如何解决？

主库是多线程并发写入的，所以主库写入数据的速度可能是很快的，但是从库是单个线程缓慢拉取数据的，所以才会导致从库复制数据的速度是比较慢的。

让从库也用多线程并行复制数据就可以了，这样从库复制数据的速度快了，延迟就会很低了。

MySQL 5.7就已经支持并行复制了，可以在从库里设置slave_parallel_workers>0，然后把slave_parallel_type设置为LOGICAL_CLOCK，就ok了。

如果你觉得还是要求刚写入的数据你立马强制必须一定可以读到，那么此时你可以使用一个办法，就是在类似MyCat或者Sharding-Sphere之类的中间件里设置强制读写都从主库走，这样你写入主库的数据，强制从主库里读取，一定立即可以读到的。

### 6.数据库高可用：基于主从复制实现故障转移

一般生产环境里用于进行数据库高可用架构管理的工具是MHA，也就是Master High Availability Manager and Tools for MySQL，用perl脚本写的一个工具，这个工具就是专门用于监控主库的状态，如果感觉不对劲，可以把从库切换为主库。

这个MHA自己也是需要单独部署的，分为两种节点，一个是Manager节点，一个是Node节点，Manager节点一般是单独部署一台机器的，Node节点一般是部署在每台MySQL机器上的，因为Node节点得通过解析各个MySQL的日志来进行一些操作。

Manager节点会通过探测集群里的Node节点去判断各个Node所在机器上的MySQL运行是否正常，如果发现某个Master故障了，就直接把他的一个Slave提升为Master，然后让其他Slave都挂到新的Master上去，完全透明。

### 7.主从复制

什么是主从复制

主从复制，是用来建立一个和主数据库完全一样的数据库环境，称为从数据库；主数据库一般是准实时的业务数据库。

主从复制的作用

1、做数据的热备，作为后备数据库，主数据库服务器故障后，可切换到从数据库继续工作，避免数据丢失。 2、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。 3、读写分离，使数据库能支撑更大的并发。在报表中尤其重要。由于部分报表sql语句非常的慢，导致锁表，影响前台服务。如果前台使用master，报表使用slave，那么报表sql将不会造成前台锁，保证了前台速度。

主从复制的原理

1.数据库有个bin-log二进制文件，记录了所有sql语句。 2.我们的目标就是把主数据库的bin-log文件的sql语句复制过来。 3.让其在从数据的relay-log重做日志文件中再执行一次这些sql语句即可。 4.下面的主从配置就是围绕这个原理配置 5.具体需要三个线程来操作： binlog输出线程。每当有从库连接到主库的时候，主库都会创建一个线程然后发送binlog内容到从库。 在从库里，当复制开始的时候，从库就会创建两个线程进行处理： 从库I/O线程。当START SLAVE语句在从库开始执行之后，从库创建一个I/O线程，该线程连接到主库并请求主库发送binlog里面的更新记录到从库上。从库I/O线程读取主库的binlog输出线程发送的更新并拷贝这些更新到本地文件，其中包括relay log文件。 从库的SQL线程。从库创建一个SQL线程，这个线程读取从库I/O线程写到relay log的更新事件并执行。可以知道，对于每一个主从复制的连接，都有三个线程。拥有多个从库的主库为每一个连接到主库的从库创建一个binlog输出线程，每一个从库都有它自己的I/O线程和SQL线程。

步骤一：主库db的更新事件(update、insert、delete)被写到binlog 步骤二：从库发起连接，连接到主库 步骤三：此时主库创建一个binlog dump thread，把binlog的内容发送到从库 步骤四：从库启动之后，创建一个I/O线程，读取主库传过来的binlog内容并写入到relay log 步骤五：还会创建一个SQL线程，从relay log里面读取内容，从Exec_Master_Log_Pos位置开始执行读取到的更新事件，将更新内容写入到slave的db（

主从复制问题

1. 负载均衡，由于复制的时间差，不能保证同步读，而且写仍然单点，没法多点写，我对这个理解就是半吊子的读写均衡。
2. 容灾，基本都是有损容灾，因为数据不同步，谁用谁知道，半吊子的容灾。

### 8.MySQL主从复制的几种方式

异步复制

MySQL的复制默认是异步的，主从复制至少需要两个MYSQL服务，这些MySQL服务可以分布在不同的服务器上，也可以在同一台服务器上。

MySQL主从异步复制是最常见的复制场景。数据的完整性依赖于主库BINLOG的不丢失，只要主库的BINLOG不丢失，那么就算主库宕机了，我们还可以通过BINLOG把丢失的部分数据通过手工同步到从库上去。

> 注意：主库宕机的情况下，DBA可以通过mysqlbinlog工具手工访问主库binlog，抽取缺失的日志并同步到从库上去；也可以通过配置高可用MHA架构来自动抽取缺失的数据补全从库，或者启用Global Transaction Identifiers（GTID)来自动抽取缺失binlog到从库。

MySQL在BINLOG中记录事务(或SQL语句),也就是说对于支持事务的的引擎(例如InnoDB)来说，每个事务提交时都需要写BINLOG；对于不支持事务的引擎(例如MyISAM)来说，每个SQL语句执行完成时，都需要些BINLOG。为了保证Binlog的安全，MySQL引入sync_binlog参数来控制BINLOG刷新到磁盘的频率。

**show** **variables** **like** 'sync_binlog';

- 在默认情况下，sync_binlog=1，表示事务提交之前，MySQL都需要先把BINLOG刷新到磁盘，这样的话，即使出现数据库主机操作系统崩溃或者主机突然掉电的情况，系统最多损失prepared状态的事务；设置sync_binlog=1，尽可能保证数据安全。
- sync_binlog=0,表示MySQL不控制binlog的刷新，由文件系统自己控制文件缓存的刷新。
- sync_binlog=N,如果N不等于0或者1，刷新方式同sync_binlog=1类似，只不过此时会延长刷新频率至N次binlog提交组之后。

以上是传统的异步复制，在MySQL5.7的并行复制技术(也称多线程复制)到来之前，为人诟病最多的还是效率问题，slave延迟是一个顽疾，虽然之前已经出现了schema级别的并行复制，但实际效果并不好。

多线程复制

在MySQL5.7中，带来了全新的多线程复制技术，解决了当master同一个schema下的数据发生了变更，从库不能并发应用的问题，同时也真正将binlog组提交的优势充分发挥出来，保障了从库并发应用Relay Log的能力。

在MySQL8.0中，多线程复制又进行了技术更新，引入了writeset的概念，而在之前的版本中，如果主库的同一个会话顺序执行多个不同相关对象的事务，例如，先执行了Update A表的数据，又执行了Update B表的数据，那么BINLOG在复制到从库后，这两个事务是不能并行执行的，writeset的到来，突破了这个限制。

增强半同步复制

前面介绍的复制是异步操作，主库和从库的数据之间难免会存在一定的延迟，这样存在一个隐患：当在主库上写入一个事务并提交成功，而从库尚未得到主库的BINLOG日志时，主库由于磁盘损坏、内存故障、断电等原因意外宕机，导致主库上该事务BINLOG丢失，此时从库就会损失这个事务，从而造成主从不一致。

为了解决这个问题，从MySQL5.5开始，引入了半同步复制，此时的技术暂且称之为传统的半同步复制，因该技术发展到MySQL5.7后，已经演变为增强半同步复制(也成为无损复制)。在异步复制时，主库执行Commit提交操作并写入BINLOG日志后即可成功返回客户端，无需等待BINLOG日志传送给从库。

而半同步复制时，为了保证主库上的每一个BINLOG事务都能够被可靠地复制到从库上，主库在每次事务成功提交时，并不及时反馈给前端应用用户，而是等待至少一个从库(详见参数rpl_semi_sync_master_wait_for_slave_count)也接收到BINLOG事务并成功写入中继日志后，主库才返回Commit操作成功给客户端(不管是传统的半同步复制，还是增强的半同步复制，目的都是一样的，只不过两种方式有一个席位地方不同，将在下面说明)

半同步复制保证了事务成功提交后，至少有两份日志记录，一份在主库的BINLOG日志上，另一份在至少一个从库的中继日志Relay Log上，从而更进一步保证了数据的完整性。

在传统的半同步复制中，主库写数据到BINLOG，且执行Commit操作后，会一直等待从库的ACK，即从库写入Relay Log后，并将数据落盘，返回给主库消息，通知主库可以返回前端应用操作成功，这样会出现一个问题，就是实际上主库已经将该事务Commit到了事务引擎层，应用已经可以可以看到数据发生了变化，只是在等待返回而已，如果此时主库宕机，有可能从库还没能写入Relay Log，就会发生主从库不一致。

增强半同步复制就是为了解决这个问题，做了微调，即主库写数据到BINLOG后，就开始等待从库的应答ACK，直到至少一个从库写入Relay Log后，并将数据落盘，然后返回给主库消息，通知主库可以执行Commit操作，然后主库开始提交到事务引擎层，应用此时可以看到数据发生了变化。

半同步复制模式下，假如在传送BINLOG日志到从库时，从库宕机或者网络延迟，导致BINLOG并没有即使地传送到从库上，此时主库上的事务会等待一段时间(时间长短由参数rpl_semi_sync_master_timeout设置的毫秒数决定)，如果BINLOG在这段时间内都无法成功发送到从库上，则MySQL自动调整复制为异步模式，事务正常返回提交结果给客户端。

半同步复制很大程度上取决于主从库之间的网络情况，往返时延RTT越小决定了从库的实时性越好。通俗地说，主从库之间的网络越快，从库约实时。

注意：往返时延RTT(Round-Trip Time)在计算机网络中是一个重要的性能指标，它表示从发送端发送数据开始到发送端接收到接收端的确认，总共经历的时长(这里可能有点拗口，我们可以理解为TCP三次握手的前两次握手)。

### 9.**MySQL 主从复制的流程是怎么样的?**

MySQL的复制原理：Master上面事务提交时会将该事务的binlog event写入binlog file,然后master将binlog event传到slave上面，slave应用该binlog event实现逻辑复制。

1、Master 上面的 `binlog dump` 线程，该线程负责将 master 的 `binlog event` 传到 `slave`。 2、Slave 上面的 IO 线程，该线程负责接收 Master 传过来的 binlog，并写入 `relay log` 。 3、Slave 上面的 SQL 线程，该线程负责读取 `relay log` 并执行。 4、如果是多线程复制，无论是 5.6 库级别的假多线程还是 MariaDB 或者 5.7 的真正的多线程复制， SQL 线程只做 coordinator ，只负责把 `relay log` 中的 `binlog` 读出来然后交给 `worker` 线程， woker 线程负责具体 `binlog event` 的执行。

### 10.一致性延时性，数据恢复

**一致性可以从以下几个方面来讲：**

a. 在MySQL5.5以及之前，slave的SQL线程执行的relay log的位置只能保存在文件（relay-log.info）里面，并且该文件默认每执行10000次事务做一次同步到磁盘，这意味着slave意外crash重启时，SQL线程执行到的位置和数据库的数据是不一致的，将导致复制报错，如果不重搭复制，则有可能会导致数据不一致。MySQL 5.6引入参数relay_log_info_repository，将该参数设置为TABLE时，MySQL将SQL线程执行到的位置存到mysql.slave_relay_log_info表，这样更新该表的位置和SQL线程执行的用户事务绑定成一个事务，这样slave意外宕机后，slave通过innodb的崩溃恢复可以把SQL线程执行到的位置和用户事务恢复到一致性的状态。

b. MySQL 5.6引入GTID复制，每个GTID对应的事务在每个实例上面最多执行一次，这极大地提高了复制的数据一致性；

c. MySQL 5.5引入半同步复制，用户安装半同步复制插件并且开启参数后，设置超时时间，可保证在超时时间内如果binlog不传到slave上面，那么用户提交事务时不会返回，直到超时后切成异步复制，但是如果切成异步之前用户线程提交时在master上面等待的时候，事务已经提交，该事务对master上面的其他session是可见的，如果这时master宕机，那么到slave上面该事务又不可见了，该问题直到5.7才解决；

d. MySQL 5.7引入无损半同步复制，引入参rpl_semi_sync_master_wait_point，该参数默认为after_sync，指的是在切成半同步之前，事务不提交，而是接收到slave的ACK确认之后才提交该事务，从此，复制真正可以做到无损的了。

**e.** 可以再说一下5.7的无损复制情况下，master意外宕机，重启后发现有binlog没传到slave上面，这部分binlog怎么办？？？分2种情况讨论，1宕机时已经切成异步了，2是宕机时还没切成异步？？？**这个怎么判断宕机时有没有切成异步呢？？？分别怎么处理？？？**

**延时性：**

可以讲下5.5是单线程复制，5.6是多库复制（对于单库或者单表的并发操作是没用的），5.7是真正意义的多线程复制，它的原理是基于group commit，只要master上面的事务是group commit的，那slave上面也可以通过多个worker线程去并发执行。和MairaDB10.0.0.5引入多线程复制的原理基本一样。

### 11.主从复制

主从复制原理

(1) Master 将数据改变记录到二进制日志(binary log)中，也就是配置文件 log-bin 指定的文件， 这些记录叫做二进制日志事件(binary log events)； (2) Slave 通过 I/O 线程读取 Master 中的 binary log events 并写入到它的中继日志(relay log)； (3) Slave 重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完 成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放)。

注意事项

(1)主从服务器操作系统版本和位数一致； (2) Master 和 Slave 数据库的版本要一致； (3) Master 和 Slave 数据库中的数据要一致； (4) Master 开启二进制日志，Master 和 Slave 的 server_id 在局域网内必须唯一；

配置主从复制步骤

Master数据库

(1) 安装数据库； (2) 修改数据库配置文件，指明 server_id，开启二进制日志(log-bin)； (3) 启动数据库，查看当前是哪个日志，position 号是多少； (4) 登录数据库，授权数据复制用户（IP 地址为从机 IP 地址，如果是双向主从，这里的 还需要授权本机 的 IP 地址，此时自己的 IP 地址就是从 IP 地址)； (5) 备份数据库（记得加锁和解锁）； (6) 传送备份数据到 Slave 上； (7) 启动数据库； 以上步骤，为单向主从搭建成功，想搭建双向主从需要的步骤： (1) 登录数据库，指定 Master 的地址、用户、密码等信息（此步仅双向主从时需要）； (2) 开启同步，查看状态；

Slave 上的配置

(1) 安装数据库； (2) 修改数据库配置文件，指明 server_id（如果是搭建双向主从的话，也要开启二进制 日志 logbin）； (3) 启动数据库，还原备份； (4) 查看当前是哪个日志，position 号是多少（单向主从此步不需要，双向主从需要）； (5) 指定 Master 的地址、用户、密码等信息； (6) 开启同步，查看状态。







## 索引

### 1.索引是什么

mysql的索引说白了就是用一个数据结构组织某一列的数据，然后如果你要根据那一列的数据查询的时候，就可以不用全表扫描，只要根据那个特定的数据结构去找到那一列的值，然后找到对应的行的物理地址即可。

索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。更通俗的说，索引就相当于目录。当你在用新华字典时，帮你把目录撕掉了，你查询某个字开头的成语只能从第一页翻到第一千页。累！把目录还给你，则能快速定位！

### 2.创建索引的原则

创建索引是我们提高数据库查询数据效率最常用的办法，也是很重要的办法。下面是常见的创建索引的原则。

- 选择唯一性索引：唯一性索引一般基于Hash算法实现，可以快速、唯一地定位某条数据。
- 为经常需要排序、分组和联合操作的字段建立索引。
- 为常作为查询条件的字段建立索引。
- 限制索引的数量：索引越多，数据更新表越慢，因为在数据更新时会不断计算和添加索引。
- 尽量使用数据量少的索引：如果索引的值很长，则占用的磁盘变大，查询速度会受到影响。
- 尽量使用前缀来索引：如果索引字段的值过长，则不但影响索引的大小，而且会降低索引的执行效率，这时需要使用字段的部分前缀来作为索引。
- 删除不再使用或者很少使用的索引。
- 最左前缀匹配原则，非常重要的原则。
- 尽量选择区分度高的列作为索引：区分度表示字段值不重复的比例。
- 索引列不能参与计算：带函数的查询不建议参与索引。
- 尽量扩展现有索引：联合索引的查询效率比多个独立索引高。

### 3.索引的使用规则

**（1）全列匹配**

这个就是说，你的一个sql里，正好where条件里就用了这3个字段，那么就一定可以用到这个联合索引的。

**（2）最左前缀匹配**

这个就是说，如果你的sql里，正好就用到了联合索引最左边的一个或者几个列表，那么也可以用上这个索引，在索引里查找的时候就用最左边的几个列就行了：

**（3）最左前缀匹配了，但是中间某个值没匹配**

这个是说，如果你的sql里，就用了联合索引的第一个列和第三个列，那么会按照第一个列值在索引里找，找完以后对结果集扫描一遍根据第三个列来过滤，第三个列是不走索引去搜索的，就是有一个额外的过滤的工作，但是还能用到索引，所以也还好。

**（4）没有最左前缀匹配**

那就不行了，一定不会用索引

**（5）前缀匹配**

这个就是说，如果你不是等值的，比如=，>=，<=的操作，而是like操作，那么必须要是like ‘XX%’这种才可以用上索引。

**（6）范围列匹配**

如果你是范围查询，比如>=，<=，between操作，你只能是符合最左前缀的规则才可以范围，范围之后的列就不用索引了。

**（7）包含函数**

如果你对某个列用了函数，比如substring之类的东西，那么那一列不用索引。

select * from product where shop_id=1 and 函数(product_id) = 2

### 4.索引的优缺点

索引的利处

在数据库中个表的某个字段创建索引，所带来的最大益处就是将该字段作为检索条件的时候可以极大的提高检索效率，加快检索时间，降低检索过程中所需要读取的数据量。

索引还有一个非常重要的用途，那就是降低数据的排序成本。

排序分组操作主要消耗的是我们的内存和CPU 资源，如果我们能够在进行排序分组操作中利用好索引，将会极大的降低CPU 资源的消耗。

索引的弊端

索引是完全独立于基础数据之外的一部分数据。假设我们在Table ta 中的Column ca 创建了索引idx_ta_ca，那么任何更新Column ca 的操作，MySQL 都需要在更新表中Column ca 的同时，也更新Column ca 的索引数据，调整因为更新所带来键值变化后的索引信息。所带来的最明显的资源消耗就是增加了更新所带来的IO 量和调整索引所致的计算量。此外，Column ca 的索引idx_ta_ca 是需要占用存储空间的，而且随着Table ta 数据量的增长，idx_ta_ca 所占用的空间也会不断增长。所以索引还会带来存储空间资源消耗的增长。

如何判定是否需要创建索引

- 较频繁的作为查询条件的字段应该创建索引；
- 唯一性太差的字段不适合单独创建索引，即使频繁作为查询条件；唯一性太差的字段主要是指哪些呢？如状态字段，类型字段等等这些字段中存方的数据可能总共就是那么几个几十个值重复使用，每个值都会存在于成千上万或是更多的记录中。
- 更新非常频繁的字段不适合创建索引；
- 不会出现在WHERE 子句中的字段不该创建索引；

### 5.索引

在MySQL 中，主要有四种类型的索引，分别为：B-Tree 索引，Hash 索引，Fulltext 索引和RTree索引。

B-Tree 索引

一般来说，MySQL 中的B-Tree 索引的物理文件大多都是以Balance Tree 的结构来存储的，也就是所有实际需要的数据都存放于Tree 的Leaf Node，而且到任何一个Leaf Node 的最短路径的长度都是完全相同的，所以我们大家都称之为B-Tree 索引。

Innodb 存储引擎的B-Tree 索引实际使用的存储结构实际上是B+Tree，也就是在B-Tree 数据结构的基础上做了很小的改造，在每一个Leaf Node 上面出了存放索引键的相关信息之外，还存储了指向与该Leaf Node 相邻的后一个LeafNode 的指针信息，这主要是为了加快检索多个相邻Leaf Node 的效率考虑。

在Innodb 存储引擎中，存在两种不同形式的索引，一种是Cluster 形式的主键索引（Primary Key），另外一种则是和其他存储引擎（如MyISAM 存储引擎）存放形式基本相同的普通B-Tree 索引，这种索引在Innodb 存储引擎中被称为Secondary Index。

在Primary Key中，Leaf Nodes 存放的是表的实际数据，不仅仅包括主键字段的数据，还包括其他字段的数据，整个数据以主键值有序的排列。而Secondary Index 则和其他普通的B-Tree 索引没有太大的差异，只是在Leaf Nodes 出了存放索引键的相关信息外，还存放了Innodb 的主键值。

所以，在Innodb 中如果通过主键来访问数据效率是非常高的，而如果是通过Secondary Index 来访问数据的话，Innodb 首先通过Secondary Index 的相关信息，通过相应的索引键检索到Leaf Node之后，需要再通过Leaf Node 中存放的主键值再通过主键索引来获取相应的数据行。

Hash 索引

Hash 索引在MySQL 中使用的并不是很多，目前主要是Memory 存储引擎使用，而且在Memory 存储引擎中将Hash 索引作为默认的索引类型。

Hash 索引，实际上就是通过一定的Hash 算法，将需要索引的键值进行Hash 运算，然后将得到的Hash 值存入一个Hash 表中。然后每次需要检索的时候，都会将检索条件进行相同算法的Hash 运算，然后再和Hash 表中的Hash 值进行比较并得出相应的信息。

由于Hash 索引结构的特殊性，其检索效率非常的高，索引的检索可以一次定位，而不需要像BTree索引需要从根节点再到枝节点最后才能访问到页节点这样多次IO 访问，所以Hash 索引的效率要远高于B-Tree 索引。

Full-text 索引

Full-text 索引也就是我们常说的全文索引，目前在MySQL 中仅有MyISAM 存储引擎支持，而且也并不是所有的数据类型都支持全文索引。目前来说，仅有CHAR，VARCHAR 和TEXT 这三种数据类型的列可以建Full-text 索引。

R-Tree 索引

R-Tree 索引可能是我们在其他数据库中很少见到的一种索引类型，主要用来解决空间数据检索的问题。

在MySQL 中，支持一种用来存放空间信息的数据类型GEOMETRY，且基于OpenGIS 规范。在MySQL5.0.16 之前的版本中，仅仅MyISAM 存储引擎支持该数据类型，但是从MySQL5.0.16 版本开始，BDB，Innodb，NDBCluster 和Archive 存储引擎也开始支持该数据类型。当然，虽然多种存储引擎都开始支持GEOMETRY 数据类型，但是仅仅之后MyISAM 存储引擎支持R-Tree 索引。

在MySQL 中采用了具有二次分裂特性的R-Tree 来索引空间数据信息，然后通过几何对象（MRB）信息来创建索引。

### 6.聚集索引和非聚集索引

聚集索引：innodb下的每张表必须要指定一个索引，而聚集索引就是基于主键去建立的。聚集索引的结构类似于{"id":"1","pointer":"0007"}其中pointer指向对应的叶子结点。

非聚集索引：又叫做辅助索引，相当于是对聚集索引的辅助，我们建立的索引都是这类索引，结构类似于{"name":"zhangsan","id":"1"}其中保存着聚集索引中的id。 以上结构仅仅用于帮助理解和记忆，具体的实现结构是偏移量之类的。

非聚簇索引和聚簇索引的关系

非聚簇索引，通常也称之为 「 二级索引 」 ( Secondary Indexes ) 或 「 辅助索引 」 ，一般是指聚簇索引之外的所有其它的索引。 在 InnoDB 中，每个辅助索引中的每条记录都会包含该行的主键列 ( 也就是聚簇索引的键 ) ，以及为辅助索引指定的列。InnoDB 使用此主键值来搜索聚簇索引中的行 如果主键很长，那么辅助索引就会占用更多空间，因此使用短主键是有利的，也是我们所推荐的。

聚簇索引和非聚簇索引的区别

1、首先，我们要认识到聚簇索引和非聚簇索引的划分依据是什么 ？ 答案就是 InnoDB 会使用聚簇索索引来保存数据，而非聚簇索引的目的仅仅是加快查询速度 2、在第一点认知基础上，我们就可以知道聚簇索引是唯一的，一个 InnoDB 表只有一个聚簇索引，而且一定会有一个聚簇索引，如果不存在， Innodb 存储引擎会自动添加一个非聚簇所以可以有多个，而且只能由用户自己添加， InnoDB 默认并不会创建任何非聚簇索引。 3、非聚簇索引中一定包含了聚簇索引的列值，但反过来却不存在。

### 7.为什么不用Hash索引而还要使用B-Tree 索引呢？

虽然Hash 索引检索效率非常之高，但是Hash 索引本身由于其实的特殊性也带来了很多限制和弊端，主要有以下这些：

1. Hash 索引仅仅只能满足“=”,“IN”和“<=>”查询，不能使用范围查询；由于Hash 索引所比较的是进行Hash 运算之后的Hash 值，所以Hash 索引只能用于等值的 过滤，而不能用于基于范围的过滤，因为经过相应的Hash 算法处理之后的Hash 值的大小关系，并不能保证还和Hash 运算之前完全一样。
2. Hash 索引无法被利用来避免数据的排序操作；由于Hash 索引中存放的是经过Hash 计算之后的Hash 值，而且Hash 值的大小关系并不一定和Hash 运算前的键值的完全一样，所以数据库无法利用索引的数据来避免任何和排序运算；
3. Hash 索引不能利用部分索引键查询；对于组合索引，Hash 索引在计算Hash 值的时候是组合索引键合并之后再一起计算Hash 值，而不是单独计算Hash 值，所以当我们通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用到；
4. Hash 索引在任何时候都不能避免表扫面；前面我们已经知道，Hash 索引是将索引键通过Hash 运算之后，将Hash 运算结果的Hash 值和所对应的行指针信息存放于一个Hash 表中，而且由于存在不同索引键存在相同Hash 值的可能，所以即使我们仅仅取满足某个Hash 键值的数据的记录条数，都无法直接从Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较而得到相应的结果。
5. Hash 索引遇到大量Hash 值相等的情况后性能并不一定就会比B-Tree 索引高；对于选择性比较低的索引键，如果我们创建Hash 索引，那么我们将会存在大量记录指针信息存与同一个Hash 值相关连。这样要定位某一条记录的时候就会非常的麻烦，可能会浪费非常多次表数据的访问，而造成整体性能的地下。

### 8.MySQL 中索引的限制

1. MyISAM 存储引擎索引键长度总和不能超过1000 字节；
2. BLOB 和TEXT 类型的列只能创建前缀索引；
3. MySQL 目前不支持函数索引；
4. 使用不等于（!= 或者<>）的时候MySQL 无法使用索引；
5. 过滤字段使用了函数运算后（如abs(column)），MySQL 无法使用索引；
6. Join 语句中Join 条件字段类型不一致的时候MySQL 无法使用索引；
7. 使用LIKE 操作的时候如果条件以通配符开始（ '%abc...'）MySQL 无法使用索引；
8. 使用非等值查询的时候MySQL 无法使用Hash 索引；

### 9.磁盘数据页的存储结构

数据库最终所有的数据（包括我们建的各种表以及表里的数据）都是要存放在磁盘上的文件里的，然后在文件里存放的物理格式就是数据页。

大量的数据页是按顺序一页一页存放的，然后两两相邻的数据页之间会采用双向链表的格式互相引用。

一个数据页在磁盘文件里就是一段数据，可能是二进制或者别的特殊格式的数据，然后数据页里包含两个指针，一个指针指向自己上一个数据页的物理地址，一个指针指向自己下一个数据页的物理地址，大概可以认为类似下面这样。

DataPage: xx=xx, xx=xx, linked_list_pre_pointer=15367, linked_list_next_pointer=34126 || DataPage: xx=xx, xx=xx, linked_list_pre_pointer=23789, linked_list_next_pointer=46589 || DataPage: xx=xx, xx=xx, linked_list_pre_pointer=33198, linked_list_next_pointer=55681

每个数据页里，可以认为就是DataPage打头一直到 || 符号的一段磁盘里的连续的数据，每个数据页，都有一个指针指向自己上一个数据页在磁盘文件里的起始物理位置，比如linked_list_pre_pointer=15367，就是指向了上一个数据页在磁盘文件里的起始物理位置，那个15367可以认为就是在磁盘文件里的position或者offset，同理，也有一个指针指向自己下一个数据页的物理位置。

然后一个数据页内部会存储一行一行的数据，也就是平时我们在一个表里插入的一行一行的数据就会存储在数据页里，然后数据页里的每一行数据都会按照主键大小进行排序存储，同时每一行数据都有指针指向下一行数据的位置，组成单向链表。

### 10.假设没有任何索引，数据库是如何根据查询语句搜索数据的？

数据页之间是组成双向链表的，然后数据页内部的数据行是组成单向链表的，而且数据行是根据主键从小到大排序的。

每个数据页里都会有一个页目录，里面根据数据行的主键存放了一个目录，同时数据行是被分散存储到不同的槽位里去的，所以实际上每个数据页的目录里，就是这个页里每个主键跟所在槽位的映射关系。

假设你要根据主键查找一条数据，而且假设此时你数据库里那个表就没几条数据，那个表总共就一个数据页，那么就太简单了！首先就会先到数据页的页目录里根据主键进行二分查找，然后通过二分查找在目录里迅速定位到主键对应的数据是在哪个槽位里，然后到那个槽位里去，遍历槽位里每一行数据，就能快速找到那个主键对应的数据了。每个槽位里都有一组数据行，你就是在里面遍历查找就可以了。

假设你要是根据非主键的其他字段查找数据呢？

没办法使用主键的那种页目录来二分查找的，只能进入到数据页里，根据单向链表依次遍历查找数据了，这就性能很差了。

假如我们有很多数据页呢？

要是没有建立任何索引，从第一个数据页开始遍历所有数据页，从第一个数据页开始，你得先把第一个数据页从磁盘上读取到内存buffer pool的缓存页里来。

然后你就在第一个数据页对应的缓存页里，按照上述办法查找，假设是根据主键查找的，你可以在数据页的页目录里二分查找，假设你要是根据其他字段查找的，只能是根据数据页内部的单向链表来遍历查找

假如第一个数据页没找到你要的那条数据，只能根据数据页的双向链表去找下一个数据页，然后读取到buffer pool的缓存页里去，然后按一样的方法在一个缓存页内部查找那条数据。

### 11.不断在表中插入数据时，物理存储是如何进行页分裂的？

数据页内部的单向链表，里面就是一行一行的数据，刚开始第一行是个起始行，他的行类型是2，就是最小的一行，然后他有一个指针指向了下一行数据，每一行数据都有自己每个字段的值，然后每一行通过一个指针不停的指向下一行数据，普通的数据行的类型都是0，最后一行是一个类型为3的，就是代表最大的一行。

索引运作的一个核心基础就是要求你后一个数据页的主键值都大于前面一个数据页的主键值。

页分裂。

万一你的主键值都是你自己设置的，那么在增加一个新的数据页的时候，实际上会把前一个数据页里主键值较大的，挪动到新的数据页里来，然后把你新插入的主键值较小的数据挪动到上一个数据页里去，保证新数据页里的主键值一定都比上一个数据页里的主键值大。

页分裂的过程，核心目标就是保证下一个数据页里的主键值都比上一个数据页里的主键值要大。

### 12.基于主键的索引是如何设计的，以及如何根据主键索引查询？

针对主键的索引实际上就是主键目录，这个主键目录呢，就是把每个数据页的页号，还有数据页里最小的主键值放在一起，组成一个索引的目录。

假设你有很多的数据页，在主键目录里就会有很多的数据页和最小主键值，此时你完全可以根据二分查找的方式来找你要找的id到底在哪个数据页里！

### 13.索引的页存储物理结构，是如何用B+树来实现的？

你的表的实际数据是存放在数据页里的，然后你表的索引其实也是存放在页里的，此时索引放在页里之后，就会有索引页，假设你有很多很多的数据页，那么此时你就可以有很多的索引页。

接下来我们又可以把索引页多加一个层级出来，在更高的索引层级里，保存了每个索引页和索引页里的最小主键值。

假设我们要查找id=46的，直接先到最顶层的索引页里去找，直接通过二分查找可以定位到下一步应该到某个索引页里去找，接下来到该索引页里通过二分查找定位，也很快可以定位到数据应该在某个数据页里，再进入该数据页里，就可以找到id=46的那行数据了。

假如你最顶层的那个索引页里存放的下层索引页的页号也太多了，此时可以再次分裂，再加一层索引页。

**这就是一颗B+树**，属于数据结构里的一种树形数据结构，所以一直说MySQL的索引是用B+树来组成的。

当你为一个表的主键建立起来索引之后，其实这个主键的索引就是一颗B+树，然后当你要根据主键来查数据的时候，直接就是从B+树的顶层开始二分查找，一层一层往下定位，最终一直定位到一个数据页里，在数据页内部的目录里二分查找，找到那条数据。

### 14.聚簇索引

如果一颗大的B+树索引数据结构里，叶子节点就是数据页自己本身，那么此时我们就可以称这颗B+树索引为聚簇索引！

在InnoDB存储引擎里，你在对数据增删改的时候，就是直接把你的数据页放在聚簇索引里的，数据就在聚簇索引里，聚簇索引就包含了数据！

如果你的数据页开始进行页分裂了，他此时会调整各个数据页内部的行数据，保证数据页内的主键值都是有顺序的，下一个数据页的所有主键值大于上一个数据页的所有主键值。同时在页分裂的时候，会维护你的上层索引数据结构，在上层索引页里维护你的索引条目，不同的数据页和最小主键值。

然后如果你的数据页越来越多，一个索引页放不下了，此时就会再拉出新的索引页，同时再搞一个上层的索引页，上层索引页里存放的索引条目就是下层索引页页号和最下主键值。

一般索引页里可以放很多索引条目，所以通常而言，即使你是亿级的大表，基本上大表里建的索引的层级也就三四层而已。

这个聚簇索引默认是按照主键来组织的，所以你在增删改数据的时候，一方面会更新数据页，一方面其实会给你自动维护B+树结构的聚簇索引，给新增和更新索引页，这个聚簇索引是默认就会给你建立的。

### 15.针对主键之外的字段建立的二级索引，又是如何运作的？

比如你基于name字段建立了一个索引，那么此时你插入数据的时候，就会重新搞一颗B+树，B+树的叶子节点也是数据页，但是这个数据页里仅仅放主键字段和name字段。

name字段的索引B+树也会构建多层级的索引页，这个索引页里存放的就是下一层的页号和最小name字段值，整体规则都是一样的，只不过存放的都是name字段的值，根据name字段值排序。

假设你要根据name字段来搜索数据，那搜索过程简直都一样了，不就是从name字段的索引B+树里的根节点开始找，一层一层往下找，一直找到叶子节点的数据页里，定位到name字段值对应的主键值。

此时还需要进行“**回表**”，这个回表，就是说还需要根据主键值，再到聚簇索引里从根节点开始，一路找到叶子节点的数据页，定位到主键对应的完整数据行，此时才能把select *要的全部字段值都拿出来。

因为我们根据name字段的索引B+树找到主键之后，还要根据主键去聚簇索引里找，所以一般把name字段这种普通字段的索引称之为二级索引，一级索引就是聚簇索引，这就是普通字段的索引的运行原理。

### 16.插入数据时到底是如何维护好不同索引的B+树的？

首先呢，其实刚开始你一个表搞出来以后，其实他就一个数据页，这个数据页就是属于聚簇索引的一部分，而且目前还是空的。

此时如果你插入数据，就是直接在这个数据页里插入就可以了，也没必要给他弄什么索引页。

然后呢，这个初始的数据页其实就是一个根页，每个数据页内部默认就有一个基于主键的页目录，所以此时你根据主键来搜索都是ok没有问题的，直接在唯一 一个数据页里根据页目录找就行了。

然后你表里的数据越来越多了，此时你的数据页满了，那么就会搞一个新的数据页，然后把你根页面里的数据都拷贝过去，同时再搞一个新的数据页，根据你的主键值的大小进行挪动，让两个新的数据页根据主键值排序，第二个数据页的主键值都大于第一个数据页的主键值。

此时根页就升级为索引页了，这个根页里放的是两个数据页的页号和他们里面最小的主键值。

不停的在表里灌入数据，然后数据页不停的页分裂，此时你的唯一 一个索引页，也就是根页里存放的数据页索引条目越来越多，连你的索引页都放不下了，那你就让一个索引页分裂成两个索引页，然后根页继续往上走一个层级引用了两个索引页。

比如你插入了一个新的name字段值，此时他需要根据name字段的B+树索引的根页面开始，去逐层寻找和定位自己这个新的name字段值应该插入到叶子节点的哪个数据页里去。

此时万一遇到一层里不同的索引页指向不同的下层页号，但是name字段值一样，此时就得根据主键值比较一下。

新的name字段值肯定是插入到主键值较大的那个数据页里去的。

### 17.基于B+树查询的原理

在一个索引B+树中，他有一些特性，那就是数据页/索引页里面的记录都是组成一个单向链表的，而且是按照数据大小有序排列的；然后数据页/索引页互相之间都是组成双向链表的，而且也都是按照数据大小有序排列的，所以其实B+树索引是一个完全有序的数据结构，无论是页内还是页之间。

如果是针对主键之外的字段建立索引的话，实际上本质就是为那个字段的值重新建立另外一颗B+树索引，那个索引B+树的叶子节点，存放的都是数据页，里面放的都是你字段的值和主键值，然后每一层索引页里存放的都是下层页的引用，包括页内的排序规则，页之间的排序规则，B+树索引的搜索规则，都是一样的。

假设我们要根据其他字段的索引来搜索，那么只能基于其他字段的索引B+树快速查找到那个值所对应的主键，接着再次做回表查询，基于主键在聚簇索引的B+树里，重新从根节点开始查找那个主键值，找到主键值对应的完整数据。

好处

你可以直接根据某个字段的索引B+树来查找数据，不需要全表搜索，性能提升是很高的。

坏处

主要是两个缺点，一个是空间上的，一个是时间上的。

空间上而言，你要是给很多字段创建很多的索引，那你必须会有很多棵索引B+树，每一棵B+树都要占用很多的磁盘空间啊！所以你要是搞的索引太多了，是很耗费磁盘空间的。

要是搞了很多索引，那么你在进行增删改查的时候，每次都需要维护各个索引的数据有序性，因为每个索引B+树都要求页内是按照值大小排序的，页之间也是有序的，下一个页的所有值必须大于上一个页的所有值！所以你不停的增删改查，必然会导致各个数据页之间的值大小可能会没有顺序，比如下一个数据页里插入了一个比较小的值，居然比上一个数据页的值要小！此时就没办法了，只能进行数据页的挪动，维护页之间的顺序。或者是你不停的插入数据，各个索引的数据页就要不停的分裂，不停的增加新的索引页，这个过程都是耗费时间的。

所以你要是一个表里搞的索引太多了，很可能就会导致你的增删改的速度就比较差了，也许查询速度确实是可以提高，但是增删改就会受到影响，因此通常来说，我们是不建议一个表里搞的索引太多的！

### 18.联合索引查询原理以及全值匹配规则

假设我们想要搜索：1班+张小强+数学的成绩，此时你可能会写一个类似下面的SQL语句，select * from student_score where class_name='1班' and student_name='张小强' and subject_name='数学'。

你发起的SQL语句里，where条件里的几个字段都是基于等值来查询，都是用的等于号！而且where条件里的几个字段的名称和顺序也跟你的联合索引一模一样！此时就是等值匹配规则，上面的SQL语句是百分百可以用联合索引来查询的。

查询的过程也很简单了，首先到索引页里去找，索引页里有多个数据页的最小值记录，此时直接在索引页里基于二分查找法来找就可以了，先是根据班级名称来找1班这个值对应的数据页，直接可以定位到他所在的数据页，然后你就直接找到索引指向的那个数据页就可以了，在数据页内部本身也是一个单向链表，你也是直接就做二分查找就可以了，先按1班这个值来找，你会发现几条数据都是1班，此时就可以按照张小强这个姓名来二分查找，此时会发现多条数据都是张小强，接着就按照科目名称数学来二分查找。

很快就可以定位到一条数据，1班的张小强的数学科目，他对应的数据的id是127。

### 19.索引使用规则

等值匹配规则

你where语句中的几个字段名称和联合索引的字段完全一样，而且都是基于等号的等值匹配，那百分百会用上我们的索引，即使你where语句里写的字段的顺序和联合索引里的字段顺序不一致，也没关系，MySQL会自动优化为按联合索引的字段顺序去找。

最左侧列匹配

假设我们联合索引是KEY(class_name, student_name, subject_name)，那么不一定必须要在where语句里根据三个字段来查，其实只要根据最左侧的部分字段来查，也是可以的。

比如你可以写select * from student_score where class_name='' and student_name=''，就查某个学生所有科目的成绩，这都是没有问题的。

但是假设你写一个select * from student_score where subject_name=''，那就不行了，因为联合索引的B+树里，是必须先按class_name查，再按student_name查，不能跳过前面两个字段，直接按最后一个subject_name查的。

另外，假设你写一个select * from student_score where class_name='' and subject_name=''，那么只有class_name的值可以在索引里搜索，剩下的subject_name是没法在索引里找的，道理同上。

最左前缀匹配原则

如果你要用like语法来查，比如select * from student_score where class_name like '1%'，查找所有1打头的班级的分数，那么也是可以用到索引的。

但是你如果写class_name like '%班'，在左侧用一个模糊匹配符，那他就没法用索引了，因为不知道你最左前缀是什么。

范围查找规则

我们可以用select * from student_score where class_name>'1班' and class_name<'5班'这样的语句来范围查找某几个班级的分数。

这个时候也是会用到索引的，因为我们的索引的最下层的数据页都是按顺序组成双向链表的，所以完全可以先找到'1班'对应的数据页，再找到'5班'对应的数据页，两个数据页中间的那些数据页，就全都是在你范围内的数据了！

但是如果你要是写select * from student_score where class_name>'1班' and class_name<'5班' and student_name>''，这里只有class_name是可以基于索引来找的，student_name的范围查询是没法用到索引的！

这也是一条规则，就是你的where语句里如果有范围查询，那只有对联合索引里最左侧的列进行范围查询才能用到索引！

等值匹配+范围匹配的规则

如果你要是用select * from student_score where class_name='1班' and student_name>'' and subject_name<''，那么此时你首先可以用class_name在索引里精准定位到一波数据，接着这波数据里的student_name都是按照顺序排列的，所以student_name>''也会基于索引来查找，但是接下来的subject_name<''是不能用索引的。

### 20.当我们在SQL里进行排序的时候，如何才能使用索引？

select * from table order by xx1,xx2,xx3 limit 100这样的SQL语句。

基于where语句通过索引快速筛选出来一波数据，接着放到内存里，或者放在一个临时磁盘文件里，然后通过排序算法按照某个字段走一个排序，最后把排序好的数据返回。万一你要排序的数据量比较大的话，还不能用内存来排序，如果基于磁盘文件来排序，那在MySQL里有一个术语，叫做filesort，这速度就比较慢了。

假设我们建立了一个INDEX(xx1,xx2,xx3)这样的一个联合索引，这个时候默认情况下在索引树里本身就是依次按照xx1,xx2,xx3三个字段的值去排序的，在联合索引的索引树里都排序好了，直接就按照索引树里的顺序，把xx1,xx2,xx3三个字段按照从小到大的值获取前面100条就可以了。然后拿到100条数据的主键再去聚簇索引里回表查询剩余所有的字段。

在你的SQL语句里，应该尽量最好是按照联合索引的字段顺序去进行order by排序，这样就可以直接利用联合索引树里的数据有序性，到索引树里直接按照字段值的顺序去获取你需要的数据了。

如果都是升序排列，直接就从索引树里最小的开始读取一定条数就可以了，要是都是降序排列，就是从索引树里最大的数据开始读取一定的条数就可以了，但是你不能order by语句里有的字段升序有的字段降序，那是不能用索引的。

另外，要是你order by语句里有的字段不在联合索引里，或者是你对order by语句里的字段用了复杂的函数，这些也不能使用索引去进行排序了。

### 21.当我们在SQL里进行分组的时候，如何才能使用索引？

select count(*) from table group by xx。

似乎看起来必须把你所有的数据放到一个临时磁盘文件里还有加上部分内存，去搞一个分组，按照指定字段的值分成一组一组的，接着对每一组都执行一个聚合函数，这个性能也是极差的，因为毕竟涉及大量的磁盘交互。

对于group by后的字段，最好也是按照联合索引里的最左侧的字段开始，按顺序排列开来，这样的话，其实就可以完美的运用上索引来直接提取一组一组的数据，然后针对每一组的数据执行聚合函数就可以了。

我们平时设计表里的索引的时候，必须充分考虑到后续你的SQL语句要怎么写，大概会根据哪些字段来进行where语句里的筛选和过滤？大概会根据哪些字段来进行排序和分组？

然后在考虑好之后，就可以为表设计两三个常用的索引，覆盖常见的where筛选、order by排序和group by分组的需求，保证常见的SQL语句都可以用上索引，这样你真正系统跑起来，起码是不会有太大的查询性能问题了。

对于更新语句而言，其实最核心的就是三大问题，一个是你索引别太多，索引太多了，更新的时候维护很多索引树肯定是不行的；一个是可能会涉及到一些锁等待和死锁的问题；一个就是可能会涉及到MySQL连接池、写redo log文件之类的问题。

### 22.回表查询对性能的损害以及覆盖索引是什么？

假设你是类似select * from table order by xx1,xx2,xx3的语句，可能你就是得从联合索引的索引树里按照顺序取出来所有数据，接着对每一条数据都走一个主键的聚簇索引的查找，其实性能也是不高的。

有的时候MySQL的执行引擎甚至可能会认为，你要是类似select * from table order by xx1,xx2,xx3的语句，相当于是得把联合索引和聚簇索引，两个索引的所有数据都扫描一遍了，那还不如就不走联合索引了，直接全表扫描得了，这样还就扫描一个索引而已。

但是你如果要是select * from table order by xx1,xx2,xx3 limit 10这样的语句，那执行引擎就知道了，你先扫描联合索引的索引树拿到10条数据，接着对10条数据在聚簇索引里查找10次就可以了，那么就还是会走联合索引的。

覆盖索引不是一种索引，他就是一种基于索引查询的方式罢了。

他的意思就是针对类似select xx1,xx2,xx3 from table order by xx1,xx2,xx3这样的语句，这种情况下，你仅仅需要联合索引里的几个字段的值，那么其实就只要扫描联合索引的索引树就可以了，不需要回表去聚簇索引里找其他字段了。

需要的字段值直接在索引树里就能提取出来，不需要回表到聚簇索引，这种查询方式就是覆盖索引。

所以在写SQL语句的时候，一方面是你要注意一下也许你会用到联合索引，但是是否可能会导致大量的回表到聚簇索引，如果需要回表到聚簇索引的次数太多了，可能就直接给你做成全表扫描不走联合索引了；

一方面是尽可能还是在SQL里指定你仅仅需要的几个字段，不要搞一个select *把所有字段都拿出来，甚至最好是直接走覆盖索引的方式，不要去回表到聚簇索引。

即使真的要回表到聚簇索引，那你也尽可能用limit、where之类的语句限定一下回表到聚簇索引的次数，就从联合索引里筛选少数数据，然后再回表到聚簇索引里去，这样性能也会好一些。

### 23.设计索引的时候，我们一般要考虑哪些因素呢？

考虑第一点，就是未来我们对表进行查询的时候，大概会如何来进行查询？

第一个索引设计原则，针对你的SQL语句里的where条件、order by条件以及group by条件去设计索引。

此时你就可以设计一个或者两三个联合索引，每一个联合索引都尽量去包含上你的where、order by、group by里的字段，接着你就要仔细审查每个SQL语句，是不是每个where、order by、group by后面跟的字段顺序，都是某个联合索引的最左侧字段开始的部分字段？

你的索引树里就仅仅包含0和1两种值，根本没法进行快速的二分查找，也根本就没有太大的意义了，所以这种时候，选用这种基数很低的字段放索引里意义就不大了。

一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，那么才能发挥出B+树快速二分查找的优势来。

其次的话，你尽量是对那些**字段的类型比较小的列来设计索引**，比如说什么tinyint之类的，因为他的字段类型比较小，说明这个字段自己本身的值占用磁盘空间小，此时你在搜索的时候性能也会比较好一点。

万一要是你真的有那种varchar(255)的字段，可能里面的值太大了，你觉得都放索引树里太占据磁盘空间了，此时你仔细考虑了一下，发现完全可以换一种策略，也就是仅仅针对这个varchar(255)字段的前20个字符建立索引，就是说，对这个字段里的每个值的前20个字符放在索引树里而已。

此时你建立出来的索引其实类似于KEY my_index(name(20),age,course)，就这样的一个形式，假设name是varchar(255)类型的，但是在索引树里你对name的值仅仅提取前20个字符而已。

此时你在where条件里搜索的时候，如果是根据name字段来搜索，那么此时就会先到索引树里根据name字段的前20个字符去搜索，定位到之后前20个字符的前缀匹配的部分数据之后，再回到聚簇索引提取出来完整的name字段值进行比对就可以了。

但是假如你要是order by name，那么此时你的name因为在索引树里仅仅包含了前20个字符，所以这个排序是没法用上索引了！group by也是同理的。

设计索引的时候尽量别把基数很低的字段包含进去，同时针对很长的字符串类型的字段，可以设计前缀索引来进行where查询。

尽量不要让你的查询语句里的字段搞什么函数，或者是搞个计算。

一般设计索引别太多，建议两三个联合索引就应该覆盖掉你这个表的全部查询了。否则索引太多必然导致你增删改数据的时候性能很差，因为要更新多个索引树。

建议大家主键一定是自增的，别用UUID之类的，因为主键自增，那么起码你的聚簇索引不会频繁的分裂，主键值都是有序的，就会自然的新增一个页而已，但是如果你用的是UUID，那么也会导致聚簇索引频繁的页分裂。

### 24.主键

- 当我们在一个 Innodb 表上定义了一个主键，InnoDB 会默认的使用它作为聚簇索引。

使用 InnoDB 存储引擎时，建议为每个表都添加一个主键。如果该表没有一个逻辑唯一且非空列或列集合，那么可以添加一个带有 AUTO_INCREMENT 约束的自增列作为主键，InnoDB 会自动填充该列。

- 如果某个 InnoDB 表并没有定义主键。那么 InnoDB 会查找第一个 「 唯一索引 」( UNIQUE Index ) ，因为唯一索引的所有键 ( key ) 都是 NOT ，因此可以用来作为聚簇索引
- 如果某个 InnoDB 表既没有定义主键，也没有一个合适的唯一索引。InnoDB 会在内部生成一个名为 GEN_CLUST_INDEX 的隐式的聚簇索引

该聚簇索引的键 ( key ) 会包含一个自动为行生成的 ID 值 ( 行号 ) 。 该表中的所有行会按 InnoDB 分配给此类表中的行的 ID 排序。 行 ID 是一个 6 字节的字段，在插入新行时会单调自增。 因此，可以认为物理上的行保存顺序就是该行 ID 排序的排序顺序

### 25.聚簇索引如何加快查询速度

通过聚簇索引访问行很快，因为索引搜索直接指向包含所有行数据页 ( data page )。 如果表很大，与那种索引页与数据页分离的 MyISAM 存储引擎相比， 聚簇索引体系结构通常可以节省磁盘 I/O 操作。

### 26.索引页

几乎所有的 Innodb 的索引都使用 B 树 数据结构，除了空间索引 ( spatial indexes ) 是个例外。 空间索引使用的是 R 树 数据结构 ，这是一种索引多维数据的专用数据结构。 但不管使用的是任何索引结构，索引记录只存储在 B 树 或 R树 数据结构的叶子节点中。索引页的默认大小为 16KB 当有新的记录插入到 InnoDB 聚簇索引中时， InnoDB 会尝试将页面的 1/16 留空，以便将来插入和更新索引记录

- 如果按顺序 ( 升序或降序 ) 插入索引记录，则生成的索引页使用率大约为 15/16
- 如果以随机顺序插入记录，则页面使用率只会在 1/2 到 15/16 之间

InnoDB 在创建或重建 B 树索引时执行批量加载，这种索引创建方法称为 「 排序索引构建 」。我们可以使用配置项 innodb_fill_factor 重新设置在排序索引构建期间填充的每个 B 树页面上的空间利用率，那么剩余的空间会保留以待将来的索引增长。这个设置项我们一般称之为 「 填充因子 」 但是，即使我们把 innodb_fill_factor 配置项的值设置为 100 ，聚簇索引页仍然会保留出 1/16的空间用于将来的索引增长。 另外，需要注意的是 「 空间索引不支持排序索引构建 」 如果 InnoDB 索引页的填充因子低于配置项 MERGE_THRESHOLD 的值 ( 如果未指定，默认情况下为 50%) ，InnoDB 会尝试优化索引树以释放页面空间。 MERGE_THRESHOLD 同时适用于 B 树和 R 树数据结构 我们可以在初始化 MySQL 实例之前设置 innodb_page_size 配置选项来定义 MySQL 实例中所有InnoDB 表空间的页面大小。InnoDB 表空间的大小一旦确定，也就是一旦 MySQL 实例初始化完成后，如果不重新初始化实例，则无法更改它。 目前支持的大小为 64KB ， 32KB ， 16KB ( 默认 ) ， 8KB 和 4KB ，对应于选项值 64k ， 32k ， 16k ，8k 和 4k 。 注意：使用特定 InnoDB 页面大小的 MySQL 实例不能使用来自不同页面大小的实例的数据文件或日志文件，也就是说两个不同数据页大小的 MySQL 实例的数据文件或日志文件是不能互通的。

### 27.排序索引构建

InnoDB 在创建或重建 B 树索引时会执行批量加载，而不是一次只插入一个索引记录。这种索引创建方法也称为排序索引构建。空间索引不支持排序索引构建。 索引构建有三个阶段： 1、 在第一阶段，扫描 聚簇索引，生成索引条目 ( entries ) 并将其添加到排序缓冲区。当排序缓冲区满了，索引条目将被排序并写入临时中间文件。这个过程也称为 「 运行 」 ( run )。 2、 在第二阶段，将一个或多个 「 运行 」 写入临时中间文件，并对文件中的所有条目执行合并排序 3、 在第三个也是最后一个阶段，已排序的条目将插入到 B 树中

自顶向下的索引构建方法

在引入引入排序索引构建之前，索引的生成和重建方式是使用插入 API 一次只将一个索引条目插入到 B树。这种方式会创建并打开 B 树游标以查找插入位置，然后使用 「 乐观插入 」 ( optimistic insert ) 将条目插入 B 树页面。 如果由于页面已满而导致插入失败，则将执行 「 悲观插入 」 ( pessimistic insert ) ，这种方式会创建并打开 B 树游标并根据需要拆分和合并 B 树节点以查找条目的空间。这种 「 自上而下 」的构建索引方法的缺点是搜索插入位置的成本太高且B 树节点需要频繁的拆分和合并

自下而上的排序构建方法

排序索引构建使用 「 自下而上 」 方法来构建索引。使用这种方式，B 树的所有级别始终都会持有最右边的叶子页面的引用。在插入索引时，会分配必要的 B 树深度的最右侧叶页，并根据其排序顺序插入条目。一旦叶子页面已满，节点指针将附加到父页面，并为下一个插入分配一个兄弟叶页面。这个过程会一直重复，直到插入所有条目，这可能导致插入到根级别。当分配兄弟页面时，将释放对先前固定的叶子页面的引用，并且新分配的叶子页面将成为最右侧的叶子页面和新的默认插入位置

### 28.InnoDB 全文索引设计

InnoDB 全文索引采用了 「 倒排 」 索引的设计方式。倒排索引存储单词列表，并为每个单词存储单词出现的文档列表。为了支持邻近搜索，每隔单词的位置信息 ( 字节偏移 ) 也会被同时存储。

### 29.InnoDB 全文索引删除处理

删除具有全文索引列的记录可能会带来辅助索引表中的大量小删除，使得对这些表的并发访问成为性能的瓶颈。 为避免此问题，每当从索引表中删除记录时，已删除文档的文档 ID ( DOC_ID ) 将记录在特殊的FTS***DELETED 表中，同时全文索引仍然会保留索引记录。全文查询返回结果前，会自动过滤掉FTS*** DELETED 表中存储的已删除的文档 ID。 这种设计的好处是删除快速且廉价。缺点是删除记录后索引的大小不会立即减少。 要删除已删除记录的全文索引条目，需要设置 innodb_optimize_fulltext_only = ON 在索引表上运行 OPTIMIZE TABLE 以重建全文索引。

### 30.InnoDB 表和主键

应该始终为 InnoDB 表指定一个主键，且定义为主键的列应该符合以下要求： 1、 被最重要的查询引用 2、 永远不会留空 ( NULL ) 3、 永远不会有重复值 4、 很少变更，一旦插入就永保不变

### 31.创建索引

第一种方式：建表时候建索引

第二种方式：使用ALTER TABLE命令去增加索引： ALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。

第三种方式：使用CREATE INDEX命令创建 CREATE INDEX可对表增加普通索引或UNIQUE索引。（但是，不能创建PRIMARY KEY索引）

### 32.索引的基本原理

索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 1、把创建了索引的列的内容进行排序 2、对排序结果生成倒排表 3、在倒排表内容上拼上数据地址链 4、在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据

### 33.索引的数据结构（b树，hash）

mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引）

查询方式： 主键索引区:PI(关联保存的时数据的地址)按主键查询, 普通索引区:si(关联的id的地址,然后再到达上面的地址)。所以按主键查询,速度最快

B+tree性质： 1.）n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。 2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。 5.）B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。

2）哈希索引 简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。

### 34.创建索引的原则

1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2）较频繁作为查询条件的字段才去创建索引 3）更新频繁字段不适合创建索引 4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 6）定义有外键的数据列一定要建立索引。 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 8）对于定义为text、image和bit的数据类型的列不要建立索引。

### 35.索引分类

索引主要分为以下三类： ①单值索引：一个索引只包含单个列，一个表可以有多个单值索引。 ②唯一索引：索引列的值必须唯一，但允许有空值，主键就是唯一索引。 ③复合索引：一个索引包含多个列 索引的结构： ①BTREE索引；②Hash索引；③Full-Text索引；④R-Tree索引。

### 36.基本语法

①创建索引

```html
create [unique] index indexname on tablename(columnname(length));
alter table tablename add index indexname (columnname(length));
```

注：如果是char、varchar类型的字段，length可以小于字段实际长度；如果是blob、text类型，必须指定length。

②删除索引

```html
drop index indexname on tablename;
```

③查看索引

```html
show index from tablename;
```

④其他创建索引方式

```html
1.添加主键索引
ALTER TABLE `table_name` ADD PRIMARY KEY (`column`)
2.添加唯一索引
ALTER TABLE `table_name` ADD UNIQUE (`column`)
3.添加全文索引
ALTER TABLE `table_name` ADD FULLTEXT (`column`)
4.添加普通索引
ALTER TABLE `table_name` ADD INDEX index_name (`column` )
5.添加组合索引
ALTER TABLE `table_name` ADD INDEX index_name (`column1`, `column2`,`column3`)
```

### 37.建立索引与否的具体情况

①需建立索引的情况 #1.主键自动建立唯一索引。 #2.频繁作为查询条件的字段。 #3.查询中与其他表关联的字段，外键关系建立索引。 #4.高并发下趋向创建组合索引。 #5.查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度。 #6.查询中统计或分组字段。 ②不需要创建索引的情况 #1.表记录太少。（数据量太少MySQL自己就可以搞定了） #2.经常增删改的表。 #3.数据重复且平均分配的字段，如国籍、性别，不适合创建索引。 #4.频繁更新的字段不适合建立索引。 #5.Where条件里用不到的字段不创建索引。

### 38.索引

什么是索引？

索引其实是数据库的一种术语，在关系数据库中，索引是一种单独的、物理的对数据库表中一列或多列的值进行排序的一种存储结 构，它是某个表中一列或若干列值的集合和相应的指向表中物理标识这些值的数据页的逻辑指针清单。索引的作用相当于图书的目 录，可以根据目录中的页码快速找到所需的内容。 索引提供指向存储在表的指定列中的数据值的指针，然后根据您指定的排序顺序对这些指针排序。数据库使用索引以找到特定值，然 后顺指针找到包含该值的行。这样可以使对应于表的SQL语句执行得更快，可快速访问数据库表中的特定信息。

索引底层实现

索引的实现通常使用B树及其变种B+树。 B-Tree 是最常用的用于索引的数据结构。因为它们是时间复杂度低， 查找、删除、插入操作都可以可以在对数时间内完成。另外一个 重要原因存储在B-Tree中的数据是有序的。 哈希表是另外一种你可能看到用作索引的数据结构-这些索引通常被称为哈希索引。使用哈希索引的原因是，在寻找值时哈希表效率极 高。所以，如果使用哈希索引，对于比较字符串是否相等的查询能够极快的检索出的值。

B树

B树事实上是一种平衡的多叉查找树，也就是说最多可以开m个叉（m>=2），我们称之为m阶b树

m阶B树满足以下条件：

- 每个节点至多可以拥有m棵子树。
- 根节点，只有至少有2个节点（要么极端情况，就是一棵树就一个根节点，单细胞生物，即是根，也是叶，也是树)。
- 非根非叶的节点至少有的Ceil(m/2)个子树(Ceil表示向上取整，图中3阶B树，每个节点至少有2个子树，也就是至少有2个叉)。
- 非叶节点中的信息包括[n,A0,K1,A1,K2,A2,…,Kn,An]，，其中n表示该节点中保存的关键字个数，K为关键字且Ki
- 从根到叶子的每一条路径都有相同的长度，也就是说，叶子节在相同的层，并且这些节点不带信息，实际上这些节点就表示找不
- 到指定的值，也就是指向这些节点的指针为空。

B树的查询过程和二叉排序树比较类似，从根节点依次比较每个结点，因为每个节点中的关键字和左右子树都是有序的，所以只要比 较节点中的关键字，或者沿着指针就能很快地找到指定的关键字，如果查找失败，则会返回叶子节点，即空指针。

对于每个结点，主要包含一个关键字数组Key[]，一个指针数组（指向儿子）Son[]。在B-Tree内，查找的流程是：使用顺序查找（数组 长度较短时）或折半查找方法查找Key[]数组，若找到关键字K，则返回该结点的地址及K在Key[]中的位置；否则，可确定K在某个 Key[i]和Key[i+1]之间，则从Son[i]所指的子结点继续查找，直到在某结点中查找成功；或直至找到叶结点且叶结点中的查找仍不成功 时，查找过程失败。 关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)logd((N+1)/2)，检索 一个key，其查找节点个数的渐进复杂度为O(logdN)O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。

B+树

B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 B+ 树是一种树数据结构，是一个n叉树，每个节点通常有多个孩子，一颗B+树包含根节点、内部节点和叶子节点。根节点可能是一个 叶子节点，也可能是一个包含两个或两个以上孩子节点的节点。 B+ 树通常用于数据库和操作系统的文件系统中。 NTFS, ReiserFS, NSS, XFS, JFS, ReFS 和BFS等文件系统都在使用B+树作为元数据索引。 B+ 树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。 B+ 树元素自底向上插入。

看节点之间有重复元素，而且在叶子节点上还用指针连接在了一起，而这些就是B+树的几个特点

1. 每个父节点的元素都出现在了子节点中，分别是子节点最大或者最小的元素。
2. 在上面的这一棵树中，根节点元素8是子节点258的最大的元素，根元素15也是。这时候要注意了，根节点最大的元素等同于整个 B+树的最大的元素，以后无论是怎么插入或者是删除，始终都要保持最大的元素在根节点中。
3. 叶子节点，因为父节点的元素都出现在了子节点当中，因此所有的叶子节点包含了全量的元素信息。

B+树与B树差异

有k个子节点的节点必然有k个元素 非叶子节点仅具有索引作用，跟记录有关的信息均存放在叶子节点中 树的所有叶子节点构成一个有序链表，可以按照元素排序的次序遍历全部记录 B树和B+树的区别在于，B+树的非叶子节点只包含导航信息，不包含实际的值，所有的叶子节点和相连的节点使用链表相连，便于区 间查找和遍历。

B+树的优点

由于B+树在内部节点上不包含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此 访问叶子节点上关联的数据也具有更好的缓存命中率。 B+树的叶子节点都是相连的，因此只要遍历叶子节点就可以实现整颗树的遍历。而且由于数据顺序排列并且相连，所以便于区间查找 和搜索。而B树则需要进行每一层的递归遍历，相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。

B+树的缺点

但是B树也有优点，其优点在于，由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 因为B+树比B树的读写代价更低，所以B+树比B树更适合操作系统的文件索引和数据库索引。

数据库索引分类

根据数据库的功能，可以在数据库设计器中创建四种索引：普通索引、唯一索引、主键索引和聚集索引。

普通索引

最基本的索引类型，没有唯一性之类的限制。普通索引可以通过以下几种方式创建：创建索引，例如 CREATE INDEX <索引的名字> ON tablename (列的列表) ； 修改表，例如 `ALTER TABLE tablename ADD INDEX [索引的名字] (列的列表)` ； 创建表的时候指定索引，例如 CREATE TABLE tablename ( [...], INDEX [索引的名字] (列的列表) ) ；

唯一索引

唯一索引是不允许其中任何两行具有相同索引值的索引。 当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复 键值的新数据。例如，如果在 employee 表中职员的姓 (lname) 上创建了唯一索引，则任何两个员工都不能同姓。 对某个列建立UNIQUE索引后，插入新记录时，数据库管理系统会自动检查新纪录在该列上是否取了重复值，在CREATE TABLE 命令 中的UNIQE约束将隐式创建UNIQUE索引。 创建唯一索引的几种方式： 创建索引，例如 CREATE UNIQUE INDEX <索引的名字> ON tablename (列的列表) ； 修改表，例如 ALTER TABLE tablename ADD UNIQUE [索引的名字] (列的列表) ; 创建表的时候指定索引，例如 `CREATE TABLE tablename ( […], UNIQUE [索引的名字] (列的列表) )` ；

主键索引

简称为主索引，数据库表中一列或列组合（字段）的值唯一标识表中的每一行。该列称为表的主键。在数据库关系图中为表定义主键 将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许 对数据的快速访问。提示尽管唯一索引有助于定位信息，但为获得最佳性能结果，建议改用主键索引。

聚集索引

也称为聚簇索引，在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引， 即如果存在聚 集索引，就不能再指定CLUSTERED 关键字。 索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。聚 集索引更适用于对很少对基表进行增删改操作的情况。 如果在表中创建了主键约束，SQL Server将自动为其产生唯一性约束。在创建主键约束时，指定了CLUSTERED关键字或干脆没有制 定该关键字，SQL Sever将会自动为表生成唯一聚集索引。

操作索引

创建索引

```html
CREATE [UNIQUE] [CLUSTERED| NONCLUSTERED] INDEX <索引名> ON <表名>(<列名>[ASC|DESC] [, <列名>[ASC|DESC]...])
```

– UNIQUE——建立唯一索引。 – CLUSTERED——建立聚集索引。 – NONCLUSTERED——建立非聚集索引。 – ASC——索引升序排序。 – DESC——索引降序排序。

修改索引

对于已经建立的索引，如果需要对其重新命名，可以使用ALTER INDEX 语句。其一般格式为 ALTER INDEX <旧引索名字> RENAME TO<新引索名>

删除索引

当某个时期基本表中数据更新频繁或者某个索引不再需要时，需要删除部分索引。SQL语言使用DROP INDEX 语句删除索引，其一般 格式是： DROP INDEX<索引名> 删除索引时，DBMS不仅在物理删除相关的索引数据，也会从数据字典删除有关该索引的描述。

### 39.为什么B+树更适合作为索引的结构以及索引原理

mysql的B+树索引 查找使用了二分查找，redis 跳表也使用了二分查找法，kafka查询消息日志也使用了二分查找法，二分查找法时间复杂度O(logn);

在MySQL中，主要有四种类型的索引，分别为：B-Tree索引，Hash索引，Fulltext索引(MyISAM 表)和R-Tree索引，本文讲的是B-Tree索引。

后面的索引原理一定要看，太重要了，阿里两个人都问这个mysql的索引原理

mysql使用了 B+索引：

- B树：有序数组+平衡多叉树；
- B+树：有序数组链表+平衡多叉树；

Mysql索引主要有两种结构：B+Tree索引和Hash索引

(a) Inodb存储引擎 默认是 B+Tree索引

(b) MyISAM 存储引擎 默认是Fulltext索引；

(c)Memory 存储引擎 默认 Hash索引；

Hash索引

mysql中，**只有Memory(Memory表只存在内存中，断电会消失，适用于临时表)存储引擎显示支持Hash索引，是Memory表的默认索引类型，尽管Memory表也可以使用B+Tree索引。**

Hash索引把数据以hash形式组织起来，因此当查找某一条记录的时候，速度非常快。但是因为hash结构，每个键只对应一个值，而且是散列的方式分布。所以它并不支持范围查找和排序等功能。

B+Tree索引

**B+Tree是mysql使用最频繁的一个索引数据结构，是Inodb和Myisam存储引擎模式的索引类型。**相对Hash索引，B+Tree在查找单条记录的速度比不上Hash索引，但是因为更适合排序等操作，所以它更受欢迎。毕竟不可能只对数据库进行单条记录的操作。

**带顺序访问指针的B+Tree**

B+Tree所有索引数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都有指向相邻叶子节点的指针。

这样做是为了提高区间效率，例如查询key为从18到49的所有数据记录，当找到18后，只要顺着节点和指针顺序遍历就可以以此向访问到所有数据节点，极大提高了区间查询效率。

**大大减少磁盘I/O读取**

数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点需要一次I/O就可以完全载入。

什么是索引

索引（Index）是帮助数据库高效获取数据的数据结构。索引是在基于数据库表创建的，它包含一个表中某些列的值以及记录对应的地址，并且把这些值存储在一个数据结构中。最常见的就是使用哈希表、B+树作为索引。

一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。

为什么要使用索引

我们知道，数据库查询是数据库最主要的功能之一。而查询速度当然是越快越好。而当数据量越来越大的时候，查询花费的时间会随之增长。而索引，可以加速数据的查询。因为索引是有序排列的。

数据库中使用什么数据结构作为索引

数据库中实际使用的索引并不会是链表结构，因为效率太低了。

我们知道链表的查询效率是O(n)。就像上面的例子，遍历了501次才找到第一条符合条件的记录，这是很低效的。而我们知道，数组+二分查找的效率是O(lgn)，但是数组的插入元素以及删除元素的效率很低，因此使用数组做为索引结构并不合适。

另外，在选择数据库索引的结构的时候，要考虑到另一个问题。索引是存在于磁盘中，当索引非常大的时候，达到几个G的时候，无法一次加载到内存中。

考虑到上面两个因素，数据库中索引使用的是树形结构。

各种树的名字

B-Tree B+-Tree B*-Tree

首先要明白三种树名中的“-”起到的是分隔的作用，并不是“减”的意思。

因此正确的翻译应该是B树，B+树，B*树。而不是B-树，B+树，B*树。因此，当你听到别人说“B减树”的时候，要明白它指的是B-Tree。即B树和B-树是同一种树。

为什么要强调上面这一点呢，因为有的博文中写的是：B树是二叉树，B-树是多路搜索树。

然而B树和B-树都是指B-Tree。

也就是说，B-Tree并不是Binart tree。B-Tree的中文名是平衡多路搜索树。（B树的相关介绍在下面）

平衡二叉树

树形结构是计算机系统里最重要的数据结构。

我们知道，二叉树的查找的时间复杂度是O(log2N)，其查找效率与深度有关，而普通的二叉树可能由于内部节点排列问题退化成链表，这样查找效率就会很低。因此平衡二叉树是更好的选择，因为它保持平衡，即通过旋转调整结构保持最小的深度。其查找的时间复杂度也是O(log2N)。

但实际上，数据库中索引的结构也并非AVL树或更优秀的红黑树，尽管它的查询的时间复杂度很低。

为什么平衡二叉树也不适合作为索引

之前说了平衡树的查找时间复杂度是O(log2N)，已经很不错了，但还是不适合作为索引结构。那么肯定是有一种更适合作为索引的数据结构。

那么这个更适合作为索引的数据结构，难道是查找的时间复杂度更低吗？并不是。这种作为索引的数据结构的查找的时间复杂度也近似O(log2N)。

那为什么平衡二叉树不适合作为索引呢？

索引是存在于索引文件中，是存在于磁盘中的。因为索引通常是很大的，因此无法一次将全部索引加载到内存当中，因此每次只能从磁盘中读取一个磁盘页的数据到内存中。而这个磁盘的读取的速度较内存中的读取速度而言是差了好几个级别。

> 注意，我们说的平衡二叉树结构，指的是逻辑结构上的平衡二叉树，其物理实现是数组。然后由于在逻辑结构上相近的节点在物理结构上可能会差很远。因此，每次读取的磁盘页的数据中有许多是用不上的。因此，查找过程中要进行许多次的磁盘读取操作。

而适合作为索引的结构应该是尽可能少的执行磁盘IO操作，因为执行磁盘IO操作非常的耗时。因此，平衡二叉树并不适合作为索引结构。

B-Tree适合作为索引

平衡二叉树不适合作为索引。那么什么才适合作为索引——B树。

平衡二叉树没能充分利用磁盘预读功能，而B树是为了充分利用磁盘预读功能来而创建的一种数据结构，也就是说B树就是为了作为索引才被发明出来的的。

来看看关于“局部性原理与磁盘预读”的知识：

局部性原理与磁盘预读：

由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：

- 当一个数据被用到时，其附近的数据也通常会马上被使用。
- 程序运行期间所需要的数据通常比较集中。
- 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。

搞清楚上面的意思。磁盘预读是具体实现，其理论依据是局部性原理。

为什么说红黑树没能充分利用磁盘预读功能，引用一篇博文的一段话：

> 红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。

也就是说，使用红黑树（平衡二叉树）结构的话，每次磁盘预读中的很多数据是用不上的数据。因此，它没能利用好磁盘预读的提供的数据。然后又由于深度大（较B树而言），所以进行的磁盘IO操作更多。

B树的每个节点可以存储多个关键字，它将节点大小设置为磁盘页的大小，充分利用了磁盘预读的功能。每次读取磁盘页时就会读取一整个节点。也正因每个节点存储着非常多个关键字，树的深度就会非常的小。进而要执行的磁盘读取操作次数就会非常少，更多的是在内存中对读取进来的数据进行查找。

B树的查询，主要发生在内存中，而平衡二叉树的查询，则是发生在磁盘读取中。因此，虽然B树查询查询的次数不比平衡二叉树的次数少，但是相比起磁盘IO速度，内存中比较的耗时就可以忽略不计了。因此，B树更适合作为索引。

比B树更适合作为索引的结构——B+树

比B树更适合作为索引的结构是B+树。MySQL中也是使用B+树作为索引。它是B树的变种，因此是基于B树来改进的。为什么B+树会比B树更加优秀呢？

- B树：有序数组+平衡多叉树；
- B+树：有序数组链表+平衡多叉树；

B+树的关键字全部存放在叶子节点中，非叶子节点用来做索引，而叶子节点中有一个指针指向一下个叶子节点。做这个优化的目的是为了提高区间访问的性能。而正是这个特性决定了B+树更适合用来存储外部数据。

引用一段话：

> 走进搜索引擎的作者梁斌老师针对B树、B+树给出了他的意见（为了真实性，特引用其原话，未作任何改动）：“B+树还有一个最大的好处，方便扫库，B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了，B+树支持range-query非常方便，而B树不支持。这是数据库选用B+树的最主要原因。

比如要查 5-10之间的，B+树一把到5这个标记，再一把到10，然后串起来就行了，B树就非常麻烦。B树的好处，就是成功查询特别有利，因为树的高度总体要比B+树矮。不成功的情况下，B树也比B+树稍稍占一点点便宜。

B树比如你的例子中查，17的话，一把就得到结果了，

有很多基于频率的搜索是选用B树，越频繁query的结点越往根上走，前提是需要对query做统计，而且要对key做一些变化。

另外B树也好B+树也好，根或者上面几层因为被反复query，所以这几块基本都在内存中，不会出现读磁盘IO，一般已启动的时候，就会主动换入内存。”

举个例子来对比。

**B树：**

比如说，我们要查找关键字范围在3到7的关键字，在找到第一个符合条件的数字3后，访问完第一个关键字所在的块后，得遍历这个B树，获取下一个块，直到遇到一个不符合条件的关键字。遍历的过程是比较复杂的。

**B+树(叶节点保存数据，其他的节点 全部存放索引):**

相比之下，B+树的基于范围的查询简洁很多。由于叶子节点有指向下一个叶子节点的指针，因此从块1到块2的访问，通过块1指向块2的指针即可。从块2到块3也是通过一个指针即可。

引用一篇博文中网友评论的一段话：

> 数据库索引采用B+树的主要原因是B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。

> B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）。

正如上面所说，在数据库中基于范围的查询是非常频繁的，因此MySQL最终选择的索引结构是B+树而不是B树。

索引的原理

索引原理

索引的目的在于提高查询效率，与我们查阅图书所用的目录是一个道理：先定位到章，然后定位到该章下的一个小节，然后找到页数。相似的例子还有：查字典，查火车车次，飞机航班等

**本质都是：通过不断地缩小想要获取数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是说，有了这种索引机制，我们可以总是用同一种查找方式来锁定数据。**

数据库也是一样，但显然要复杂的多，因为不仅面临着等值查询，还有范围查询(>、<、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？

最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段......这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。

但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的。而数据库实现比较复杂，一方面数据是保存在磁盘上的，另外一方面为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。

磁盘IO与预读

考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。

每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。

索引的数据结构

任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。

那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。

b+树的查找过程

如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。

真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。

b+树性质

1.索引字段要尽量的小

通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；

而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。

这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。

2.索引的最左匹配特性（即从左往右匹配）

当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；

但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。

比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。

**这也是经常考察的，比如 我定义了 A,B,C的联合索引，如果 我只传递了 A,B 能走索引吗？答案是能，因为最左侧原理(百度问过)**

**补充一下. 全文索引（FULLTEXT）=mysql的 myISAM搜索引擎默认的索引类型**

MySQL从3.23.23版开始支持全文索引和全文检索，fulltext索引仅可用于 MyISAM 表；他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加。

对于较大的数据集，将你的资料输入一个没有FULLTEXT索引的表中，然后创建索引，其速度比把资料输入现有FULLTEXT索引的速度更为快。不过切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法。

文本字段上的普通索引只能加快对出现在字段内容最前面的字符串(也就是字段内容开头的字符)进行检索操作。如果字段里存放的是由几个、甚至是多个单词构成的较大段文字，普通索引就没什么作用了。这种检索往往以LIKE %word%的形式出现，这对MySQL来说很复杂，如果需要处理的数据量很大，响应时间就会很长。

这类场合正是全文索引(full-text index)可以大显身手的地方。在生成这种类型的索引时，MySQL将把在文本中出现的所有单词创建为一份清单，查询操作将根据这份清单去检索有关的数据记录。全文索引即可以随数据表一同创建，也可以等日后有必要时再使用下面这条命令添加：

```html
ALTER TABLE table_name ADD FULLTEXT(column1, column2) 
```

有了全文索引，就可以用SELECT查询命令去检索那些包含着一个或多个给定单词的数据记录了。下面是这类查询命令的基本语法：

```html
SELECT * FROM table_name 
WHERE MATCH(column1, column2) AGAINST('word1', 'word2', 'word3')
```

上面这条命令将把column1和column2字段里有word1、word2和word3的数据记录全部查询出来。

索引使用注意事项

1，不要滥用索引

①，索引提高查询速度，却会降低更新表的速度，因为更新表时，mysql不仅要更新数据，保存数据，还要更新索引，保存索引

②，索引会占用磁盘空间

2，索引不会包含含有NULL值的列

复合索引只要有一列含有NULL值,那么这一列对于此符合索引就是无效的，因此我们在设计数据库设计时不要让字段的默认值为NULL。

3，MySQL查询只是用一个索引

如果where字句中使用了索引的话，那么order by中的列是不会使用索引的

4，like

like '%aaa%'不会使用索引而like "aaa%"可以使用索引

选择索引的数据类型

Mysql支持很多数据类型，选择合适的数据类型存储数据对性能有很大的影响。

(1)越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和cpu缓存中都需要更少的空间，处理起来更快。

(2)简单的数据类型更好：整形数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应用内置的日期和时间数据类型，而不是字符串来存储时间；以及用整形数据存储IP地址。

(3)尽量避免NULL：应该制定列为NOT NULL，除非你想存储NULL。在MySQL中，含有空值的列很难进行查询优化，因为他们使得索引、索引的统计信息以及比较运算更加复杂。

MySQL常见索引有：主键索引、唯一索引、普通索引、全文索引、组合索引

1，INDEX（普通索引）：`ALTER TABLE 'table_name' ADD INDEX index_name('col')`

最基本的索引，没有任何限制

2，UNIQUE（唯一索引）：`ALTER TABLE 'table_name' ADD UNIQUE('col')`

与“普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。

3，PRIMARY KEY（主键索引）：`ALTER TABLE 'table_name' ADD PRIMARY KEY('col')`

是一种特殊的唯一索引，不允许有空值。

4，FULLTEXT（全文索引）：`ALTER TABLE 'table_name' ADD FULLTEXT('col')`

仅可用于MyISAM和InoDB，针对较大的数据，生成全文索引很耗时耗空间

组合索引：`ALTER TABLE 'table_name' ADD INDEX index_name('col1','col2','col3')`

为了更多的提高mysql效率可建立组合索引，遵循“最左前缀”原则。创建复合索引应该将最常用（频率）做限制条件的列放在最左边，一次递减。

组合索引最左字段用in是可以用到索引的。相当于建立了col1,col1col2,col1col2col3三个索引

### 40.唯一索引 主键索引 聚集索引

接下来说以下三种不同的索引： 根据数据库的功能，可以在数据库设计器中创建三种索引：唯一索引、主键索引和聚集索引。 提示：尽管唯一索引有助于定位信息，但为获得最佳性能结果，建议改用主键或唯一约束。

唯一索引 唯一索引是不允许其中任何两行具有相同索引值的索引。不是只能建一个索引。 当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。例如，如果在

employee表中职员的姓(lname)上创建了唯一索引，则任何两个员工都不能同姓。

主键索引 数据库表经常有一列或多列组合，其值唯一标识表中的每一行。该列称为表的主键。在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。

聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。

索引列 可以基于数据库表中的单列或多列创建索引。多列索引可以区分其中一列可能有相同值的行。

如果经常同时搜索两列或多列或按两列或多列排序时，索引也很有帮助。例如，如果经常在同一查询中为姓和名两列设置判据，那么在这两列上创建多列索引将很有意义。

### 41.MySQ索引的原理和数据结构能介绍一下吗？b+树和b-树有什么区别？MySQL聚簇索引和非聚簇索引的区别是什么？他们分别是如何存储的？使用MySQL索引都有哪些原则？MySQL复合索引如何使用？

1.**索引的数据结构**

其实就是让你聊聊mysql的索引底层是什么数据结构实现的，弄不好现场还会让你画一画索引的数据结构，然后会问问你mysql索引的常见使用原则，弄不好还会拿个SQL来问你，就这SQL建个索引一半咋建？

索引是啥？总不能让我来解释了吧，这个问题太基础了，大家都知道，mysql的索引说白了就是用一个数据结构组织某一列的数据，然后如果你要根据那一列的数据查询的时候，就可以不用全表扫描，只要根据那个特定的数据结构去找到那一列的值，然后找到对应的行的物理地址即可。

可以用一颗随意准备的，不知道是什么的树，来演示一把，大家就随便看看，理解个以上。

那么回答面试官的一个问题，mysql的索引是怎么实现的？答案是，不是二叉树，也不是一颗乱七八糟的树，而是一颗b+树。这个很多人都会这么回答，然后面试官一定会追问，那么你能聊聊b+树吗？

所以下面咱们先来聊聊b-树是啥，从数据结构的角度来看，b-树要满足下面的条件：

（1）d为大于1的一个正整数，称为B-Tree的度。

（2）h为一个正整数，称为B-Tree的高度。

（3）每个非叶子节点由n-1个key和n个指针组成，其中d<=n<=2d。

（4）每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。

（5）所有叶节点具有相同的深度，等于树高h。

（6）key和指针互相间隔，节点两端是指针。

（7）一个节点中的key从左到右非递减排列。

（8）所有节点组成树结构。

（9）每个指针要么为null，要么指向另外一个节点。

（10）如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。

（11）如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。

（12）如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。

上面那段规则，我也是从网上找的，说实话，没几个java程序员能耐心去看明白或者是背下来，大概知道是个树就好了。真是疯了，好多好多，那就拿个网上的图给大家示范一下吧：

比如说我们现在有一张表：

(

id int

name varchar

age int

)

我们现在对id建个索引：15、56、77、20、49

select * from table where id = 49

select * from table where id = 15

反正大概就长上面那个样子，查找的时候，就是从根节点开始二分查找。大概就知道这个是事儿就好了，深讲里面的数学问题和算法问题，时间根本不够，面试官也没指望你去讲里面的数学和算法问题，因为我估计他自己也不一定能记住。

好了，过，直接看下一个，b+树。b+树是b-树的变种，啥叫变种？就是说一些原则上不太一样了，稍微有点变化，同样的一套数据，放b-树和b+树看着排列不太一样的。而mysql里面一般就是b+树来实现索引，所以b+树很重要。

b+树跟b-树不太一样的地方在于：

（1）每个节点的指针上限为2d而不是2d+1。

（2）内节点不存储data，只存储key；叶子节点不存储指针。

select * from table where id = 15

select * from table where id>=18 and id<=49

但是一般数据库的索引都对b+树进行了优化，加了顺序访问的指针，如网上弄的一个图，这样在查找范围的时候，就很方便，比如查找18~49之间的数据：

2.myism存储引擎的索引实现

先来看看myisam存储引擎的索引实现。就拿上面那个图，咱们来现场手画一下这个myisam存储的索引实现，在myisam存储引擎的索引中，每个叶子节点的data存放的是数据行的物理地址，比如0x07之类的东西，然后我们可以画一个数据表出来，一行一行的，每行对应一个物理地址。

id=15，data：0x07，0a89，数据行的物理地址

数据文件单独放一个文件

物理地址 id name age

0x07 15 张三 22

0a89 22 李四 25

select * from table where id = 15 -> 0x07物理地址 -> 15，张三，22

myisam最大的特点是数据文件和索引文件是分开的，大家看到了么，上面是索引文件里搜索，然后到数据文件里定位一个行的。

3.**innodb存储引擎的索引**

再来看看innodb存储引擎的索引实现，跟myisam最大的区别在于说，innodb的数据文件本身就是个索引文件，就是key就是主键，然后叶子节点的data就是那个数据行。

innodb存储引擎，要求必须有主键，可以默认内置的就会根据主键建立一个索引，叫做聚簇索引，innodb的数据文件本身同时也是个索引文件，这个索引就是默认根据主键建立的叫做聚簇索引

15，data：0x07，完整的一行数据，（15,张三,22）

22，data：完整的一行数据，（22,李四,30）

innodb这种原生的数据文件就是索引文件的组织结构，就叫默认的主键索引为聚簇索引。就是因为这个原因，innodb表是要求必须有主键的，但是myisam表不要求必须有主键。另外一个是，innodb存储引擎下，如果对某个非主键的字段创建个索引，那么最后那个叶子节点的值就是主键的值，因为可以用主键的值到聚簇索引里根据主键值再次查找到数据。

select * from table where name = ‘张三’

先到name的索引里去找，找到张三对应的叶子节点，叶子节点的data就是那一行的主键，id=15，然后再根据id=15，到数据文件里面的聚簇索引（根据主键组织的索引）根据id=15去定位出来id=15这一行的完整的数据

所以这里就明白了一个道理，为啥innodb下不要用UUID生成的超长字符串作为主键？因为这么玩儿会导致所有的索引的data都是那个主键值，最终导致索引会变得过大，浪费很多磁盘空间。

还有一个道理，一般innodb表里，建议统一用auto_increment自增值作为主键值，因为这样可以保持聚簇索引直接加记录就可以，如果用那种不是单调递增的主键值，可能会导致b+树分裂后重新组织，会浪费时间。

4.**索引的使用规则**

我现在在指导我们架构班的同学在外面跳槽，一般来说，索引这块必问，b+树索引的结构，一般是怎么存放的，出个题，针对这个SQL，索引应该怎么来建立

select * from table where a=1 and b=2 and c=3，你知道不知道，你要怎么建立索引，才可以确保这个SQL使用索引来查询

好了，各位同学，聊到这里，你应该知道具体的myisam和innodb索引的区别了，同时也知道什么是聚簇索引了，现场手画画，都ok了。然后我们再来说几个最最基本的使用索引的基本规则。

其实最基本的，作为一个java码农，你得知道最左前缀匹配原则，这个东西是跟联合索引（复合索引）相关联的，就是说，你很多时候不是对一个一个的字段分别搞一个一个的索引，而是针对几个索引建立一个联合索引的。

给大家举个例子，你如果要对一个商品表按照店铺、商品、创建时间三个维度来查询，那么就可以创建一个联合索引：shop_id、product_id、gmt_create

一般来说，你有一个表（product）：shop_id、product_id、gmt_create，你的SQL语句要根据这3个字段来查询，所以你一般来说不是就建立3个索引，一般来说会针对平时要查询的几个字段，建立一个联合索引

后面在java系统里写的SQL，都必须符合最左前缀匹配原则，确保你所有的sql都可以使用上这个联合索引，通过索引来查询

create index (shop_id,product_id,gmt_create)

（1）全列匹配

这个就是说，你的一个sql里，正好where条件里就用了这3个字段，那么就一定可以用到这个联合索引的：

select * from product where shop_id=1 and product_id=1 and gmt_create=’2018-01-01 10:00:00’

（2）最左前缀匹配

这个就是说，如果你的sql里，正好就用到了联合索引最左边的一个或者几个列表，那么也可以用上这个索引，在索引里查找的时候就用最左边的几个列就行了：

select * from product where shop_id=1 and product_id=1，这个是没问题的，可以用上这个索引的

（3）最左前缀匹配了，但是中间某个值没匹配

这个是说，如果你的sql里，就用了联合索引的第一个列和第三个列，那么会按照第一个列值在索引里找，找完以后对结果集扫描一遍根据第三个列来过滤，第三个列是不走索引去搜索的，就是有一个额外的过滤的工作，但是还能用到索引，所以也还好。

select * from product where shop_id=1 and gmt_create=’2018-01-01 10:00:00’

就是先根据shop_id=1在索引里找，找到比如100行记录，然后对这100行记录再次扫描一遍，过滤出来gmt_create=’2018-01-01 10:00:00’的行

这个我们在线上系统经常遇到这种情况，就是根据联合索引的前一两个列按索引查，然后后面跟一堆复杂的条件，还有函数啥的，但是只要对索引查找结果过滤就好了，根据线上实践，单表几百万数据量的时候，性能也还不错的，简单SQL也就几ms，复杂SQL也就几百ms。可以接受的。

（4）没有最左前缀匹配

那就不行了，那就在搞笑了，一定不会用索引，所以这个错误千万别犯

select * from product where product_id=1，这个肯定不行

（5）前缀匹配

这个就是说，如果你不是等值的，比如=，>=，<=的操作，是like操作，那么必须要是like ‘XX%’这种才可以用上索引，比如说

select * from product where shop_id=1 and product_id=1 and gmt_create like ‘2018%’

（6）范围列匹配

如果你是范围查询，比如>=，<=，between操作，你只能是符合最左前缀的规则才可以范围，范围之后的列就不用索引了

select * from product where shop_id>=1 and product_id=1

这里就在联合索引中根据shop_id来查询了

（7）包含函数

如果你对某个列用了函数，比如substring之类的东西，那么那一列不用索引

select * from product where shop_id=1 and 函数(product_id) = 2

上面就根据shop_id在联合索引中查询

5.**索引的缺点以及使用注意**

索引是有缺点的，比如常见的就是会增加磁盘消耗，因为要占用磁盘文件，同时高并发的时候频繁插入和修改索引，会导致性能损耗的。我们给的建议，尽量创建少的索引，比如说一个表一两个索引，两三个索引，十来个，20个索引，高并发场景下还可以。

字段，status，100行，status就2个值，0和1

你觉得你建立索引还有意义吗？几乎跟全表扫描都差不多了

select * from table where status=1，相当于是把100行里的50行都扫一遍

你有个id字段，每个id都不太一样，建立个索引，这个时候其实用索引效果就很好，你比如为了定位到某个id的行，其实通过索引二分查找，可以大大减少要扫描的数据量，性能是非常好的

在创建索引的时候，要注意一个选择性的问题，select count(discount(col)) / count(*)，就可以看看选择性，就是这个列的唯一值在总行数的占比，如果过低，就代表这个字段的值其实都差不多，或者很多行的这个值都类似的，那创建索引几乎没什么意义，你搜一个值定位到一大坨行，还得重新扫描。

就是要一个字段的值几乎都不太一样，此时用索引的效果才是最好的

还有一种特殊的索引叫做前缀索引，就是说，某个字段是字符串，很长，如果你要建立索引，最好就对这个字符串的前缀来创建，比如前10个字符这样子，要用前多少位的字符串创建前缀索引，就对不同长度的前缀看看选择性就好了，一般前缀长度越长选择性的值越高。

### 42.数据库索引

> - 什么是索引？

**索引是对数据库表中一个或多个列的值进行排序的数据结构，以协助快速查询、更新数据库表中数据。**

你也可以这样理解：索引就是加快检索表中数据的方法。数据库的索引类似于书籍的索引。在书籍中，索引允许用户不必翻阅完整个书就能迅速地找到所需要的信息。在数据库中，索引也允许数据库程序迅速地找到表中的数据，而不必扫描整个数据库。

> - 底层数据结构是什么，为什么使用这种数据结构？

**（1）底层数据结构是B+树：** 在数据结构中，我们最为常见的搜索结构就是二叉搜索树和AVL树(高度平衡的二叉搜索树，为了提高二叉搜索树的效率，减少树的平均搜索长度)了。然而，无论二叉搜索树还是AVL树，当数据量比较大时，都会由于树的深度过大而造成I/O读写过于频繁，进而导致查询效率低下，因此对于索引而言，多叉树结构成为不二选择。特别地，B-Tree的各种操作能使B树保持较低的高度，从而保证高效的查找效率。

**（2）使用B+树的原因：** 查找速度快、效率高，在查找的过程中，每次都能抛弃掉一部分节点，减少遍历个数。（此时，你应该在白纸上画出什么是B+树）

> - 索引的分类？

- **唯一索引**：唯一索引不允许两行具有相同的索引值
- **主键索引**：为表定义一个主键将自动创建主键索引，主键索引是唯一索引的特殊类型。主键索引要求主键中的每个值是唯一的，并且不能为空
- **聚集索引(Clustered)**：表中各行的物理顺序与键值的逻辑（索引）顺序相同，每个表只能有一个
- **非聚集索引(Non-clustered)**：非聚集索引指定表的逻辑顺序。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于249个

> - 索引的优缺点？

**（1）优点：**

- **大大加快数据的检索速度**，这也是创建索引的最主要的原因；
- 加速表和表之间的连接；
- 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间；
- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性；

**（2）缺点：**

- 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度；
- 空间方面：索引需要占物理空间。

> - 什么样的字段适合创建索引？

- 经常作查询选择的字段
- 经常作表连接的字段
- 经常出现在order by, group by, distinct 后面的字段

> - 创建索引时需要注意什么？

- **非空字段**：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；
- **取值离散大的字段**：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；
- **索引字段越小越好**：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。

## 约束

### 1.AUTO_INCREMENT 约束

AUTO_INCREMENT 是 Innodb 提供的一种可配置的锁定机制，如果某个表的某一列具有AUTO_INCREMENT 约束，那么向该表添加数据的时候可以很明显的提高 SQL 语句的性能和可伸缩性。

为了充分使用 Innodb 表的 AUTO_INCREMENT 机制，必须 将 AUTO_INCREMENT 字段 ( 或列，下面提到的 「 列 」 和字段可以等价 ) 定义为 「 索引 」 的一部分，这样就可以在表上使用索引执行下面的语句来查找最大的列值。

```html
SELECT MAX (ai_col ) FROM tablename;
```

通常情况下，为了最大化性能，添加了 AUTO_INCREMENT 约束的列要么独自成一个索引 ( 主索引 )，那么是组合索引中的第一列。

需要注意的是： 虽然我们日常使用中会把 AUTO_INCREMENT 添加为主键，但它其实也可以不是主键的。甚至可以不是唯一索引。

AUTO_INCREMENT 不仅仅是一个字段约束条件，它还是一个 「 锁 」，也就是那个很少见到的 「AUTO_INCREMENT 锁」。

### 2.Innodb AUTO_INCREMENT 锁模式

「 AUTO_INCREMENT 锁」模式的配置变量为 innodb_autoinc_lock_mode ，我们可以通过下面的语句查看当前的模式是什么

```html
show variables like 'innodb_autoinc_lock_mode';
```

配置参数 innodb_autoinc_lock_mode 有三个可选的值，分别是 0 、1 和 2 ，分别代表着 「 传统」，「 连续 」 或 「 交错 」 三种锁模式

简单理解这三种锁模式 1、 传统锁模式 - 不管三七二十一，先用表级的 AUTO-INC 锁，直到语句插入完成，然后释放锁 2、 连续锁模式 - 不管三七二十一，先用互斥锁，然后生成所有插入行需要的自增值，然后释放互斥锁，最后使用这些自增值来插入数据。对于行数未知的，那就只能使用 「 传统锁 」 模式了，先锁起来，执行完毕了再释放，因为人家不知道要生成多少自增值啊 3、 交错模式 - 管它刮风下雨，需要的时候再生成，也管它连续与否，用了就是了...

### 3.SQL 约束有哪几种？

- NOT NULL: 用于控制字段的内容一定不能为空（NULL）。
- UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。
- PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。
- FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。
- CHECK: 用于控制字段的值范围。



## 事务

### 1.数据库事务

数据库事务执行一系列基本操作，这些基本操作组成一个逻辑工作单元一起向数据库提交，要么都执行，要么都不执行。事务是一个不可分割的工作逻辑单元。

事务必须具备以下4个属性，简称ACID属性。

- 原子性（Atomicity）：事务是一个完整操作，参与事务的逻辑单元要么都执行，要么都不执行。
- 一致性（Consistency）：在事务执行完毕时（无论是正常执行完毕还是异常退出），数据都必须处于一致状态。
- 隔离性（Isolation）：对数据进行修改的所有并发事务都是彼此隔离的，它不应以任何方式依赖或影响其他事务。
- 永久性（Durability）：在事务操作完成后，对数据的修改将被持久化到永久性存储中。

### 2.事务隔离级别

（1）读未提交，Read Uncommitted：某个事务还没提交的时候，修改的数据，就让别的事务给读到了，这个也叫做脏读。

（2）读已提交，Read Committed（不可重复读）：就是说事务A在跑的时候， 先查询了一个数据是值1，然后过了段时间，事务B把那个数据给修改了一下还提交了，此时事务A再次查询这个数据就成了值2了，这是读了人家事务提交的数据啊，所以是读已提交。

这个也叫做不可重复读，就是所谓的一个事务内对一个数据两次读，可能会读到不一样的值。

（3）可重复读，Read Repeatable：这个比上面那个再好点儿，就是说事务A在执行过程中，对某个数据的值，无论读多少次都是值1；哪怕这个过程中事务B修改了数据的值还提交了，但是事务A读到的还是自己事务开始时这个数据的值。

（4）串行化：如果要解决幻读，就需要使用串行化级别的隔离级别，所有事务都串行起来，不允许多个事务并行操作。

MySQL的默认隔离级别是Read Repeatable，就是可重复读，就是说每个事务都会开启一个自己要操作的某个数据的快照，事务期间，读到的都是这个数据的快照罢了，对一个数据的多次读都是一样的。

| 隔离级别                     | 脏读 | 不可重复读 | 幻读 |
| ---------------------------- | ---- | ---------- | ---- |
| Read Uncommitted（读未提交） | 是   | 是         | 是   |
| Read Committed（读已提交）   | 否   | 是         | 是   |
| Read Repeatable（可重复读）  | 否   | 否         | 是   |
| Serializable（串行化）       | 否   | 否         | 否   |

### 3.MySQL 事务介绍

MySQL和其它的数据库产品有一个很大的不同就是事务由存储引擎所决定，例如 MYISAM,MEMORY,ARCHIVE都不支持事务，事务就是为了解决一组查询要么全部执行成功，要么全部执行失败。

MySQL事务默认是采取自动提交的模式，除非显示开始一个事务。

```html
SHOW VARIABLES LIKE 'AUTOCOMMIT';
```

修改自动提交模式， 0=OFF,1=ON。

注意：修改自动提交对非事务类型的表是无效的，因为它们本身就没有提交和回滚的概念，还有一些命令是会强制自动提交的，比如 DLL 命令、 lock tables 等。

```html
SET AUTOCOMMIT=OFF 或 SET AUTOCOMMIT=0
```

### 4.MySQL是如何实现Read Repeatable的

当我们使用innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；

同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。

### 5.MySQL是如何支持4种事务隔离级别的？Spring事务注解是如何设置的？

MySQL默认设置的事务隔离级别，都是RR级别的，而且MySQL的RR级别是可以避免幻读发生的。

这点是MySQL的RR级别的语义跟SQL标准的RR级别不同的，毕竟SQL标准里规定RR级别是可以发生幻读的，但是MySQL的RR级别避免了！

假设你要修改MySQL的默认事务隔离级别，是下面的命令，可以设置级别为不同的level，level的值可以是REPEATABLE READ，READ COMMITTED，READ UNCOMMITTED，SERIALIZABLE几种级别。

SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level;

@Transactiona

用Spring里的@Transactional注解来做事务，想设置事务隔离级别。具体的设置方式如下：

@Transactional(isolation=Isolation.DEFAULT)，然后默认的就是DEFAULT值，这个就是MySQL默认支持什么隔离级别就是什么隔离级别。

也可以改成Isolation.READ_COMMITTED，Isolation.REPEATABLE_READ，Isolation.SERIALIZABLE几个级别，都是可以的。

### 6.MVCC机制

MVCC多版本并发控制机制，能让RR级别避免不可重复读和幻读的问题。

undo log版本链是个什么东西？

我们每条数据其实都有两个隐藏字段，一个是trx_id，一个是roll_pointer，这个trx_id就是最近一次更新这条数据的事务id，roll_pointer就是指向你了你更新这个事务之前生成的undo log。

比如有一个事务A（id=50），插入了一条数据，这条数据的txr_id就是50，roll_pointer指向一个空的undo log，因为之前这条数据是没有的。

接着假设有一个事务B跑来修改了一下这条数据，把值改成了值B，事务B的id是58，txr_id就是事务B的id，也就是58，roll_pointer指向了undo log，这个undo log就记录你更新之前的那条数据的值。

假设事务C又来修改了一下这个值为值C，他的事务id是69，此时会把数据行里的txr_id改成69，然后生成一条undo log，记录之前事务B修改的那个值。

每个人修改了一行数据，都会更新隐藏字段txr_id和roll_pointer，同时之前多个数据快照对应的undo log，会通过roll_pinter指针串联起来，形成一个重要的版本链！

基于undo log多版本链条实现的ReadView机制，到底是什么？

ReadView机制，你执行一个事务的时候，就给你生成一个ReadView，里面比较关键的东西有4个

- 一个是m_ids，这个就是说此时有哪些事务在MySQL里执行还没提交的；
- 一个是min_trx_id，就是m_ids里最小的值；
- 一个是max_trx_id，这是说mysql下一个要生成的事务id，就是最大事务id；
- 一个是creator_trx_id，就是你这个事务的id。

假设原来数据库里就有一行数据，很早以前就有事务插入过了，事务id是32，他的值就是初始值。

此时两个事务并发过来执行了，一个是事务A（id=45），一个是事务B（id=59），事务B是要去更新这行数据的，事务A是要去读取这行数据的值的。

现在事务A直接开启一个ReadView，这个ReadView里的m_ids就包含了事务A和事务B的两个id，45和59，然后min_trx_id就是45，max_trx_id就是60，creator_trx_id就是45，是事务A自己。

这个时候事务A第一次查询这行数据，会走一个判断，就是判断一下当前这行数据的txr_id是否小于ReadView中的min_trx_id，此时发现txr_id=32，是小于ReadView里的min_trx_id就是45的，说明你事务开启之前，修改这行数据的事务早就提交了，所以此时可以查到这行数据

接着事务B开始动手了，他把这行数据的值修改为了值B，然后这行数据的txr_id设置为自己的id，也就是59，同时roll_pointer指向了修改之前生成的一个undo log，接着这个事务B就提交了。

这个时候事务A再次查询，会发现此时数据行里的txr_id=59，这个txr_id是大于ReadView里的min_txr_id(45)，同时小于ReadView里的max_trx_id（60）的，说明更新这条数据的事务，很可能就跟自己差不多同时开启的，于是会看一下这个txr_id=59，是否在ReadView的m_ids列表里？

果然，在ReadView的m_ids列表里，有45和59两个事务id，直接证实了，这个修改数据的事务是跟自己同一时段并发执行然后提交的，所以对这行数据是不能查询的！

此时顺着这条数据的roll_pointer顺着undo log日志链条往下找，就会找到最近的一条undo log，trx_id是32，此时发现trx_id=32，是小于ReadView里的min_trx_id（45）的，说明这个undo log版本必然是在事务A开启之前就执行且提交的。

那么就查询最近的那个undo log里的值好了，这就是undo log多版本链条的作用，他可以保存一个快照链条，让你可以读到之前的快照值。

Read Committed隔离级别是如何基于ReadView机制实现的？

ReadView机制，是基于undo log版本链条实现的一套读视图机制，意思就是说你事务生成一个ReadView，然后呢，如果是你事务自己更新的数据，自己是可以读到的，或者是在你生成ReadView之前提交的事务修改的值，也是可以读取到的。

但是如果是你生成ReadView的时候，就已经活跃的事务，在你生成ReadView之后修改了数据，接着提交了，此时你是读不到的，或者是你生成ReadView以后再开启的事务修改了数据，还提交了，此时也是读不到的。

如何基于ReadView机制来实现RC隔离级别呢？

当你一个事务设置他处于RC隔离级别的时候，他是每次发起查询，都重新生成一个ReadView！

事务A（id=60），事务B（id=70），事务B发起了一次update操作，此时数据的trx_id会变为事务B的id=70，同时会生成一条undo log，由roll_pointer来指向。此时事务A要发起一次查询操作，会生成一个ReadView，此时ReadView里的min_trx_id=60，max_trx_id=71，creator_trx_id=60。发现当前这条数据的trx_id是70，属于ReadView的事务id范围之间，但是此时这个事务B还没提交，所以ReadView的m_ids活跃事务列表里，是有[60, 70]两个id的，所以此时根据ReadView的机制，此时事务A是无法查到事务B修改的值B的。

事务B提交事务，事务A下次发起查询，再次生成一个ReadView。此时数据库内活跃的事务只有事务A了，因此min_trx_id是60，mac_trx_id是71，但是m_ids这个活跃事务列表里，只会有一个60了，事务B的id=70不会出现在m_ids活跃事务列表里了。虽然在ReadView的min_trx_id和max_trx_id范围之间，但是此时并不在m_ids列表内，会查到值B。

RR隔离级别，是如何基于ReadView机制实现的？

你事务A多次读同一个数据，每次读到的都是一样的值，除非是他自己修改了值，否则读到的一直会一样的值。

不管别的事务如何修改数据，事务A的ReadView始终是不变的，他基于这个ReadView始终看到的值是一样的！

事务A根本不会发生幻读，他根据条件范围查询的时候，每次读到的数据都是一样的，不会读到人家插入进去的数据，这都是依托ReadView机制实现的！





## 锁

### 1.数据库的并发策略

数据库的并发控制一般采用三种方法实现，分别是乐观锁、悲观锁及时间戳。

乐观锁

乐观锁在读数据时，认为别人不会去写其所读的数据；悲观锁就刚好相反，觉得自己读数据时，别人可能刚好在写自己刚读的数据，态度比较保守；时间戳在操作数据时不加锁，而是通过时间戳来控制并发出现的问题。

悲观锁

悲观锁指在其修改某条数据时，不允许别人读取该数据，直到自己的整个事务都提交并释放锁，其他用户才能访问该数据。悲观锁又可分为排它锁（写锁）和共享锁（读锁）。

时间戳

时间戳指在数据库表中额外加一个时间戳列TimeStamp。每次读数据时，都把时间戳也读出来，在更新数据时把时间戳加1，在提交之前跟数据库的该字段比较一次，如果比数据库的值大，就允许保存，否则不允许保存。这种处理方法虽然不使用数据库系统提供的锁机制，但是可以大大提高数据库处理的并发量。

### 2.数据库锁

一般其实就是表锁、行锁和页锁。

行级锁

行级锁指对某行数据加锁，是一种排他锁，防止其他事务修改此行。在执行以下数据库操作时，数据库会自动应用行级锁。

- INSERT、UPDATE、DELETE、SELECT … FOR UPDATE [OFcolumns](#)。
- SELECT … FOR UPDATE语句允许用户一次针对多条记录执行更新。
- 使用COMMIT或ROLLBACK语句释放锁。

innodb的行锁有共享锁（S）和排他锁（X），两种，其实说白了呢，共享锁就是，多个事务都可以加共享锁读同一行数据，但是别的事务不能写这行数据；排他锁，就是就一个事务可以写这行数据，别的事务只能读，不能写。

手动加共享锁：select * from table where id=1 lock in share mode，那你就给那一行加了个共享锁，其他事务就不能来修改这行数据了 。

手动加排他锁：select * from table where id=1 for update，那你就给那一行加了个排他锁，意思就是你准备修改，别的事务就别修改了，别的事务的修改会hang住。这个要慎用，一般我们线上系统不用这个，容易搞出问题来。

表级锁

表级锁指对当前操作的整张表加锁，它的实现简单，资源消耗较少，被大部分存储引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。

一般myisam会加表锁，就是myisam引擎下，执行查询的时候，会默认加个表共享锁，也就是表读锁，这个时候别人只能来查，不能写数据的；然后myisam写的时候，也会加个表独占锁，也就是表写锁，别人不能读也不能写。

innodb的表锁，分成意向共享锁，就是说加共享行锁的时候，必须先加这个共享表锁；还有一个意向排他锁，就是说，给某行加排他锁的时候，必须先给表加排他锁。这个表锁，是innodb引擎自动加的，不用你自己去加。

insert、update、delete，innodb会自动给那一行加行级排他锁 。

手动加共享锁：select * from table where id=1 lock in share mode，那你就给那一行加了个共享锁，其他事务就不能来修改这行数据了 。

手动加排他锁：select * from table where id=1 for update，那你就给那一行加了个排他锁，意思就是你准备修改，别的事务就别修改了，别的事务的修改会hang住。这个要慎用，一般我们线上系统不用这个，容易搞出问题来。

在MySQL 中，主要通过四个队列来维护这两种锁定：两个存放当前正在锁定中的读和写锁定信息，另外两个存放等待中的读写锁定信息，如下：

- Current read-lock queue (lock->read)
- Pending read-lock queue (lock->read_wait)
- Current write-lock queue (lock->write)
- Pending write-lock queue (lock->write_wait)

页级锁

页级锁的锁定粒度介于行级锁和表级锁之间。表级锁的加锁速度快，但冲突多，行级冲突少，但加锁速度慢。页级锁在二者之间做了平衡，一次锁定相邻的一组记录。

总结

在MySQL 数据库中，使用表级锁定的主要是MyISAM，Memory，CSV 等一些非事务性存储引擎，而使用行级锁定的主要是Innodb 存储引擎和NDB Cluster 存储引擎，页级锁定主要是BerkeleyDB 存储引擎的锁定方式。

### 3.悲观锁和乐观锁是啥？

mysql里的悲观锁是走select * from table where id=1 for update，就这个，意思是我很悲观，我担心自己拿不到这把锁，我必须先锁死，然后就我一个人可以干这事儿，别人都干不了了，不能加共享锁，也不能加排他锁。

乐观锁，就是说我觉得应该没啥问题，我修改的时候感觉差不多可以获取到锁，不需要提前搞一把锁，我就先查出来某个数据，select id,name,version from table where id=1，接着再执行各种业务逻辑之后再修改，update table set name=’新值’,version=version+1 where id=1 and version=1，就是说每次修改，比较一下这条数据的当前版本号跟我之前查出来的版本号是不是一样的，如果是一样的就修改然后把版本号加1，否则就不会更新任何一行数据，此时就重新查询后再次更新。

一般悲观锁什么时候用呢？比如你查出来了一条数据，要在内存中修改后再更新到数据库中去，但是如果这个过程中数据被别人更新了，你是不能直接干这个操作的，这个时候，你就得走上面那个操作，查询之后就不让别人更新了，你搞完了再说。

但是真有这种场景，推荐你还是用乐观锁把，悲观锁实现简单一点，但是太有风险了，很容易很容易死锁，比如事务A拿了数据1的锁，事务B拿了数据2的锁，然后事务A又要获取数据2的锁就会等待，事务B又要获取数据1的锁，也会等待。

### 4.读锁定与写锁定

读锁定

一个新的客户端请求在申请获取读锁定资源的时候，需要满足两个条件： 1、请求锁定的资源当前没有被写锁定； 2、写锁定等待队列（Pending write-lock queue）中没有更高优先级的写锁定等待；

如果满足了上面两个条件之后，该请求会被立即通过，并将相关的信息存入Current read-lock queue 中，而如果上面两个条件中任何一个没有满足，都会被迫进入等待队列Pending read-lock queue中等待资源的释放。

写锁定

当客户端请求写锁定的时候，MySQL 首先检查在Current write-lock queue 是否已经有锁定相同资源的信息存在。

如果Current write-lock queue 没有，则再检查Pending write-lock queue，如果在Pendingwrite-lock queue 中找到了，自己也需要进入等待队列并暂停自身线程等待锁定资源。反之，如果Pending write-lock queue 为空，则再检测Current read-lock queue，如果有锁定存在，则同样需要进入Pending write-lock queue 等待。当然，也可能遇到以下这两种特殊情况：

1. 请求锁定的类型为WRITE_DELAYED;
2. 请求锁定的类型为WRITE_CONCURRENT_INSERT 或者是TL_WRITE_ALLOW_WRITE， 同时Current read lock 是READ_NO_INSERT 的锁定类型。

当遇到这两种特殊情况的时候，写锁定会立即获得而进入Current write-lock queue 中，如果刚开始第一次检测就Current write-lock queue 中已经存在了锁定相同资源的写锁定存在，那么就只能进入等待队列等待相应资源锁定的释放了。

读请求和写等待队列中的写锁请求的优先级规则主要为以下规则决定：

1. 除了READ_HIGH_PRIORITY 的读锁定之外，Pending write-lock queue 中的WRITE 写锁定能够阻塞所有其他的读锁定；
2. READ_HIGH_PRIORITY 读锁定的请求能够阻塞所有Pending write-lock queue 中的写锁定；
3. 除了WRITE 写锁定之外，Pending write-lock queue 中的其他任何写锁定都比读锁定的优先级低。

写锁定出现在Current write-lock queue 之后，会阻塞除了以下情况下的所有其他锁定的请求：

1. 在某些存储引擎的允许下，可以允许一个WRITE_CONCURRENT_INSERT 写锁定请求
2. 写锁定为WRITE_ALLOW_WRITE 的时候，允许除了WRITE_ONLY 之外的所有读和写锁定请求
3. 写锁定为WRITE_ALLOW_READ 的时候，允许除了READ_NO_INSERT 之外的所有读锁定请求
4. 写锁定为WRITE_DELAYED 的时候，允许除了READ_NO_INSERT 之外的所有读锁定请求
5. 写锁定为WRITE_CONCURRENT_INSERT 的时候，允许除了READ_NO_INSERT 之外的所有读锁定请求

### 5.Innodb 锁定模式

Innodb 的行级锁定同样分为两种类型，共享锁和排他锁，而在锁定机制的实现过程中为了让行级锁定和表级锁定共存， Innodb 也同样使用了意向锁（表级锁定）的概念，也就有了意向共享锁和意向排他锁这两种。

当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。所以，可以说Innodb 的锁定模式实际上可以分为四种：共享锁（S），排他锁（X），意向共享锁（IS）和意向排他锁（IX）。

### 6.死锁条件

（1） 互斥条件：一个资源每次只能被一个进程使用。 （2） 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 （3） 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 （4） 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。

### 7.MySQL锁机制

多个事务更新同一行数据时，是如何加锁避免脏写的？

有一个事务来了要更新一行数据，发现没有被锁，这个事务就会创建一个锁，里面包含了自己的trx_id和等待状态，然后把锁跟这行数据关联在一起。

事务B也想更新那行数据，发现被锁了，也会生成一个锁数据结构，里面有他的trx_id，还有自己的等待状态，但是他因为是在排队等待，所以他的等待状态就是true。

接着事务A这个时候更新完了数据，就会把自己的锁给释放掉了。锁一旦释放了，他就会去找，此时还有没有别人也对这行数据加锁了呢？他会发现事务B也加锁了。于是这个时候，就会把事务B的锁里的等待状态修改为false，然后唤醒事务B继续执行，此时事务B就获取到锁了

共享锁和独占锁到底是什么？

在多个事务运行的时候，加的是X锁，也就是Exclude独占锁，当有一个事务加了独占锁之后，此时其他事务再要更新这行数据，都是要加独占锁的，但是只能生成独占锁在后面等待。

默认情况下，有人在更新数据的时候，然后你要去读取这行数据，直接默认就是开启mvcc机制的。

也就是说，此时对一行数据的读和写两个操作默认是不会加锁互斥的，因为MySQL设计mvcc机制就是为了解决这个问题，避免频繁加锁互斥。

万一要是你在执行查询操作的时候，就是想要加锁呢？

MySQL首先支持一种共享锁，就是S锁，这个共享锁的语法如下：select * from table lock in share mode，你在一个查询语句后面加上lock in share mode，意思就是查询的时候对一行数据加共享锁。

如果此时有别的事务在更新这行数据，已经加了独占锁了，此时你的共享锁能加吗？

当然不行了，共享锁和独占锁是互斥的！此时你这个查询就只能等着了。

那么如果你先加了共享锁，然后别人来更新要加独占锁行吗？当然不行了，此时锁是互斥的，他只能等待。

那么如果你在加共享锁的时候，别人也加共享锁呢？此时是可以的，你们俩都是可以加共享锁的，共享锁和共享锁是不会互斥的。

| 锁类型 | 独占锁 | 共享锁 |
| ------ | ------ | ------ |
| 独占锁 | 互斥   | 互斥   |
| 共享锁 | 互斥   | 不互斥 |

查询操作还能加互斥锁，他的方法是：select * from table for update。

这个意思就是，我查出来数据以后还要更新，此时我加独占锁了，其他人都不要更新这个数据了。

一旦你查询的时候加了独占锁，此时在你事务提交之前，任何人都不能更新数据了，只能你在本事务里更新数据，等你提交了，别人再更新数据。

### 8.在数据库里，哪些操作会导致在表级别加锁呢？

在多个事务并发更新数据的时候，都是要在行级别加独占锁的，这就是行锁，独占锁都是互斥的，所以不可能发生脏写问题，一个事务提交了才会释放自己的独占锁，唤醒下一个事务执行。

如果你此时去读取别的事务在更新的数据，有两种可能：

- 第一种可能是基于mvcc机制进行事务隔离，读取快照版本，这是比较常见的；
- 第二种可能是查询的同时基于特殊语法去加独占锁或者共享锁。

如果你查询的时候加独占锁，那么跟其他更新数据的事务加的独占锁都是互斥的；如果你查询的时候加共享锁，那么跟其他查询加的共享锁是不互斥的，但是跟其他事务更新数据就加的独占锁是互斥的，跟其他查询加的独占锁也是互斥的。

在数据库里，你不光可以通过查询中的特殊语法加行锁，比如lock in share mode、for update等等，还可以通过一些方式在表级别去加锁。

### 9.表锁和行锁互相之间的关系以及互斥规则是什么呢？

表锁分为两种，一种就是表锁，一种是表级的意向锁。

表锁，可以用如下语法来加：

LOCK TABLES xxx READ：这是加表级共享锁

LOCK TABLES xxx WRITE：这是加表级独占锁

另外两个情况会加表级锁。如果有事务在表里执行增删改操作，那在行级会加独占锁，此时其实同时会在表级加一个意向独占锁；如果有事务在表里执行查询操作，那么会在表级加一个意向共享锁。

| 锁类型     | 独占锁 | 意向独占锁 | 共享锁 | 意向共享锁 |
| ---------- | ------ | ---------- | ------ | ---------- |
| 独占锁     | 互斥   | 互斥       | 互斥   | 互斥       |
| 意向独占锁 | 互斥   | 不互斥     | 互斥   | 不互斥     |
| 共享锁     | 互斥   | 互斥       | 不互斥 | 不互斥     |
| 意向共享锁 | 互斥   | 不互斥     | 不互斥 | 不互斥     |

更新数据自动加的表级意向独占锁，会跟你用 LOCK TABLES xxx WRITE 手动加的表级独占锁是互斥的，所以说，假设你手动加了表级独占锁，此时任何人都不能执行更新操作了！

或者你用LOCK TABLES xxx READ手动加了表级共享锁，此时任何人也不能执行更新操作了，因为更新就要加意向独占锁，此时是跟你手动加的表级共享锁，是互斥的！

### 10.InnoDB 常见的几种锁机制

1. 共享锁和独占锁（Shared and Exclusive Locks），InnoDB 通过共享锁和独占所两种方式实现了标准的行锁。共享锁（S 锁）： 允许事务获得锁后去读数据，独占锁（X 锁）：允许事务获得锁后去更新或删除数据。一个事务获取的共享锁 S 后，允许其他事 务获取 S 锁，此时两个事务都持有共享锁 S，但是不允许其他事务获取 X 锁。如果一个事务获取的独占锁（X），则不允许其他 事务获取 S 或者 X 锁，必须等到该事务释放锁后才可以获取到。
2. 意向锁（Intention Locks），上面说过 InnoDB 支持行锁和表锁，意向锁是一种表级锁，用来指示接下来的一个事务将要获取的 是什么类型的锁（共享还是独占）。意向锁分为意向共享锁（IS）和意向独占锁（IX），依次表示接下来一个事务将会获得共享 锁或者独占锁。意向锁不需要显示的获取，在我们获取共享锁或者独占锁的时候会自动的获取，意思也就是说，如果要获取共享 锁或者独占锁，则一定是先获取到了意向共享锁或者意向独占锁。 意向锁不会锁住任何东西，除非有进行全表请求的操作，否则 不会锁住任何数据。存在的意义只是用来表示有事务正在锁某一行的数据，或者将要锁某一行的数据。
3. 记录锁（record Locks），锁住某一行，如果表存在索引，那么记录锁是锁在索引上的，如果表没有索引，那么 InnoDB 会创建一 个隐藏的聚簇索引加锁。所以我们在进行查询的时候尽量采用索引进行查询，这样可以降低锁的冲突。
4. 间隙锁（Gap Locks），间隙锁是一种记录行与记录行之间存在空隙或在第一行记录之前或最后一行记录之后产生的锁。间隙锁 可能占据的单行，多行或者是空记录。通常的情况是我们采用范围查找的时候，比如在学生成绩管理系统中，如果此时有学生成 绩 60，72，80，95，一个老师要查下成绩大于 72 的所有同学的信息，采用的语句是 select * from student where grade > 72 for update ，这个时候 InnoDB 锁住的不仅是 80，95，而是所有在 72-80，80- 95，以及 95 以上的所有记录。为什么会 这样呢？实际上是因为如果不锁住这些行，那么如果另一个事务在此时插入了一条分数 大于 72 的记录，那会导致第一次的事务两次查询的结果不一样，出现了幻读。所以为了在满足事务隔离级别的情况下需要锁住 所有满足条件的行。
5. Next-Key Locks，NK 是一种记录锁和间隙锁的组合锁。是 2 和 3 的组合形式，既锁住行也锁住间隙。并且采用的左开右闭的原 则。InnoDB 对于查询都是采用这种锁的。

### 11.innodb下的记录锁，间隙锁，next-key锁

innodb下的记录锁（也叫行锁），间隙锁，next-key锁统统属于排他锁。

**行锁**

记录锁其实很好理解，对表中的记录加锁，叫做记录锁，简称行锁。

**生活中的间隙锁**

编程的思想源于生活，生活中的例子能帮助我们更好的理解一些编程中的思想。

生活中排队的场景，小明，小红，小花三个人依次站成一排，此时，如何让新来的小刚不能站在小红旁边，这时候只要将小红和她前面的小明之间的空隙封锁，将小红和她后面的小花之间的空隙封锁，那么小刚就不能站到小红的旁边。

这里的小红，小明，小花，小刚就是数据库的一条条记录。

他们之间的空隙也就是间隙，而封锁他们之间距离的锁，叫做间隙锁。

**间隙是怎么划分的？**

**注**：为了方面理解，我们规定（id=A,number=B）代表一条字段id=A,字段number=B的记录，（C，D）代表一个区间，代表C-D这个区间范围。

只要这些区间对应的两个临界记录中间可以插入记录，就认为区间对应的记录之间有间隙。

**间隙锁锁定的区域**

根据检索条件向左寻找最靠近检索条件的记录值A，作为左区间，向右寻找最靠近检索条件的记录值B作为右区间，即锁定的间隙为（A，B）。

**间隙锁的目的是为了防止幻读，其主要通过两个方面实现这个目的：**

- 防止间隙内有新数据被插入
- 防止已存在的数据，更新成间隙内的数据（例如防止numer=3的记录通过update变成number=5）

**innodb自动使用间隙锁的条件：**

- 必须在RR级别下
- 检索条件必须有索引（没有索引的话，mysql会全表扫描，那样会锁定整张表所有的记录，包括不存在的记录，此时其他事务不能修改不能删除不能添加）

**next-key锁**

next-key锁其实包含了记录锁和间隙锁，即锁定一个范围，并且锁定记录本身，InnoDB默认加锁方式是next-key 锁。

### 12.MySQL/InnoDB的加锁过程

多版本并发控制**MVCC：Snapshot Read vs Current Read**

MySQL InnoDB存储引擎，实现的是基于多版本的并发控制协议——MVCC (Multi-Version Concurrency Control) (注：与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control)。

MVCC最大的好处，相信也是耳熟能详：**读不加锁，读写不冲突**。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能，这也是为什么现阶段，几乎所有的RDBMS，都支持了MVCC。

在MVCC并发控制中，读操作可以分成两类：**快照读 (snapshot read)与当前读 (current read)** 。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。

在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以MySQL InnoDB为例：

**快照读：** 简单的select操作，属于快照读，不加锁。

**当前读：** 特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。

### 13.MySQL中InnoDB引擎的行锁是通过加在什么上完成（或称实现）的？为什么是这样子的？

InnoDB存储引擎的锁是通过加在索引上面完成的，如果表没有创建索引，InnoDB会自动创建一个6字节的自增索引。

因为Innodb是索引组织表，通过索引去找对应的数据行

###  1.锁机制与InnoDB锁算法

**MyISAM和InnoDB存储引擎使用的锁：**

- MyISAM采用表级锁(table-level locking)。
- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁

**表级锁和行级锁对比：**

- **表级锁：** MySQL中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。
- **行级锁：** MySQL中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

**InnoDB存储引擎的锁的算法有三种：**

- Record lock：单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record+gap 锁定一个范围，包含记录本身

**相关知识点：**

1. innodb对于行的查询使用next-key lock
2. Next-locking keying为了解决Phantom Problem幻读问题
3. 当查询的索引含有唯一属性时，将next-key lock降级为record key
4. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
5. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

### 2.共享锁（s）和排他锁（X）

表级锁和行级锁可以进一步划分为共享锁（s）和排他锁（X）。

共享锁（s）

共享锁（Share Locks，简记为S）又被称为读锁，其他用户可以并发读取数据，但任何事务都不能获取数据上的排他锁，直到已释放所有共享锁。若事务T对数据对象A加上S锁，则事务T只能读A；其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这就保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。

排他锁（X）

排它锁（(Exclusive lock,简记为X锁)）又称为写锁，若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁。它防止任何其它事务获取资源上的锁，直到在事务的末尾将资源上的原始锁释放为止。在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁。

两者之间的区别

共享锁（S锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不 能加排他锁。获取共享锁的事务只能读数据，不能修改数据。

排他锁（X锁）：如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获取排他锁的事务既能读数据，又能修改数据。 另外两个表级锁：IS和IX

当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。

### 3.表锁

首先，从锁的粒度，我们可以分成两大类：

- 表锁
  - 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低
- 行锁
  - 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高

不同的存储引擎支持的锁粒度是不一样的：

- **InnoDB行锁和表锁都支持**！
- **MyISAM只支持表锁**！

InnoDB只有通过**索引条件**检索数据**才使用行级锁**，否则，InnoDB将使用**表锁**

- 也就是说，**InnoDB的行锁是基于索引的**！

**表锁下又分为两种模式**：

- 表读锁（Table Read Lock）
- 表写锁（Table Write Lock）
- 从下图可以清晰看到，在表读锁和表写锁的环境下：读读不阻塞，读写阻塞，写写阻塞！
  - 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁
  - 读写阻塞：当前用户在读数据，其他的用户**不能修改当前用户读的数据**，会加锁！
  - 写写阻塞：当前用户在修改数据，其他的用户**不能修改当前用户正在修改的数据**，会加锁！

**MyISAM可以**支持查询和插入操作的**并发**进行。可以通过系统变量`concurrent_insert`来指定哪种模式，在**MyISAM**中它默认是：如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从**表尾**插入记录。

但是**InnoDB存储引擎是不支持的**！

### 4.行锁

我们使用Mysql一般是使用InnoDB存储引擎的。InnoDB和MyISAM有两个本质的区别：

- InnoDB支持行锁
- InnoDB支持事务

从上面也说了：我们是**很少手动加表锁**的。表锁对我们程序员来说几乎是透明的，即使InnoDB不走索引，加的表锁也是自动的！

我们应该**更加关注行锁的内容**，因为InnoDB一大特性就是支持行锁！

InnoDB实现了以下**两种**类型的行锁。

- 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。
  - 也叫做**读锁**：读锁是**共享**的，多个客户可以**同时读取同一个**资源，但**不允许其他客户修改**。
- 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。
  - 也叫做**写锁**：写锁是排他的，**写锁会阻塞其他的写锁和读锁**。

另外，**为了允许行锁和表锁共存，实现多粒度锁机制**，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是**表锁**：

- 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。
- 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。
- 意向锁也是数据库隐式帮我们做了，**不需要程序员操心**！

### 5.数据库的并发策略

数据库的并发控制一般采用三种方法实现，分别是乐观锁、悲观锁及时间戳。

乐观锁

乐观锁在读数据时，认为别人不会去写其所读的数据；悲观锁就刚好相反，觉得自己读数据时，别人可能刚好在写自己刚读的数据，态度比较保守；时间戳在操作数据时不加锁，而是通过时间戳来控制并发出现的问题。

悲观锁

悲观锁指在其修改某条数据时，不允许别人读取该数据，直到自己的整个事务都提交并释放锁，其他用户才能访问该数据。悲观锁又可分为排它锁（写锁）和共享锁（读锁）。

时间戳

时间戳指在数据库表中额外加一个时间戳列TimeStamp。每次读数据时，都把时间戳也读出来，在更新数据时把时间戳加 1，在提交之前跟数据库的该字段比较一次，如果比数据库的值大，就允许保存，否则不允许保存。这种处理方法虽然不使用数据库系统提供的锁机制，但是可以大大提高数据库处理的并发量。

### 6.数据库锁

1.行级锁

行级锁指对某行数据加锁，是一种排他锁，防止其他事务修改此行。在执行以下数据库操作时，数据库会自动应用行级锁。

- INSERT 、UPDATE 、DELETE 、SELECT … FOR UPDATE [OF columns] [WAIT n|NOWAIT]。
- SELECT … FOR UPDATE语句允许用户一次针对多条记录执行更新。
- 使用COMMIT或ROLLBACK语句释放锁。

2.表级锁

表级锁指对当前操作的整张表加锁，它的实现简单，资源消耗较少，被大部分存储引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。

3.页级锁

页级锁的锁定粒度介于行级锁和表级锁之间。表级锁的加锁速度快，但冲突多，行级冲突少，但加锁速度慢。页级锁在二者之间做了平衡，一次锁定相邻的一组记录。

4.基于Redis的分布式锁

数据库锁是基于单个数据库实现的，在我们的业务跨多个数据库时，就要使用分布式锁来保证数据的一致性。下面介绍使用Redis实现一个分布式锁的流程。Redis实现的分布式锁以Redis setnx命令为中心实现， setnx 是Redis 的写入操作命令， 具体语法为setnx(key val)。在且仅在key不存在时，则插入一个key为val的字符串，返回1；若key存在，则什么都不做，返回0。通过setnx实现分布式锁的思路如下。

- 获取锁：在获取锁时调用setnx，如果返回 0，则该锁正在被别人使用；如果返回1，则成功获取锁。
- 释放锁：在释放锁时，判断锁是否存在，如果存在，则执行Redis的delete操作释放锁。

注意，如果锁并发比较大，则可以设置一个锁的超时时间，在超时时间到后，Redis会自动释放锁给其他线程使用。

## 高可用

### 1.MySQL 主从复制原理的是啥？

主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。

有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。

而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。

这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。

所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。

### 2.MySQL 主从同步延时问题

我们通过 MySQL 命令：

show status

查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。

一般来说，如果主从延迟较为严重，有以下解决方案：

- 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
- 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
- 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
- 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。

## 优化

### 1.Join 语句的优化

1. 尽可能减少Join 语句中的Nested Loop 的循环总次数；
2. 优先优化Nested Loop 的内层循环；
3. 保证Join 语句中被驱动表上Join 条件字段已经被索引；
4. 当无法保证被驱动表的Join 条件字段被索引且内存资源充足的前提下，不要太吝惜JoinBuffer 的设置；

### 2.不适合在数据库中存放

1. 二进制多媒体数据
2. 流水队列数据
3. 超大文本数据

### 3.锁机制优化

1、缩短锁定时间

如何让锁定时间尽可能的短呢？唯一的办法就是让我们的Query 执行时间尽可能的短。 a) 尽两减少大的复杂Query，将复杂Query 分拆成几个小的Query 分布进行； b) 尽可能的建立足够高效的索引，让数据检索更迅速； c) 尽量让MyISAM 存储引擎的表只存放必要的信息，控制字段类型； d) 利用合适的机会优化MyISAM 表数据文件；

2、分离能并行的操作

MyISAM 的存储引擎还有一个非常有用的特性，那就是Concurrent Insert（并发插入）的特性。MyISAM 存储引擎有一个控制是否打开Concurrent Insert 功能的参数选项：concurrent_insert，可以设置为0，1 或者2。三个值的具体说明如下： a) concurrent_insert=2，无论MyISAM 存储引擎的表数据文件的中间部分是否存在因为删除数据而留下的空闲空间，都允许在数据文件尾部进行Concurrent Insert; b) concurrent_insert=1，当MyISAM 存储引擎表数据文件中间不存在空闲空间的时候，可以从文件尾部进行Concurrent Insert; c) concurrent_insert=0，无论MyISAM 存储引擎的表数据文件的中间部分是否存在因为删除数据而留下的空闲空间，都不允许Concurrent Insert。

3、合理利用读写优先级

默认情况下是写优先级要大于读优先级。。如果我们的系统是一个以读为主，而且要优先保证查询性能的话，我们可以通过设置系统参数选项low_priority_updates=1，将写的优先级设置为比读的优先级低，即可让告诉MySQL 尽量先处理读请求。

### 4.Innodb 行锁优化建议

a) 尽可能让所有的数据检索都通过索引来完成，从而避免Innodb 因为无法通过索引键加锁而升级为表级锁定； b) 合理设计索引，让Innodb 在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他Query 的执行； c) 尽可能减少基于范围的数据检索过滤条件，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录； d) 尽量控制事务的大小，减少锁定的资源量和锁定时间长度； e) 在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少MySQL 因为实现事务隔离级别所带来的附加成本；

### 5.Query 语句的优化思路和原则

1. 优化更需要优化的Query；
2. 定位优化对象的性能瓶颈；
3. 明确的优化目标；
4. 从Explain 入手；
5. 多使用profile
6. 永远用小结果集驱动大的结果集；
7. 尽可能在索引中完成排序；
8. 只取出自己需要的Columns；
9. 仅仅使用最有效的过滤条件；
10. 尽可能避免复杂的Join 和子查询；
11. <> 用 < 、 > 代替，>用>=代替，<用<=代替，这样可以有效的利用索引。

### 6.根据你以往的经验简单叙述一下MYSQL 的优化

1.数据库的设计

尽量把数据库设计的更小的占磁盘空间. 1).尽可能使用更小的整数类型.(mediumint 就比int 更合适). 2).尽可能的定义字段为not null,除非这个字段需要null. 3).如果没有用到变长字段的话比如varchar,那就采用固定大小的纪录格式比如char. 4).表的主索引应该尽可能的短.这样的话每条纪录都有名字标志且更高效. 5).只创建确实需要的索引。索引有利于检索记录，但是不利于快速保存记录。如果总是要在表的组合字段上做搜索，那么就在这些字段上创建索引。索引的第一部分必须是最常使用的字段.如果总是需要用到很多字段，首先就应该多复制这些字段，使索引更好的压缩。 6).所有数据都得在保存到数据库前进行处理。

7).所有字段都得有默认值。 8).在某些情况下,把一个频繁扫描的表分成两个速度会快好多。在对动态格式表扫描以取得相关记录时，它可能使用更小的静态格式表的情况下更是如此。

2.系统的用途

1).尽量使用长连接. 2).explain 复杂的SQL 语句。 3).如果两个关联表要做比较话，做比较的字段必须类型和长度都一致. 4).LIMIT 语句尽量要跟order by 或者 distinct.这样可以避免做一次full tablescan. 5).如果想要清空表的所有记录,建议用truncate table tablename 而不是delete from tablename. 6).能使用STORE PROCEDURE 或者 USER FUNCTION 的时候. 7).在一条insert 语句中采用多重纪录插入格式.而且使用load data infile 来导入大量数据，这比单纯的indert 快好多. 8).经常OPTIMIZE TABLE 来整理碎片. 9).还有就是date 类型的数据如果频繁要做比较的话尽量保存在unsigned int类型比较快。

3.系统的瓶颈

1).磁盘搜索. 并行搜索,把数据分开存放到多个磁盘中，这样能加快搜索时间. 2).磁盘读写(IO)可以从多个媒介中并行的读取数据。 3).CPU 周期 数据存放在主内存中.这样就得增加CPU 的个数来处理这些数据。 4).内存带宽 当CPU 要将更多的数据存放到CPU 的缓存中来的话,内存的带宽就成了瓶颈.

### 7.数据库连接池优化

（1）maxWait

表示从池里获取连接的等待时间，万一你暂时没有可用的连接，就可能要等待别的连接用完释放，你再去使用，通常建议设置在1000以上，就是等待1s以上，比如你可以设置1200，因为有的时候要等待建立新的TCP连接，最多在1s内，那你就得等一会儿。

如果这个参数默认设置为0，意思就是无限的等待获取连接，在高并发场景下，可能瞬间连接池耗尽，大量的请求都卡死在这里等待获取连接，进而导致你tomcat里没有可用的线程，服务就是一个假死的样子。

（2）connectionProperties

里面可以放connectionTimeout和socketTimeout，分别代表建立TCP连接的超时时间，以及发送请求后等待响应的超时时间，推荐connectionTimeout设置为1200，socketTimeout设置为3000。

之所以必须设置他们俩，是因为高并发场景下，万一遇到网络问题，可能会导致你跟数据库的Socket连接异常无法通信，此时你Socket可能一直卡死等待某个请求的响应，然后其他请求无法获取连接，只能是重启系统重新建立连接才行。

所以设置一下超时时间，可以让网络异常之后，连接自动超时断开重连。

（3）maxActive

最大连接池数量，一般建议是设置个20就够了，如果确实有高并发场景，可以适当增加到3~5倍，但是不要太多，其实一般这个数字在几十到100就很大了，因为这仅仅是你一个服务连接数据库的数量，你数据库整体能承受的连接数量是有限的。

而且连接越多不是越好，数据库连接太多了，会导致cpu负载很高，可能反而会导致性能降低的，所以这个参数你一般设置个20，最多加到个几十，其实就差不多了 。

### 8.Explain展示的信息

- ID：Query Optimizer 所选定的执行计划中查询的序列号；
- Select_type：所使用的查询类型，主要有以下这几种查询类型
  - DEPENDENT SUBQUERY：子查询中内层的第一个SELECT，依赖于外部查询的结果集；
  - DEPENDENT UNION：子查询中的UNION，且为UNION 中从第二个SELECT 开始的后面所有SELECT，同样依赖于外部查询的结果集；
  - PRIMARY：子查询中的最外层查询，注意并不是主键查询；
  - SIMPLE：除子查询或者UNION 之外的其他查询；
  - SUBQUERY：子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集；
  - UNCACHEABLE SUBQUERY：结果集无法缓存的子查询；
  - UNION：UNION 语句中第二个SELECT 开始的后面所有SELECT，第一个SELECT 为PRIMARY
  - UNION RESULT：UNION 中的合并结果；
- Table：显示这一步所访问的数据库中的表的名称；
- Type：告诉我们对表所使用的访问方式，主要包含如下集中类型；
  - all：全表扫描
  - const：读常量，且最多只会有一条记录匹配，由于是常量，所以实际上只需要读一次；
  - eq_ref：最多只会有一条匹配结果，一般是通过主键或者唯一键索引来访问；
  - fulltext：
  - index：全索引扫描；
  - index_merge：查询中同时使用两个（或更多）索引，然后对索引结果进行merge 之后再读取表数据；
  - index_subquery：子查询中的返回结果字段组合是一个索引（或索引组合），但不是一个主键或者唯一索引；
  - rang：索引范围扫描；
  - ref：Join 语句中被驱动表索引引用查询；
  - ref_or_null：与ref 的唯一区别就是在使用索引引用查询之外再增加一个空值的查询；
  - system：系统表，表中只有一行数据；
  - unique_subquery：子查询中的返回结果字段组合是主键或者唯一约束；
- Possible_keys：该查询可以利用的索引. 如果没有任何索引可以使用，就会显示成null，这一项内容对于优化时候索引的调整非常重要；
- Key：MySQL Query Optimizer 从possible_keys 中所选择使用的索引；
- Key_len：被选中使用索引的索引键长度；
- Ref：列出是通过常量（const），还是某个表的某个字段（如果是join）来过滤（通过key）的；
- Rows：MySQL Query Optimizer 通过系统收集到的统计信息估算出来的结果集记录条数；
- Extra：查询中每一步实现的额外细节信息，主要可能会是以下内容：
  - Distinct：查找distinct 值，所以当mysql 找到了第一条匹配的结果后，将停止该值的查询而转为后面其他值的查询；
  - Full scan on NULL key：子查询中的一种优化方式，主要在遇到无法通过索引访问null值的使用使用；
  - Impossible WHERE noticed after reading const tables：MySQL Query Optimizer 通过收集到的统计信息判断出不可能存在结果；
  - No tables：Query 语句中使用FROM DUAL 或者不包含任何FROM 子句；
  - Not exists：在某些左连接中MySQL Query Optimizer 所通过改变原有Query 的组成而使用的优化方法，可以部分减少数据访问次数；
  - Range checked for each record (index map: N)：通过MySQL 官方手册的描述，当MySQL Query Optimizer 没有发现好的可以使用的索引的时候，如果发现如果来自前面的表的列值已知，可能部分索引可以使用。对前面的表的每个行组合，MySQL 检查是否可以使用range 或index_merge 访问方法来索取行。
  - Select tables optimized away：当我们使用某些聚合函数来访问存在索引的某个字段的时候，MySQL Query Optimizer 会通过索引而直接一次定位到所需的数据行完成整个查询。当然，前提是在Query 中不能有GROUP BY 操作。如使用MIN()或者MAX（）的时候；
  - Using filesort：当我们的Query 中包含ORDER BY 操作，而且无法利用索引完成排序操作的时候，MySQL Query Optimizer 不得不选择相应的排序算法来实现。
  - Using index：所需要的数据只需要在Index 即可全部获得而不需要再到表中取数据；
  - Using index for group-by：数据访问和Using index 一样，所需数据只需要读取索引即可，而当Query 中使用了GROUP BY 或者DISTINCT 子句的时候，如果分组字段也在索引中，Extra 中的信息就会是Using index for group-by；
  - Using temporary：当MySQL 在某些操作中必须使用临时表的时候，在Extra 信息中就会出现Using temporary 。主要常见于GROUP BY 和ORDER BY 等操作中。
  - Using where：如果我们不是读取表的所有数据，或者不是仅仅通过索引就可以获取所有需要的数据，则会出现Using where 信息；
  - Using where with pushed condition：这是一个仅仅在NDBCluster 存储引擎中才会出现的信息，而且还需要通过打开Condition Pushdown 优化功能才可能会被使用。控制参数为engine_condition_pushdown 。

### 9.SQL调优方法

看执行计划，一般其实就是看SQL有没有走索引。

explain select * from table

table | type | possible_keys | key | key_len | ref | rows | Extra

- table：哪个表
- type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）
- possible_keys：显示可能使用的索引
- key：实际使用的索引
- key_len：使用索引的长度
- ref：联合索引的哪一列被用了
- rows：一共扫描和返回了多少行
- extra：using filesort（需要额外进行排序），using temporary（mysql构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据where来过滤出了结果）

### 10.SQL调优

半连接优化

SELECT COUNT(id) FROM users WHERE id IN (SELECT user_id FROM users_extent_info WHERE latest_login_time < xxxxx)

执行的过程就是先执行了子查询查出来4561条数据，物化成了一个临时表，接着他对users主表做了一个全表扫描，扫描的过程中把每一条数据都放到物化临时表里去做全表扫描，本质在做join的事情。

在执行完上述SQL的EXPLAIN命令，看到执行计划之后，可以执行一下**show warnings**命令。

这个show warnings命令此时显示出来的内容如下：

/* select#1 */ select count(`d2.`users`.`user_id``) AS`COUNT(users.user_id)`from`d2`.`usersusers` semi join xxxxxx

MySQL生成执行计划的时候，自动就把一个普通的IN子句，“优化”成了基于semi join来进行IN+子查询的操作.

对users表里每一条数据，去对物化临时表全表扫描做semi join，不需要把users表里的数据真的跟物化临时表里的数据join上。只要users表里的一条数据，在物化临时表里可以找到匹配的数据，那么users表里的数据就会返回，这就叫做semi join，他是用来筛选的。

执行SET optimizer_switch='semijoin=off'，也就是关闭掉半连接优化，此时执行EXPLAIN命令看一下此时的执行计划，发现此时会恢复为一个正常的状态。

就是有一个SUBQUERY的子查询，基于range方式去扫描索引搜索出4561条数据，接着有一个PRIMARY类型的主查询，直接是基于id这个PRIMARY主键聚簇索引去执行的搜索，然后再把这个SQL语句真实跑一下看看，发现性能一下子提升了几十倍，变成了100多毫秒！

总结

MySQL内部自动使用了半连接优化，结果半连接的时候导致大量无索引的全表扫描，引发了性能的急剧下降

force index

select * from products where category='xx' and sub_category='xx' order by id desc limit xx,xx

KEY index_category(catetory,sub_category)

在主键的聚簇索引上进行扫描，一边扫描，一边还用了where条件里的两个字段去进行筛选，所以这么扫描的话，那必然就是会耗费几十秒了！

为了快速解决这个问题，就需要强制性的改变MySQL自动选择这个不合适的聚簇索引进行扫描的行为

使用force index语法，如下：

select * from products force index(index_category) where category='xx' and sub_category='xx' order by id desc limit xx,xx

大量删除数据

永远不要在业务高峰期去运行那种删除大量数据的语句，因为这可能导致一些正常的SQL都变慢查询，因为那些SQL也许会不断扫描你标记为删除的大量数据，好不容易扫描到一批数据，结果发现是标记为删除的，于是继续扫描下去，导致了慢查询！

### 11.MySQL 问题排查都有哪些手段？

使用 show processlist 命令查看当前所有连接信息。 使用 explain 命令查询 SQL 语句执行计划。 开启慢查询日志，查看慢查询的 SQL。

### 12.单表查询优化

（0）可以先使用 EXPLAIN 关键字可以让你知道MySQL是如何处理你的SQL语句的。这可以帮我们分析是查询语句或是表结构的性能瓶颈。 （1）写sql要明确需要的字段，要多少就写多少字段，而不是滥用 select * （2）可以用使用连接（JOIN）来代替子查询 （3）使用分页语句：limit start , count 或者条件 where子句时，有什么可限制的条件尽量加上，查一条就limit一条。做到不滥用。比如说我之前做过的的p2p项目,只是需要知道有没有一个满标的借款，这样的话就可以用上 limit 1，这样mysql在找到一条数据后就停止搜索，而不是全文搜索完再停止。 （4）开启查询缓存： 大多数的MySQL服务器都开启了查询缓存。这是提高查询有效的方法之一。当有很多相同的查询被执行了多次的时候，这些查询结果会被放到一个缓存中，这样，后续的相同的查询就不用操作表而直接访问缓存结果了。

查询缓存工作流程： A)：服务器接收SQL，以SQL+DB+Query_cache_query_flags作为hash查找键； B)：找到了相关的结果集就将其返回给客户端； C)：如果没有找到缓存则执行权限验证、SQL解析、SQL优化等一些列的操作； D)：执行完SQL之后，将结果集保存到缓存

当然，并不是每种情况都适合使用缓存，衡量打开缓存是否对系统有性能提升是一个整体的概念。那怎么判断要不要开启缓存呢，如下： 1）通过缓存命中率判断, 缓存命中率 = 缓存命中次数 (Qcache_hits) / 查询次数 (Com_select)、 2）通过缓存写入率, 写入率 = 缓存写入次数 (Qcache_inserts) / 查询次数 (Qcache_inserts) 3）通过 命中-写入率 判断, 比率 = 命中次数 (Qcache_hits) / 写入次数 (Qcache_inserts), 高性能MySQL中称之为比较能反映性能提升的指数,一般来说达到3:1则算是查询缓存有效,而最好能够达到10:1

缓存数据失效时机 在表的结构或数据发生改变时，查询缓存中的数据不再有效。有这些INSERT、UPDATE、 DELETE、TRUNCATE、ALTER TABLE、DROP TABLE或DROP DATABASE会导致缓存数据失效。所以查询缓存适合有大量相同查询的应用，不适合有大量数据更新的应用。 可以使用下面三个SQL来清理查询缓存： 1、FLUSH QUERY CACHE; // 清理查询缓存内存碎片。 2、RESET QUERY CACHE; // 从查询缓存中移出所有查询。 3、FLUSH TABLES; //关闭所有打开的表，同时该操作将会清空查询缓存中的内容。 InnoDB与查询缓存： Innodb会对每个表设置一个事务计数器,里面存储当前最大的事务ID.当一个事务提交时,InnoDB会使用MVCC中系统事务ID最大的事务ID跟新当前表的计数器. 只有比这个最大ID大的事务能使用查询缓存,其他比这个ID小的事务则不能使用查询缓存. 另外,在InnoDB中,所有有加锁操作的事务都不使用任何查询缓存

### 13.内存优化

注意：以下都是在MySQL目录下的my.ini文件中改写（技术文）。 一、InnoDB内存优化 InnoDB用一块内存区域做I/O缓存池，该缓存池不仅用来缓存InnoDB的索引块，而且也用来缓存InnoDB的数据块。 1、innodb_log_buffer_size 决定了InnoDB重做日志缓存的大小，可以避免InnoDB在事务提交前就执行不必要的日志写入磁盘操作。 2、设置Innodb_buffer_pool_size 改变量决定了InnoDB存储引擎表数据和索引数据的最大缓存区大小。 二、MyISAM内存优化 MyISAM存储引擎使用key_buffer缓存索引模块，加速索引的读写速度。对于MyISAM表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。 1、read_rnd_buffer_size 对于需要做排序的MyISAM表查询，如带有order by子句的sql，适当增加read_rnd_buffer_size的值，可以改善此类的sql性能。但需要注意的是read_rnd_buffer_size独占的，如果默认设置值太大，就会造成内存浪费。 2、key_buffer_size设置 key_buffer_size决定MyISAM索引块缓存分区的大小。直接影响到MyISAM表的存取效率。对于一般MyISAM数据库，建议1/4可用内存分配给key_buffer_size: key_buffer_size=2G 3、read_buffer_size 如果需要经常顺序扫描MyISAM表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个seesion独占的，如果默认值设置太大，就会造成内存浪费。 三、调整MySQL参数并发相关的参数 1、调整max_connections 提高并发连接 2、调整thread_cache_size 加快连接数据库的速度，MySQL会缓存一定数量的客户服务线程以备重用，通过参数thread_cache_size可控制mysql缓存客户端线程的数量。 3、innodb_lock_wait_timeout 控制InnoDB事务等待行锁的时间，对于快速处理的SQL语句，可以将行锁等待超时时间调大，以避免发生大的回滚操作。

### 14.索引优化

1.最佳左前缀法则

\#1.定义：在创建了多列索引的情况下，查询从索引的最左前列开始且不能跳过索引中的列。

\#2.创建组合索引，并执行explain。

2.不要在索引列上做任何操作

在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效从而转向全表扫描。

3.范围右边全失效 存储引擎不能使用索引中范围右边的列，也就是说范围右边的索引列会失效。

4.尽量使用覆盖索引 尽量使用覆盖索引（查询列和索引列尽量一致，通俗说就是对A、B列创建了索引，然后查询中也使用A、B列），减少select *的使用。

5.使用不等于（！=或<>）会使索引失效

结论：使用！=会使type=ALL，key=Null，导致全表扫描，并且索引失效。

6.is null或 is not null也无法使用索引

在使用is null的时候，索引完全失效，使用is not null的时候，type=ALL全表扫描，key=Null索引失效。

7.like通配符以%开头会使索引失效

①like的%位置不同，所产生的效果不一样，当%出现在左边的时候，type=ALL，key=Null（全表扫描，索引失效），当%出现在右边的时候，type=range，索引未失效。 ②like查询为范围查询，%出现在左边，则索引失效。%出现在右边索引未失效。口诀：like百分加右边。

8.字符串不加单引号导致索引失效

结论：varchar类型的字段，在查询的时候不加单引号导致索引失效，转向全表扫描。

9.少用or，用or连接会使索引失效

结论：通过上述explain的执行结果可看出，在使用or连接的时候type=ALL，key=Null，索引失效，并全表扫描。

### 15.实践中如何优化MySQL

实践中，MySQL的优化主要涉及SQL语句及索引的优化、数据表结构的优化、系统配置的优化和硬件的优化四个方面

⑴ SQL语句优化：

SQL语句的优化主要包括三个问题，即如何发现有问题的SQL、如何分析SQL的执行计划以及如何优化SQL，下面将逐一解释。

**① 怎么发现有问题的SQL?（通过MySQL慢查询日志对有效率问题的SQL进行监控）**

MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10s以上的语句。

通过MySQL的慢查询日志，我们可以查询出执行的次数多占用的时间长的SQL、可以通过pt_query_disgest(一种mysql慢日志分析工具)分析Rows examine(MySQL执行器需要检查的行数)项去找出IO大的SQL以及发现未命中索引的SQL，对于这些SQL，都是我们优化的对象。

**② 通过explain查询和分析SQL的执行计划：**

使用 EXPLAIN 关键字可以知道MySQL是如何处理你的SQL语句的，以便分析查询语句或是表结构的性能瓶颈。通过explain命令可以得到表的读取顺序、数据读取操作的操作类型、哪些索引可以使用、哪些索引被实际使用、表之间的引用以及每张表有多少行被优化器查询等问题。当扩展列extra出现Using filesort和Using temporay，则往往表示SQL需要优化了。

**③ SQL语句的优化：**

**⒈优化insert语句：一次插入多值；**

**⒉应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描；**

**⒊应尽量避免在 where 子句中对字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描；**

**⒋优化嵌套查询：子查询可以被更有效率的连接(Join)替代；**

**⒌很多时候用 exists 代替 in 是一个好的选择。**

**⒍选择最有效率的表名顺序：数据库的解析器按照从右到左的顺序处理FROM子句中的表名，FROM子句中写在最后的表将被最先处理**

在FROM子句中包含多个表的情况下：

- 如果三个表是完全无关系的话，将记录和列名最少的表，写在最后，然后依次类推
- 也就是说：选择记录条数最少的表放在最后

如果有3个以上的表连接查询：

- 如果三个表是有关系的话，将引用最多的表，放在最后，然后依次类推。
- 也就是说：被其他表所引用的表放在最后

**⒎用IN代替OR：**

```html
select * from emp where sal = 1500 or sal = 3000 or sal = 800;
select * from emp where sal in (1500,3000,800);
```

**⒏SELECT子句中避免使用\*号：**

我们最开始接触 SQL 的时候，“`*`” 号是可以获取表中全部的字段数据的，**但是它要通过查询数据字典完成，这意味着将消耗更多的时间**，而且使用 “`*`” 号写出来的 SQL 语句也不够直观。

⑵ 索引优化：

建议在经常作查询选择的字段、经常作表连接的字段以及经常出现在 order by、group by、distinct 后面的字段中建立索引。但必须注意以下几种可能会引起索引失效的情形：

- 以 “%(表示任意0个或多个字符)” 开头的 LIKE 语句，模糊匹配；
- OR语句前后没有同时使用索引；
- 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型）；
- 对于多列索引，必须满足最左匹配原则(eg,多列索引col1、col2和col3，则 索引生效的情形包括col1或col1，col2或col1，col2，col3)。

⑶ 数据库表结构的优化：

**① 选择合适数据类型：**

- 使用较小的数据类型解决问题；
- 使用简单的数据类型(mysql处理int要比varchar容易)；
- 尽可能的使用not null 定义字段；
- 尽量避免使用text类型，非用不可时最好考虑分表；

**② 表的范式的优化：**

一般情况下，表的设计应该遵循三大范式。

**③ 表的垂直拆分：**

把含有多个列的表拆分成多个表，解决表宽度问题，具体包括以下几种拆分手段：

- 把不常用的字段单独放在同一个表中；
- 把大字段独立放入一个表中；
- 把经常使用的字段放在一起；

这样做的好处是非常明显的，具体包括：拆分后业务清晰，拆分规则明确、系统之间整合或扩展容易、数据维护简单

**④ 表的水平拆分：**

表的水平拆分用于解决数据表中数据过大的问题，水平拆分每一个表的结构都是完全一致的。一般地，将数据平分到N张表中的常用方法包括以下两种：

- 对ID进行hash运算，如果要拆分成5个表，mod(id,5)取出0~4个值；
- 针对不同的hashID将数据存入不同的表中；

表的水平拆分会带来一些问题和挑战，包括跨分区表的数据查询、统计及后台报表的操作等问题，但也带来了一些切实的好处：

- 表分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询速度；
- 表中的数据本来就有独立性，例如表中分别记录各个地区的数据或不同时期的数据，特别是有些数据常用，而另外一些数据不常用。
- 需要把数据存放到多个数据库中，提高系统的总体可用性(分库，鸡蛋不能放在同一个篮子里)。

⑸ 硬件的优化：

- CPU：核心数多并且主频高的
- 内存：增大内存
- 磁盘配置和选择：磁盘性能

### 16.**sql优化**

（1）explain出来的各种item的意义

（2）profile的意义以及使用场景。

（3）explain中的索引问题。

（1） explain出来的各种item的意义

**id**:每个被独立执行的操作的标志，表示对象被操作的顺序。一般来说，id值大，先被执行；如果id值相同，则顺序从上到下。

**select_type**：查询中每个select子句的类型。具体待明天补充。

**table**:名字，被操作的对象名称，通常的表名(或者别名)，但是也有其他格式。

**partitions**:匹配的分区信息。

**type**:join类型。具体指待明天补充。

**possible_keys**：列出可能会用到的索引。

**key**:实际用到的索引。

**key_len**:用到的索引键的平均长度，单位为字节。

**ref**:表示本行被操作的对象的参照对象，可能是一个常量用const表示，也可能是其他表的key指向的对象，比如说驱动表的连接列。

**rows**:估计每次需要扫描的行数。

**filtered**:rows*filtered/100表示该步骤最后得到的行数(估计值)。

**extra**:重要的补充信息。

（2） profile的意义以及使用场景。

Profile用来分析sql性能的消耗分布情况。当用explain无法解决慢SQL的时候，需要用profile来对sql进行更细致的分析，找出sql所花的时间大部分消耗在哪个部分，确认sql的性能瓶颈。

（3） explain中的索引问题。

Explain结果中，一般来说，要看到尽量用index(type为const、ref等，key列有值)，避免使用全表扫描(type显式为ALL)。比如说有where条件且选择性不错的列，需要建立索引。被驱动表的连接列，也需要建立索引。被驱动表的连接列也可能会跟where条件列一起建立联合索引。当有排序或者group by的需求时，也可以考虑建立索引来达到直接排序和汇总的需求。

### 17.**innodb的读写参数优化**

（1）读取参数，global buffer pool以及 local buffer

（2）写入参数

（3）与IO相关的参数

（4）缓存参数以及缓存的适用场景

（1）读取参数，global buffer pool以及 local buffer

Global buffer：

Innodb_buffer_pool_size

innodb_log_buffer_size

innodb_additional_mem_pool_size

local buffer(下面的都是server层的session变量，不是innodb的)：

Read_buffer_size

Join_buffer_size

Sort_buffer_size

Key_buffer_size

Binlog_cache_size

（2）写入参数

insert_buffer_size

innodb_double_write

innodb_write_io_thread

innodb_flush_method

（3）与IO相关的参数

Sync_binlog

Innodb_flush_log_at_trx_commit

Innodb_lru_scan_depth

Innodb_io_capacity

Innodb_io_capacity_max

innodb_log_buffer_size

innodb_max_dirty_pages_pct

（4）缓存参数以及缓存的适用场景

**指的是查询缓存吗？？？使用于读多写少，如分析报表等等**

query_cache_size

query_cache_type

query_cache_limit

maximumquery_cache_size

### 18.mysql调优

1.选择最合适的字段属性：类型、长度、是否允许NULL等；尽量把字段设为not null，⼀面查询时对比是否为null； 2.要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 3.应尽量避免在 where 子句中对字段进行 null 值判断、使用!= 或 <> 操作符，否则将导致引擎放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描。 5.in 和 not in 也要慎用，否则会导致全表扫描。 6.模糊查询也将导致全表扫描，若要提高效率，可以考虑字段建立前置索引或用全文检索； 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12.不要写一些没有意义的查询，如需要生成一个空表结构： 13.Update 语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。 14.对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。 15.select count(*) from table；这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。 16.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 17.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 18.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 19.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 20.任何地方都不要使用 select * from t ，⽤具体的字段列表代替“*”，不要返回用不到的任何字段。 21.尽量使用表变量来代替临时表。如果表变量包含⼤量数据，请注意索引非常有限（只有主键索引）。

1. 避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。
2. 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。
3. 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。
4. 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。
5. 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。
6. 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引⽤几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。
7. 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。
8. 尽量避免⼤事务操作，提高系统并发能力。
9. 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。

### 19.MySQL 数据库开发规范

\1. 所有的数据库对象名称必须使用小写字母并用下划线分割（MySQL大小写敏感，名称要见名知意，最好不超过32字符）

\2. 所有的数据库对象名称禁止使用MySQL保留关键字（如 desc、range、match、delayed 等，请参考 MySQL官方保留字 ）

\3. 临时库表必须以tmp为前缀并以日期为后缀（tmp_）

\4. 备份库和库必须以bak为前缀并以日期为后缀(bak_)

5.所有存储相同数据的列名和列类型必须一致。（在多个表中的字段如user_id，它们类型必须一致）

\6. mysql5.5之前默认的存储的引擎是myisam，没有特殊要求，所有的表必须使用innodb（innodb好处支持失误，行级锁，高并发下性能更好，对多核，大内存，ssd等硬件支持更好）

\7. 数据库和表的字符集尽量统一使用utf8（字符集必须统一，避免由于字符集转换产生的乱码，汉字utf8下占3个字节）

\8. 所有表和字段都要添加注释COMMENT，从一开始就进行数据字典的维护

\9. 尽量控制单表数据量的大小在500w以内，超过500w可以使用历史数据归档，分库分表来实现（500万行并不是MySQL数据库的限制。过大对于修改表结构，备份，恢复都会有很大问题。MySQL没有对存储有限制，取决于存储设置和文件系统）

\10. 谨慎使用mysql分区表（分区表在物理上表现为多个文件，在逻辑上表现为一个表）

\11. 谨慎选择分区键，跨分区查询效率可能更低

\12. 建议使用物理分表的方式管理大数据

\13. 尽量做到冷热数据分离，减小表的宽度（mysql限制最多存储4096列，行数没有限制，但是每一行的字节总数不能超过65535。列限制好处：减少磁盘io，保证热数据的内存缓存命中率，避免读入无用的冷数据）

\14. 禁止在表中建立预留字段（无法确认存储的数据类型，对预留字段类型进行修改，会对表进行锁定）

\15. 禁止在数据中存储图片，文件二进制数据（使用文件服务器）

\16. 禁止在线上做数据库压力测试

\17. 禁止从开发环境，测试环境直接连生产环境数据库

\18. 限制每张表上的索引数量，建议单表索引不超过5个（索引会增加查询效率，但是会降低插入和更新的速度）

\19. 避免建立冗余索引和重复索引（冗余：index（a,b,c) index(a,b) index(a)）

\20. 禁止给表中的每一列都建立单独的索引

\21. 每个innodb表必须有一个主键，选择自增id（不能使用更新频繁的列作为主键，不适用UUID,MD5,HASH,字符串列作为主键）

\22. 区分度最高的列放在联合索引的最左侧

\23. 尽量把字段长度小的列放在联合索引的最左侧

\24. 尽量避免使用外键（禁止使用物理外键，建议使用逻辑外键）

\25. 优先选择符合存储需要的最小数据类型

\26. 优先使用无符号的整形来存储

\27. 优先选择存储最小的数据类型（varchar(N),N代表的是字符数，而不是字节数，N代表能存储多少个汉字）

\28. 避免使用Text或是Blob类型

\29. 避免使用ENUM数据类型（修改ENUM值需要使用ALTER语句，ENUM类型的ORDER BY操作效率低，需要额外操作，禁止使用书值作为ENUM的枚举值

\30. 尽量把所有的字段定义为NOT NULL（索引NULL需要额外的空间来保存，所以需要暂用更多的内存，进行比较和计算要对NULL值做特别的处理）

\31. 使用timestamp或datetime类型来存储时间

\32. 同财务相关的金额数据，采用decimal类型（不丢失精度，禁止使用 float 和 double）

\33. 避免使用双%号和like，搜索严禁左模糊或者全模糊（如果需要请用搜索引擎来解决。索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索）

\34. 建议使用预编译语句进行数据库操作

\35. 禁止跨库查询（为数据迁移和分库分表留出余地，降低耦合度，降低风险）

\36. 禁止select * 查询（消耗更多的cpu和io及网络带宽资源，无法使用覆盖索引）

\37. 禁止使用不含字段列表的insert语句（不允许insert into t values（‘a’，‘b’，‘c’）不允许）

\38. in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内

\39. 禁止使用order by rand（）进行随机排序

\40. 禁止where从句中对列进行函数转换和计算（例如：where date（createtime）=‘20160901’ 会无法使用createtime列上索引。改成 where createtime>='20160901' and createtime <'20160902'）

\41. 尽量使用 union all 代替 union

\42. 拆分复杂的大SQL为多个小SQL（ MySQL一个SQL只能使用一个CPU进行计算）

\43. 尽量避免使用子查询，可以把子查询优化为join操作（子查询的结果集无法使用索引，子查询会产生临时表操作，如果子查询数据量大会影响效率，消耗过多的CPU及IO资源）

\44. 超过100万行的批量写操作，要分批多次进行操作（大批量操作可能会造成严重的主从延迟，binlog日志为row格式会产生大量的日志，避免产生大事务操作）

\45. 对于大表使用pt—online-schema-change修改表结构（避免大表修改产生的主从延迟，避免在对表字段进行修改时进行锁表）

\46. 对于程序连接数据库账号，遵循权限最小原则

\47. 超过三个表禁止 join。（需要 join 的字段，数据类型必须绝对一致；多表关联查询时，保证被关联的字段需要有索引。即使双表 join 也要注意表索引、SQL 性能。）

\48. 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。

\49. SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts最好

\50. 使用 ISNULL()来判断是否为 NULL 值。

\51. 尽量不要使用物理删除（即直接删除，如果要删除的话提前做好备份），而是使用逻辑删除，使用字段delete_flag做逻辑删除，类型为tinyint，0表示未删除，1表示已删除

\52. 如果有 order by 的场景，请注意利用索引的有序性。order by 最后的字段是组合,索引的一部分，并且放在索引组合顺序的最后，避免出现 file_sort 的情况，影响查询性能。

\53. 在代码中写分页查询逻辑时，若 count 为 0 应直接返回，避免执行后面的分页语句

### 20.优化

优化的范围有哪些

存储、主机和操作系统⽅⾯: 1) 主机架构稳定性 2) I/O规划及配置 3) Swap交换分区 4) OS内核参数和⽹络问题 应⽤程序⽅⾯: 1) 应⽤程序稳定性 2) SQL语句性能 3) 串⾏访问资源 4) 性能⽋佳会话管理 5) 这个应⽤适不适合⽤MySQL 数据库优化⽅⾯: 1) 内存 2) 数据库结构(物理&逻辑) 3) 实例配置

优化维度

数据库优化维度有四个： 硬件、系统配置、数据库表结构、SQL及索引。 优化选择： 1) 优化成本: 硬件>系统配置>数据库表结构>SQL及索引 2) 优化效果: 硬件<系统配置<数据库表结构<SQL及索引

系统层⾯

cpu⽅⾯： vmstat、sar top、htop、nmon、mpstat 内存： free 、ps -aux 、 IO设备（磁盘、⽹络）： iostat 、 ss 、 netstat 、 iptraf、iftop、lsof、 vmstat 命令说明： Procs：r显⽰有多少进程正在等待CPU时间。b显⽰处于不可中断的休眠的进程数量。在等待I/OMemory：swpd显⽰被交换到磁盘 的数据块的数量。未被使⽤的数据块，⽤⼾缓冲数据块，⽤于操作系统的数据块的数量Swap：操作系统每秒从磁盘上交换到内存 和从内存交换到磁盘的数据块的数量。s1和s0最好是0Io：每秒从设备中读⼊b1的写⼊到设备b0的数据块的数量。反映了磁盘 I/OSystem：显⽰了每秒发⽣中断的数量(in)和上下⽂交换(cs)的数量Cpu：显⽰⽤于运⾏⽤⼾代码，系统代码，空闲，等待I/O的 CPU时间 iostat命令说明 实例命令：iostat -dk 1 5 iostat -d -k -x 5 （查看设备使⽤率（%util）和响应时间（await）） 1) tps：该设备每秒的传输次数。“⼀次传输”意思是“⼀次I/O请求”。多个逻辑请求可能会被合并为“⼀次I/O请求”。 2) iops ：硬件出⼚的时候，⼚家定义的⼀个每秒最⼤的IO次数,"⼀次传输"请求的⼤⼩是未知的。 3) kBread/s：每秒从设备（drive expressed）读取的数据量； 4) KBwrtn/s：每秒向设备（drive expressed）写⼊的数据量； 5) kBread：读取的总数据量；7、kBwrtn：写⼊的总数量数据量；这些单位都为Kilobytes。

系统层面问题解决办法

你认为到底负载高好，还是低好呢？ 在实际的生产中，一般认为 cpu只要不超过90%都没什么问题 。 当然不排除下面这些特殊情况： 问题.：cpu负载高，IO负载低 1、内存不够 2、磁盘性能差 3、SQL问题 ------>去数据库层，进一步排查sql问题 4、IO出问题了（磁盘到临界了、raid设计不好、 raid降级、锁、在单位时间内tps过.） 5、tps过高: 大量的小数据IO、大量的全表扫描 问题.：IO负载高，cpu负载低 1、大量小的IO 写操作：2、autocommit ，产生大量小IO 3、IO/PS,磁盘的一个定值，硬件出厂的时候，厂家定义的一个每秒最大 的IO次数。4、大量大的IO 写操作 5、SQL问题的几率比较大 问题三：IO和cpu负载都很高 硬件不够了或sql存在问题

基础优化

1、优化思路 定位问题点: 硬件 --> 系统 --> 应⽤ --> 数据库 --> 架构（⾼可⽤、读写分离、分库分表） 处理⽅向： 明确优化⽬标、性能和安全的折中、防患未然 2、硬件优化 主机⽅⾯： 1) 根据数据库类型，主机CPU选择、内存容量选择、磁盘选择 2) 平衡内存和磁盘资源 3) 随机的I/O和顺序的I/O 4) 主机 RAID卡的BBU(Battery Backup Unit)关闭 cpu的选择： 1) cpu的两个关键因素：核数、主频 2) 根据不同的业务类型进⾏选择 3) cpu密集型：计算⽐较多，OLTP 主频很⾼的cpu、核数还要多 4) IO密集型：查询⽐较，OLAP 核数要多，主频不⼀定⾼的 内存的选择： 1) OLAP类型数据库，需要更多内存，和数据获取量级有关。 2) OLTP类型数据⼀般内存是cpu核⼼数量的2倍到4倍，没有最佳实践。 存储⽅⾯： 1) 根据存储数据种类的不同，选择不同的存储设备 2) 配置合理的RAID级别(raid5、raid10、热备盘) 3) 对与操作系统来讲，不需要太特殊的选择，最好做好冗余（raid1）（ssd、sas 、sata） raid卡：主机raid卡选择： 1) 实现操作系统磁盘的冗余（raid1） 2) 平衡内存和磁盘资源 3) 随机的I/O和顺序的I/O 4) 主机 RAID卡的BBU(Battery Backup Unit)要关闭。 ⽹络设备⽅⾯： 使⽤流量⽀持更⾼的⽹络设备（交换机、路由器、⽹线、⽹卡、HBA卡） 注意：以上这些规划应该在初始设计系统时就应该考虑好。 3、服务器硬件优化 1) 物理状态灯： 2) ⾃带管理设备：远程控制卡（FENCE设备：ipmi ilo idarc），开关机、硬件监控。 3) 第三⽅的监控软件、设备（snmp、agent）对物理设施进⾏监控 4) 存储设备：⾃带的监控平台。EMC2（hp收购了）， ⽇⽴（hds），IBM低端OEM hds，⾼端存储是⾃⼰技术，华为存储 4、系统优化 Cpu： 基本不需要调整，在硬件选择⽅⾯下功夫即可。 内存： 基本不需要调整，在硬件选择⽅⾯下功夫即可。 SWAP： MySQL尽量避免使⽤swap。阿⾥云的服务器中默认swap为0 IO ： 1) raid、no lvm、 ext4或xfs、ssd、IO调度策略 2) Swap调整(不使⽤swap分区)

5、系统参数调整 Linux系统内核参数优化

6、应⽤优化 业务应⽤和数据库应⽤独⽴,防⽕墙：iptables、selinux等其他⽆⽤服务(关闭)：

数据库优化 SQL优化⽅向： 执⾏计划、索引、SQL改写 架构优化⽅向： ⾼可⽤架构、⾼性能架构、分库分表 1、数据库参数优化

```html
thread_concurrency # 并发线程数量个数
sort_buffer_size # 排序缓存
read_buffer_size # 顺序读取缓存
read_rnd_buffer_size # 随机读取缓存
key_buffer_size # 索引缓存
thread_cache_size # (1G—>8, 2G—>16, 3G—>32, >3G—>64)
```

连接层（基础优化） 设置合理的连接客⼾和连接⽅式

```html
max_connections # 最⼤连接数，看交易笔数设置
max_connect_errors # 最⼤错误连接数，能⼤则⼤
connect_timeout # 连接超时
max_user_connections # 最⼤⽤⼾连接数
skip-name-resolve # 跳过域名解析
wait_timeout # 等待超时
back_log # 可以在堆栈中的连接数量
```

SQL层（基础优化） querycachesize：查询缓存-->>>OLAP类型数据库,需要重点加⼤此内存缓存. 1) 但是⼀般不会超过GB. 2) 对于经常被修改的数据，缓存会⽴⻢失效。 3) 我们可以实⽤内存数据库（redis、memecache），替代他的功能。

### 21.常规调优思路：

针对业务周期性的卡顿，例如在每天10-11点业务特别慢，但是还能够使⽤，过了这段时间就好了。 1) 查看slowlog，分析slowlog，分析出查询慢的语句。 2) 按照⼀定优先级，进⾏⼀个⼀个的排查所有慢语句。 3) 分析top sql，进⾏explain调试，查看语句执⾏时间。 4) 调整索引或语句本⾝。

## 数据类型

### 1.整数类型

| 类型           | 存储长度 | 最小值               | 最大值              |
| -------------- | -------- | -------------------- | ------------------- |
| TINYINT        | 1        | －128                | 127                 |
| SMALLINT       | 2        | -32768               | 32767               |
| MEDIUMINT      | 3        | -8388608             | 8388607             |
| INT（INTEGER） | 4        | -2147483648          | 2147483647          |
| BIGINT         | 8        | -9223372036854775808 | 9223372036854775807 |

### 2.小数类型

| 类型                                       | 存储长度 |                                                              |
| ------------------------------------------ | -------- | ------------------------------------------------------------ |
| FLOAT[(M[,D])]                             | 4 or 8   | -3.402823466E+38-1.175494351E-38 1.175494351E-38～3.402823466E+38 |
| DOUBLE[(M[,D])] （ REAL,DOUBLE PRECISION） | 8        | -1.7976931348623157E+308～-2.2250738585072014E-308; 2.2250738585072014E-308～1.7976931348623157E+308 |

M 代表整个位数长度，而D 则表示小数点后的位数，默认M 为10，D 为0。

### 3.时间类型

|           |      |                     |                     |
| --------- | ---- | ------------------- | ------------------- |
| DATETIME  | 8    | 1001-01-01 00:00:00 | 9999-12-31 23:59:59 |
| DATE      | 3    | 1001-01-01          | 9999-12-31          |
| TIME      | 3    | 00:00:00            | 23:59:59            |
| YEAR      | 1    | 1001                | 9999                |
| TIMESTAMP | 4    | 1970-01-01 00:00:00 |                     |

### 4.字符存储类型

| 类型            | 存储占用最大空间                       |
| --------------- | -------------------------------------- |
| CHAR[(M)]       | 255 characters(independent of charset) |
| VARCHAR[(M)]    | 65535 bytes or 255 characters          |
| TINYTEXT[(M)]   | 255 characters（sigle-byte）           |
| TEXT[(M)]       | 65535 characters（sigle-byte）         |
| MEDIUMTEXT[(M)] | 16777215 characters（sigle-byte）      |
| LONGTEXT[(M)]   | 4294967295 characters（sigle-byte）    |

CHAR[(M)]类型属于静态长度类型，存放长度完全以字符数来计算，所以最终的存储长度是基于字符集的，。CHAR 类型的存储特点是不管我们实际存放多长数据，在数据库中都会存放M 个字符，不够的通过空格补上，M 默认为1。虽然CHAR 会通过空格补齐存放的空间，但是在访问数据的时候，MySQL 会忽略最后的所有空格，所以如果我们的实际数据中如果在最后确实需要空格，则不能使用CHAR 类型来存放。

VARCHAR[(M)]属于动态存储长度类型，仅存占用实际存储数据的长度。其存放的最大长度与MySQL版本有关，在5.0.3 之前的版本VARCHAR 以字符数控制最存储的最大长度，最大只能存放255 个字符，占用存储空间的实际大小与字符集有关。但是从5.0.3 开始，VARCHAR 的最大存储限制已经更改为字节数限制了，扩展到可以存放65535 bytes 的数据，不同的字符集可能存放的字符数并不一样。

TINYTEXT，TEXT，MEDIUMTEXT 和LONGTEXT 这四种类型同属于一种存储方式，都是动态存储长度类型，不同的仅仅是最大长度的限制。

这四种TEXT 类型和CHAR 及VARCHAR 在实际使用中存在几个不一样的地方：

- 不能设置默认值；
- 只有TEXT 可以使用TEXT[(M)]这样的方式通过M 设置大小；
- 基于这四种类型的索引必须指定前缀长度；

### 5.其他常用类型

| 类型               | 存储占用最大空间                                    |
| ------------------ | --------------------------------------------------- |
| BIT[(M)]           | (M+7)/8 bytes ,最大(64+7)/8                         |
| SET('v1','v2'...)  | 1,2,4 or 8 bytes（取决于存储值的数目，最大64 个值） |
| ENUM('v1','v2'...) | 1 or 2 bytes （取决于存储值的数目，最大65535 个值） |

对于BIT 类型，M 表示每个值的bits 数目，默认为1，最大为64 bits。

对于SET 和ENUM 类型，主要内容基本处于较少变化状态且值比较少的字段。

### 6.char 和 varchar 的区别是什么？

char是一种固定长度的类型，varchar是一种可变长度的类型，例如：

定义一个char[10]和varchar[10]，如果存进去的是 'test'，那么char所占的长度依然为10，除了字符 'test' 外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的

char的存取速度还是要比varchar要快得多，因为其长度固定，方便程序的存储于查找

char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。

varchar是以空间效率为首位。

char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。

varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。 两者的存储数据都非unicode的字符数据。

### 7.float 和 double 的区别是什么？

float 最多可以存储 8 位的十进制数，并在内存中占 4 字节。 double 最可可以存储 16 位的十进制数，并在内存中占 8 字节。

### 8.DATE、DATETIME 和 TIMESTAMP 的格式

1、对于 date 类型，支持 YYYY-MM-DD 或 YY-MM-DD 以及它们各自的变形

2、对于 datetime 和 timestamp 类型，支持 YYYY-MM-DD HH:MM:SS 或 YY-MM-DD HH:MM:SS ，以及它们的变形

这种格式下也有几个点要注意 1、日期和时间部分与小数秒部分之间的唯一可用的分隔符号是小数点 ( . ) 例如 2018-09-13 21:15:10.11 2、日期与时间部分的分隔符不一定就要使用空格 ( ' ' )，还可以使用字符 T ，例如 2018-09-13 21:15:10 和 2018-09-13T21:15:10 是等价的 同时还可以使用没有任何分隔符的形式，比如 YYYYMMDDHHMMSS 或 YYMMDDHHMMSS ，例如 '20180913211510' 与 2018-09-13 21:15:10 是等价的

### 9.数值类型的宽度显示属性

对于所有的数值类型，MySQL 通过扩展的方式，可以有选择性的在类型的 base 关键字后面的括号中指定 「 整数 」 数据类型的 「 显示宽度 」 。 注意: 我们使用 「」 括号扩起来的 整数 和 显示宽度 两个词。 例如，例如， INT(4) 指定显示宽度为 4 位的 INT 。对于一个类型为 INT(4) 的列，实际存储仍然使用 4 个字节，但显示的时候就有有细微的差别： 1、 如果给该列传递的值大于等于四位，比如 5201314 ，那么显示的时候会使用原值，也就是显示5201314 2、 但如果给该列传递的值小于四位，比如 520 ，那么显示的时候就会在左边填充 0 ，也就是显示位0520

### 10.数值类型的 UNSIGNED 显示属性

所有整数类型都可以具有可选 ( 非标准 ) 属性 UNSIGNED 。无符号类型可用于仅允许列中的非负数或当需要更大的列的上限数字范围时。例如，如果 INT 列为 UNSIGNED，则列的范围大小相同，终端显示时范围从 -2147483648 和 2147483647 更改为 0 和 4294967295 。 从某些方面说，就是 UNSIGNED 属性并不会更改底层的数据存储格式，仍然是 SIGNED 有符号整数，但在显示时，会将有符号的值自动转换为无符号的值。也就是说 UNSIGNED 也是一个显示属性浮点和精确精度类型也可以是 UNSIGNED ，与整数类型一样，此属性可防止负值存储在列中。但与整数类型不同，列值的上限范围保持不变。

### 11.存储时期

Datatime:以YYYY-MM-DD HH:MM:SS 格式存储时期时间，精确到秒，占用8 个字节得存储空间，datatime 类型与时区无关 Timestamp:以时间戳格式存储，占用4 个字节，范围小1970-1-1 到2038-1-19，显示依赖于所指定得时区，默认在第一个列行的数据修改时可以自动得修改timestamp 列得值 Date:（生日） 占用得字节数比使用字符串.datatime.int 储存要少，使用date 只需要3 个字节， 存储日期月份，还可以利用日期时间函数进行日期间得计算 Time:存储时间部分得数据 注意:不要使用字符串类型来存储日期时间数据（ 通常比字符串占用得储存空间小，在进行查找过滤可以利用日期得函数） 使用int 存储日期时间不如使用timestamp 类型

### 12.mysql有哪些数据类型

1、整数类型 ，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3 字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整 数。 长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意 义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配 合使用才有意义。 例子，假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数 据库实际存储数据为00012。 2、实数类型，包括FLOAT、DOUBLE、DECIMAL。 DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。 而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。 计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。 3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB VARCHAR用于存储可变长字符串，它比定长类型更节省空间。 VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2 字节表示。 VARCHAR存储的内容超出设置的长度时，内容会被截断。 CHAR是定长的，根据定义的字符串长度分配足够的空间。 CHAR会根据需要使用空格进行填充方便比较。 CHAR适合存储很短的字符串，或者所有值都接近同一个长度。 CHAR存储的内容超出设置的长度时，内容同样会被截断。 使用策略： 对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。 对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。 尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。 4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。 有时可以使用ENUM代替常用的字符串类型。 ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。 ENUM在内部存储时，其实存的是整数。 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。 排序是按照内部存储的整数

5、日期和时间类型，尽量使用timestamp，空间效率高于datetime， 用整数保存时间戳通常不方便处理。 如果需要存储微妙，可以使用bigint存储。

### 13.**varchar(50)中的30代表的涵义**

（1） varchar(50)中50的涵义

在早期MySQL版本中，50代表字节数，现在代表字符数。

（2） int（20）中20的涵义

不影响内部存储，只是影响带zerofill定义的int时，前面补多少个0，易于报表展示。

## 物理文件

### 1.日志文件

1、错误日志：Error Log

错误日志记录了MyQL Server 运行过程中所有较为严重的警告和错误信息，以及MySQL Server 每次启动和关闭的详细信息。

2、二进制日志：Binary Log & Binary Log Index

binlog，MySQL 会将所有修改数据库数据的query 以二进制形式记录到日志文件中。当然，日志中并不仅限于query 语句这么简单，还包括每一条query 所执行的时间，所消耗的资源，以及相关的事务信息，所以binlog是事务安全的。

3、更新日志：update log

其功能和binlog 基本类似，只不过不是以二进制格式来记录而是以简单的文本格式记录内容。从版本5.0 开始，MySQL 已经不再支持更新日志了。

4、查询日志：query log

查询日志记录MySQL 中所有的query，通过“--log[=fina_name]”来打开该功能。由于记录了所有的query，包括所有的select，体积比较大，开启后对性能也有较大的影响，所以请大家慎用该功能。

5、慢查询日志：slow query log

慢查询日志中记录的是执行时间较长的query，也就是我们常说的slowquery，通过设--log-slow-queries[=file_name]来打开该功能并设置记录位置和文件名。

慢查询日志采用的是简单的文本格式，可以通过各种文本编辑器查看其中的内容。其中记录了语句执行的时刻，执行所消耗的时间，执行用户，连接主机等相关信息。

6、Innodb 的在线redo 日志：innodb redo log

Innodb 是一个事务安全的存储引擎，其事务安全性主要就是通过在线redo 日志和记录在表空间中的undo 信息来保证的。redo 日志中记录了Innodb 所做的所有物理变更和事务信息，通过redo 日志和undo 信息，Innodb 保证了在任何情况下的事务安全性。

### 2.数据文件

1、“.frm”文件

与表相关的元数据（meta）信息都存放在“.frm”文件中，包括表结构的定义信息等。不论是什么存储引擎，每一个表都会有一个以表名命名的“.frm”文件。所有的“.frm”文件都存放在所属数据库的文件夹下面。

2、“.MYD”文件

“.MYD”文件是MyISAM 存储引擎专用，存放MyISAM 表的数据。每一个MyISAM 表都会有一个“.MYD”文件与之对应，同样存放于所属数据库的文件夹下，和“.frm”文件在一起。

3、“.MYI”文件

“.MYI”文件也是专属于MyISAM 存储引擎的，主要存放MyISAM 表的索引相关信息。对于MyISAM 存储来说，可以被cache 的内容主要就是来源于“.MYI”文件中。每一个MyISAM表对应一个“.MYI”文件，存放于位置和“.frm”以及“.MYD”一样。

4、“.ibd”文件和ibdata 文件

这两种文件都是存放Innodb 数据的文件，之所以有两种文件来存放Innodb 的数据（包括索引），是因为Innodb 的数据存储方式能够通过配置来决定是使用共享表空间存放存储数据，还是独享表空间存放存储数据。独享表空间存储方式使用“.ibd”文件来存放数据，且每个表一个“.ibd”文件，文件存放在和MyISAM 数据相同的位置。如果选用共享存储表空间来存放数据，则会使用ibdata 文件来存放，所有表共同使用一个（或者多个，可自行配置）ibdata 文件。

### 3.redo log的底层实现原理

缓存页和redo log写磁盘区别

redo log日志大致的格式如下：对表空间XX中的数据页XX中的偏移量为XXXX的地方更新了数据XXX。

缓存页刷入磁盘是随机写磁盘，性能是很差的。

edo log写日志，是顺序写入磁盘文件，每次都是追加到磁盘文件末尾去，速度很快。

在Buffer Pool执行完增删改之后，写入日志文件的redo log长什么样？

redo log里本质上记录的就是在对某个表空间的某个数据页的某个偏移量的地方修改了几个字节的值，具体修改的值是什么，他里面需要记录的就是**表空间号+数据页号+偏移量+修改几个字节的值+具体的值**。

所以根据你修改了数据页里的几个字节的值，redo log就划分为了不同的类型，MLOG_1BYTE类型的日志指的就是修改了1个字节的值，MLOG_2BYTE类型的日志指的就是修改了2个字节的值，以此类推，还有修改了4个字节的值的日志类型，修改了8个字节的值的日志类型。

当然，如果你要是一下子修改了一大串的值，类型就是MLOG_WRITE_STRING，就是代表你一下子在那个数据页的某个偏移量的位置插入或者修改了一大串的值。

所以其实一条redo log看起来大致的结构如下所示：

日志类型（就是类似MLOG_1BYTE之类的），表空间ID，数据页号，数据页中的偏移量，具体修改的数据。

大致就是一条redo log中依次排列上述的一些东西，这条redo log表达的语义就很明确了，他的类型是什么，类型就告诉了你他这次增删改操作修改了多少字节的数据；

然后在哪个表空间里操作的，这个就是跟你SQL在哪个表里执行的是对应的；接着就是在这个表空间的哪个数据页里执行的，在数据页的哪个偏移量开始执行的，具体更新的数据是哪些呢。

有了上述信息，就可以精准完美的还原出来一次数据增删改操作做的变动了。

只不过如果是MLOG_WRITE_STRING类型的日志，因为不知道具体修改了多少字节的数据，所以其实会多一个修改数据长度，就告诉你他这次修改了多少字节的数据，如下所示他的格式：

日志类型（就是类似MLOG_1BYTE之类的），表空间ID，数据页号，数据页中的偏移量，修改数据长度，具体修改的数据。

redo log block！

MySQL内有另外一个数据结构，叫做**redo log block**，

对于redo log也不是单行单行的写入日志文件的，他是用一个redo log block来存放多个单行日志的。一个redo log block是512字节，这个redo log block的512字节分为3个部分，一个是12字节的header块头，一个是496字节的body块体，一个是4字节的trailer块尾。

12字节的header头又分为了4个部分。

1. 包括4个字节的block no，就是块唯一编号；
2. 2个字节的data length，就是block里写入了多少字节数据；
3. 2个字节的first record group。这个是说每个事务都会有多个redo log，是一个redo log group，即一组redo log。那么在这个block里的第一组redo log的偏移量，就是这2个字节存储的；
4. 4个字节的checkpoint on。

每一个redo log都是写入到文件里的一个redo log block里去的，一个block最多放496自己的redo log日志。

如果依次在磁盘文件里的末尾追加不停的写字节数据，就是磁盘顺序写；但是假设现在磁盘文件里已经有很多很多的redo log block了，此时要在磁盘里某个随机位置找到一个redo log block去修改他里面几个字节的数据，这就是磁盘随机写。

redo log buffer！

redo log buffer，他就是MySQL专门设计了用来缓冲redo log写入的。

redo log buffer其实就是MySQL在启动的时候，就跟操作系统申请的一块连续内存空间，大概可以认为相当于是buffer pool吧。redo log buffer也是申请出来的一片连续内存，然后里面划分出了N多个空的redo log block。

通过设置mysql的innodb_log_buffer_size可以指定这个redo log buffer的大小，默认的值就是16MB，其实已经够大了，毕竟一个redo log block才512字节而已，每一条redo log其实也就几个字节到几十个字节罢了。

当你要写一条redo log的时候，就会先从第一个redo log block开始写入，写满了一个redo log block，就会继续写下一个redo log block，以此类推，直到所有的redo log block都写满。万一要是redo log buffer里所有的redo log block都写满了呢？那此时必然会强制把redo log block刷入到磁盘中去的！

如果一组redo log实在是太多了，那么就可能会存放在两个redo log block中。

redo log buffer中的缓冲日志，到底什么时候可以写入磁盘？

redo log在写的时候，都是一个事务里的一组redo log，先暂存在一个地方，完事儿了以后把一组redo log写入redo log buffer。

写入redo log buffer的时候，是写入里面提前划分好的一个一个的redo log block的，选择有空闲空间的redo log block去写入，然后redo log block写满之后，其实会在某个时机刷入到磁盘里去。

redo log block是哪些时候会刷入到磁盘文件里去：

**（1）**如果写入redo log buffer的日志已经占据了redo log buffer总容量的一半了，也就是超过了8MB的redo log在缓冲里了，此时就会把他们刷入到磁盘文件里去。

**（2）**一个事务提交的时候，必须把他的那些redo log所在的redo log block都刷入到磁盘文件里去，只有这样，当事务提交之后，他修改的数据绝对不会丢失，因为redo log里有重做日志，随时可以恢复事务做的修改。

**（3）**后台线程定时刷新，有一个后台线程每隔1秒就会把redo log buffer里的redo log block刷到磁盘文件里去。

**（4）**MySQL关闭的时候，redo log block都会刷入到磁盘里去。

如果你瞬间执行大量的高并发的SQL语句，1秒内就产生了超过8MB的redo log，此时占据了redo log buffer一半的空间了，必然会直接把你的redo log刷入磁盘里去。

提交事务的时候，事务对应的redo log必须是刷入磁盘文件，接着才算是事务提交成功，否则事务提交就是失败，保证这一点，就能确保事务提交之后，数据不会丢，有redo log在磁盘里就行了。

当然，绝对保证数据不丢，还得配置一个参数，提交事务把redo log刷入磁盘文件的os cache之后，还得强行从os cache刷入物理磁盘。

默认情况下，redo log都会写入一个目录中的文件里，这个目录可以通过show variables like 'datadir'来查看，可以通过innodb_log_group_home_dir参数来设置这个目录的。

然后redo log是有多个的，写满了一个就会写下一个redo log，而且可以限制redo log文件的数量，通过innodb_log_file_size可以指定每个redo log文件的大小，默认是48MB，通过innodb_log_files_in_group可以指定日志文件的数量，默认就2个。

所以默认情况下，目录里就两个日志文件，分别为ib_logfile0和ib_logfile1，每个48MB，最多就这2个日志文件，就是先写第一个，写满了写第二个。那么如果第二个也写满了呢？别担心，继续写第一个，覆盖第一个日志文件里原来的redo log就可以了。

所以最多这个redo log，mysql就给你保留了最近的96MB的redo log而已，不过这其实已经很多了，毕竟redo log真的很小，一条通常就几个字节到几十个字节不等，96MB足够你存储上百万条redo log了！

如果你还想保留更多的redo log，其实调节上述两个参数就可以了，比如每个redo log文件是96MB，最多保留100个redo log文件。

### 4.undo log日志/回滚日志原理！

如果事务执行到一半要回滚怎么办？

在执行事务的时候，必须引入另外一种日志，就是undo log回滚日志。

这个回滚日志，他记录的东西其实非常简单，比如你要是在缓存页里执行了一个insert语句，那么此时你在undo log日志里，对这个操作记录的回滚日志就必须是有一个主键和一个对应的delete操作，要能让你把这次insert操作给回退了。

那么比如说你要是执行的是delete语句，那么起码你要把你删除的那条数据记录下来，如果要回滚，就应该执行一个insert操作把那条数据插入回去。

如果你要是执行的是update语句，那么起码你要把你更新之前的那个值记录下来，回滚的时候重新update一下，把你之前更新前的旧值给他更新回去。

如果你要是执行的是select语句呢？不好意思，select语句压根儿没有在buffer pool里执行任何修改，所以根本不需要undo log！

INSRET语句的undo log回滚日志长什么样？

你执行事务的时候，里面很多INSERT、UPDATE和DELETE语句都在更新缓存页里的数据，但是万一事务回滚，你必须有每条SQL语句对应的undo log回滚日志，根据回滚日志去恢复缓存页里被更新的数据。

INSERT语句的undo log的类型是TRX_UNDO_INSERT_REC，这个undo log里包含了以下一些东西：

- 这条日志的开始位置
- 主键的各列长度和值
- 表id
- undo log日志编号
- undo log日志类型
- 这条日志的结束位置

首先，一条日志必须得有自己的一个开始位置。

主键的各列长度和值，意思就是你插入的这条数据的主键的每个列，他的长度是多少，具体的值是多少。即使你没有设置主键，MySQL自己也会给你弄一个row_id作为隐藏字段，做你的主键。

表id，你插入一条数据必然是往一个表里插入数据的，记录下来表id是在哪个表里插入的数据了。

undo log日志编号，每个undo log日志都是有自己的编号的。在一个事务里会有多个SQL语句，就会有多个undo log日志，在每个事务里的undo log日志的编号都是从0开始的，然后依次递增。

至于undo log日志类型，就是TRX_UNDO_INSERT_REC，insert语句的undo log日志类型就是这个东西。

最后一个undo log日志的结束位置，告诉你undo log日志结束的位置是什么。

### 5.关于MySQL的一些文件

①二进制日志log-bin：用于主从复制。 ②错误日志log-error：默认关闭，记录严重的警告和错误信息，每次启动和关闭的详细信息等。 ③查询日志show-log：默认关闭，记录查询的sql语句，如果开启会降低mysql的整体性能，因为记录日志也是需要消耗系统资源的。 ④frm文件：存放表结构。 ⑤myd文件：存放表数据。 ⑥myi文件：存放表索引。 特别提出MySQL中的重要配置文件：Windows下名为my.ini，Linux下为/etc/my.cnf。对于服务器的调优相关过程都在改配置文件中，需要特别掌握。

### 6.MySQL逻辑架构

①连接层；②服务层（主要进行sql语句相关的操作）；③引擎层（注意引擎层是可拔插的）；④存储层。

## 系统架构

### 1.逻辑模块组成

总的来说，MySQL 可以看成是二层架构，第一层我们通常叫做SQL Layer，在MySQL 数据库系统处理底层数据之前的所有工作都是在这一层完成的，包括权限判断，sql 解析，执行计划优化，query cache 的处理等等；第二层就是存储引擎层，我们通常叫做Storage Engine Layer，也就是底层数据存取操作实现部分，由多种存储引擎共同组成。

SQL Layer 子模块

1、初始化模块

初始化模块就是在MySQL Server 启动的时候，对整个系统做各种各样的初始化操作，比如各种buffer，cache 结构的初始化和内存空间的申请，各种系统变量的初始化设定，各种存储引擎的初始化设置，等等。

2、核心API

核心API 模块主要是为了提供一些需要非常高效的底层操作功能的优化实现，包括各种底层数据结构的实现，特殊算法的实现，字符串处理，数字处理等，小文件I/O，格式化输出，以及最重要的内存管理部分。

3、网络交互模块

底层网络交互模块抽象出底层网络交互所使用的接口api，实现底层网络数据的接收与发送，以方便其他各个模块调用，以及对这一部分的维护。

4、Client & Server 交互协议模块

任何C/S 结构的软件系统，都肯定会有自己独有的信息交互协议，MySQL 也不例外。MySQL的Client & Server 交互协议模块部分，实现了客户端与MySQL 交互过程中的所有协议。当然这些协议都是建立在现有的OS 和网络协议之上的，如TCP/IP 以及Unix Socket。

5、用户模块

用户模块所实现的功能，主要包括用户的登录连接权限控制和用户的授权管理。他就像MySQL 的大门守卫一样，决定是否给来访者“开门”。

6、访问控制模块

造访客人进门了就可以想干嘛就干嘛么？为了安全考虑，肯定不能如此随意。这时候就需要访问控制模块实时监控客人的每一个动作，给不同的客人以不同的权限。访问控制模块实现的功能就是根据用户模块中各用户的授权信息，以及数据库自身特有的各种约束，来控制用户对数据的访问。用户模块和访问控制模块两者结合起来，组成了MySQL 整个数据库系统的权限安全管理的功能。

7、连接管理、连接线程和线程管理

连接管理模块负责监听对MySQL Server 的各种请求，接收连接请求，转发所有连接请求到线程管理模块。每一个连接上MySQL Server 的客户端请求都会被分配（或创建）一个连接线程为其单独服务。而连接线程的主要工作就是负责MySQL Server 与客户端的通信，接受客户端的命令请求，传递Server 端的结果信息等。线程管理模块则负责管理维护这些连接线程。包括线程的创建，线程的cache 等。

8、Query 解析和转发模块

在MySQL 中我们习惯将所有Client 端发送给Server 端的命令都称为query，在MySQLServer 里面，连接线程接收到客户端的一个Query 后，会直接将该query 传递给专门负责将各种Query 进行分类然后转发给各个对应的处理模块，这个模块就是query 解析和转发模块。其主要工作就是将query 语句进行语义和语法的分析，然后按照不同的操作类型进行分类，然后做出针对性的转发。

9、Query Cache 模块

Query Cache 模块在MySQL 中是一个非常重要的模块，他的主要功能是将客户端提交给MySQL 的Select 类query 请求的返回结果集cache 到内存中，与该query 的一个hash 值做一个对应。该Query 所取数据的基表发生任何数据的变化之后，MySQL 会自动使该query 的Cache 失效。在读写比例非常高的应用系统中，Query Cache 对性能的提高是非常显著的。当然它对内存的消耗也是非常大的。

10、Query 优化器模块

Query 优化器，顾名思义，就是优化客户端请求的query，根据客户端请求的query 语句，和数据库中的一些统计信息，在一系列算法的基础上进行分析，得出一个最优的策略，告诉后面的程序如何取得这个query 语句的结果。

11、表变更管理模块

表变更管理模块主要是负责完成一些DML 和DDL 的query，如：update，delte，insert，create table，alter table 等语句的处理。

12、表维护模块

表的状态检查，错误修复，以及优化和分析等工作都是表维护模块需要做的事情。

13、系统状态管理模块

系统状态管理模块负责在客户端请求系统状态的时候，将各种状态数据返回给用户，像DBA 常用的各种show status 命令，show variables 命令等，所得到的结果都是由这个模块返回的。

14、表管理器

这个模块从名字上看来很容易和上面的表变更和表维护模块相混淆，但是其功能与变更及维护模块却完全不同。大家知道，每一个MySQL 的表都有一个表的定义文件，也就是*.frm文件。表管理器的工作主要就是维护这些文件，以及一个cache，该cache 中的主要内容是各个表的结构信息。此外它还维护table 级别的锁管理。

15、日志记录模块

日志记录模块主要负责整个系统级别的逻辑层的日志的记录，包括error log，binary log，slow query log 等。

16、复制模块

复制模块又可分为Master 模块和Slave 模块两部分， Master 模块主要负责在Replication 环境中读取Master 端的binary 日志，以及与Slave 端的I/O 线程交互等工作。Slave 模块比Master 模块所要做的事情稍多一些，在系统中主要体现在两个线程上面。一个是负责从Master 请求和接受binary 日志，并写入本地relay log 中的I/O 线程。另外一个是负责从relay log 中读取相关日志事件，然后解析成可以在Slave 端正确执行并得到和Master 端完全相同的结果的命令并再交给Slave 执行的SQL 线程。

17、存储引擎接口模块

存储引擎接口模块可以说是MySQL 数据库中最有特色的一点了。目前各种数据库产品中，基本上只有MySQL 可以实现其底层数据存储引擎的插件式管理。这个模块实际上只是一个抽象类，但正是因为它成功地将各种数据处理高度抽象化，才成就了今天MySQL 可插拔存储引擎的特色。

### 2.MySQL各模块工作配合

当我们执行启动MySQL 命令之后，MySQL 的初始化模块就从系统配置文件中读取系统参数和命令行参数，并按照参数来初始化整个系统，如申请并分配buffer，初始化全局变量，以及各种结构等。同时各个存储引擎也被启动，并进行各自的初始化工作。当整个系统初始化结束后，由连接管理模块接手。连接管理模块会启动处理客户端连接请求的监听程序，包括tcp/ip 的网络监听，还有unix 的socket。这时候，MySQL Server 就基本启动完成，准备好接受客户端请求了。

当连接管理模块监听到客户端的连接请求（借助网络交互模块的相关功能），双方通过Client & Server 交互协议模块所定义的协议“寒暄”几句之后，连接管理模块就会将连接请求转发给线程管理模块，去请求一个连接线程。

线程管理模块马上又会将控制交给连接线程模块，告诉连接线程模块：现在我这边有连接请求过来了，需要建立连接，你赶快处理一下。连接线程模块在接到连接请求后，首先会检查当前连接线程池中是否有被cache 的空闲连接线程，如果有，就取出一个和客户端请求连接上，如果没有空闲的连接线程，则建立一个新的连接线程与客户端请求连接。当然，连接线程模块并不是在收到连接请求后马上就会取出一个连接线程连和客户端连接，而是首先通过调用用户模块进行授权检查，只有客户端请求通过了授权检查后，他才会将客户端请求和负责请求的连接线程连上。

在MySQL 中，将客户端请求分为了两种类型：一种是query，需要调用Parser 也就是Query 解析和转发模块的解析才能够执行的请求；一种是command，不需要调用Parser 就可以直接执行的请求。如果我们的初始化配置中打开了Full Query Logging 的功能，那么Query 解析与转发模块会调用日志记录模块将请求计入日志，不管是一个Query 类型的请求还是一个command 类型的请求，都会被记录进入日志，所以出于性能考虑，一般很少打开Full Query Logging 的功能。

当客户端请求和连接线程“互换暗号（互通协议）”接上头之后，连接线程就开始处理客户端请求发送过来的各种命令（或者query），接受相关请求。它将收到的query 语句转给Query 解析和转发模块，Query 解析器先对Query 进行基本的语义和语法解析，然后根据命令类型的不同，有些会直接处理，有些会分发给其他模块来处理。

如果是一个Query 类型的请求，会将控制权交给Query 解析器。Query 解析器首先分析看是不是一个select 类型的query，如果是，则调用查询缓存模块，让它检查该query 在query cache 中是否已经存在。如果有，则直接将cache 中的数据返回给连接线程模块，然后通过与客户端的连接的线程将数据传输给客户端。如果不是一个可以被cache 的query类型，或者cache 中没有该query 的数据，那么query 将被继续传回query 解析器，让query解析器进行相应处理，再通过query 分发器分发给相关处理模块。

如果解析器解析结果是一条未被cache 的select 语句，则将控制权交给Optimizer，也就是Query 优化器模块，如果是DML 或者是DDL 语句，则会交给表变更管理模块，如果是一些更新统计信息、检测、修复和整理类的query 则会交给表维护模块去处理，复制相关的query 则转交给复制模块去进行相应的处理，请求状态的query 则转交给了状态收集报告模块。实际上表变更管理模块根据所对应的处理请求的不同，是分别由insert 处理器、delete处理器、update 处理器、create 处理器，以及alter 处理器这些小模块来负责不同的DML和DDL 的。

在各个模块收到Query 解析与分发模块分发过来的请求后，首先会通过访问控制模块检查连接用户是否有访问目标表以及目标字段的权限，如果有，就会调用表管理模块请求相应的表，并获取对应的锁。表管理模块首先会查看该表是否已经存在于table cache 中，如果已经打开则直接进行锁相关的处理，如果没有在cache 中，则需要再打开表文件获取锁，然后将打开的表交给表变更管理模块。

当表变更管理模块“获取”打开的表之后，就会根据该表的相关meta 信息，判断表的存储引擎类型和其他相关信息。根据表的存储引擎类型，提交请求给存储引擎接口模块，调用对应的存储引擎实现模块，进行相应处理。不过，对于表变更管理模块来说，可见的仅是存储引擎接口模块所提供的一系列“标准” 接口，底层存储引擎实现模块的具体实现，对于表变更管理模块来说是透明的。他只需要调用对应的接口，并指明表类型，接口模块会根据表类型调用正确的存储引擎来进行相应的处理。

当一条query 或者一个command 处理完成（成功或者失败）之后，控制权都会交还给连接线程模块。如果处理成功，则将处理结果（可能是一个Result set，也可能是成功或者失败的标识）通过连接线程反馈给客户端。如果处理过程中发生错误，也会将相应的错误信息发送给客户端，然后连接线程模块会进行相应的清理工作，并继续等待后面的请求，重复上面提到的过程，或者完成客户端断开连接的请求。

如果在上面的过程中，相关模块使数据库中的数据发生了变化，而且MySQL 打开了binlog功能，则对应的处理模块还会调用日志处理模块将相应的变更语句以更新事件的形式记录到相关参数指定的二进制日志文件中。在上面各个模块的处理过程中，各自的核心运算处理功能部分都会高度依赖整个MySQL的核心API 模块，比如内存管理，文件I/O，数字和字符串处理等等。

### 3.Mysql 架构器中各个模块都是什么？

（1 ）、连接管理与安全验证是什么? 每个客户端都会建立一个与服务器连接的线程，服务器会有一个线程池来管理这些连接；如果客户端需要连接到 MYSQL 数据库还需要进行验证，包括用户名、密码、 主机信息等。 （2 ）、解析器是什么? 解析器的作用主要是分析查询语句，最终生成解析树；首先解析器会对查询语句的语法进行分析，分析语法是否有问题。还有解析器会查询缓存，如果在缓存中有对应的语句，就返回查询结果不进行接下来的优化执行操作。前提是缓存中的数据没有被修改，当然如果被修改了也会被清出缓存。 （3 ）、优化器怎么用? 优化器的作用主要是对查询语句进行优化操作，包括选择合适的索引，数据的读取方式，包括获取查询的开销信息，统计信息等，这也是为什么图中会有优化器指向存储引擎的箭头。之前在别的文章没有看到优化器跟存储引擎之间的关系，在这里我个人的理解是因为优化器需要通过存储引擎获取查询的大致数据和统计信息。 （4 ）、执行器是什么? 执行器包括执行查询语句，返回查询结果，生成执行计划包括与存储引擎的一些处理操作。

### 4.MySQL驱动到底是什么东西？

MySQL驱动，他会在底层跟数据库建立网络连接，有网络连接，接着才能去发送请求给数据库服务器！

然后当我们跟数据库之间有了网络连接之后，我们的Java代码才能基于这个连接去执行各种各样的增删改查SQL语句.

### 5.MySQL用了什么样的架构设计

SQL接口：负责处理接收到的SQL语句

SQL接口（SQL Interface），他是一套执行SQL语句的接口，专门用于执行我们发送给MySQL的那些增删改查的SQL语句.

MySQL的工作线程接收到SQL语句之后，就会转交给SQL接口去执行。

查询解析器：让MySQL能看懂SQL语句

查询解析器（Parser）就是负责对SQL语句进行解析的。

所谓的SQL解析，就是按照既定的SQL语法，对我们按照SQL语法规则编写的SQL语句进行解析，然后理解这个SQL语句要干什么事情。

查询优化器：选择最优的查询路径

当我们通过解析器理解了SQL语句要干什么之后，接着会找查询优化器（Optimizer）来选择一个最优的查询路径。

相当于他会告诉你，你应该按照一个什么样的步骤和顺序，去执行哪些操作，然后一步一步的把SQL语句就给完成了。

存储引擎接口

把查询优化器选择的最优查询路径，也就是你到底应该按照一个什么样的顺序和步骤去执行这个SQL语句的计划，把这个计划交给底层的存储引擎去真正的执行。

存储引擎其实就是执行SQL语句的，他会按照一定的步骤去查询内存缓存数据，更新磁盘数据，查询磁盘数据，等等，执行诸如此类的一系列的操作。

MySQL的架构设计中，SQL接口、SQL解析器、查询优化器其实都是通用的，他就是一套组件而已。

但是存储引擎的话，他是支持各种各样的存储引擎的，比如我们常见的InnoDB、MyISAM、Memory等等，我们是可以选择使用哪种存储引擎来负责具体的SQL语句执行的。

执行器：根据执行计划调用存储引擎的接口

执行器会根据优化器选择的执行方案，去调用存储引擎的接口按照一定的顺序和步骤，就把SQL语句的逻辑给执行了。

执行器就会去根据我们的优化器生成的一套执行计划，然后不停的调用存储引擎的各种接口去完成SQL语句的执行计划，大致就是不停的更新或者提取一些数据出来.

### 6.InnoDB存储引擎的架构设计

InnoDB的重要内存结构：缓冲池

InnoDB存储引擎中有一个非常重要的放在内存里的组件，就是缓冲池（Buffer Pool），这里面会缓存很多的数据，以便于以后在查询的时候，万一你要是内存缓冲池里有数据，就可以不用去查磁盘了。

undo日志文件：如何让你更新的数据可以回滚？

考虑到未来可能要回滚数据的需要，这里会把你更新前的值写入undo日志文件。

更新buffer pool中的缓存数据

当我们把要更新的那行记录从磁盘文件加载到缓冲池，同时对他加锁之后，而且还把更新前的旧值写入undo日志文件之后，我们就可以正式开始更新这行记录了，更新的时候，先是会更新缓冲池中的记录，此时这个数据就是脏数据了。

因为这个时候磁盘上还是更新前的旧值，但是内存里这行数据已经被修改了，所以就会叫他是脏数据。

Redo Log Buffer

万一MySQL所在的机器宕机了，必然会导致内存里修改过的数据丢失，这可怎么办呢？

这个时候，就必须要把对内存所做的修改写入到一个**Redo Log Buffer**里去，这也是内存里的一个缓冲区，是用来存放redo日志的.

所谓的redo日志，就是记录下来你对数据做了什么修改，比如对“id=10这行记录修改了name字段的值为xxx”，这就是一个日志。

这个redo日志其实是用来在MySQL突然宕机的时候，用来恢复你更新过的数据的。

提交事务的时候将redo日志写入磁盘中

我们想要提交一个事务了，此时就会根据一定的策略把redo日志从redo log buffer里刷入到磁盘文件里去。

此时这个策略是通过innodb_flush_log_at_trx_commit来配置的，他有几个选项。

当这个参数的值为0的时候，那么你提交事务的时候，不会把redo log buffer里的数据刷入磁盘文件的，此时可能你都提交事务了，结果mysql宕机了，然后此时内存里的数据全部丢失。相当于你提交事务成功了，但是由于MySQL突然宕机，导致内存中的数据和redo日志都丢失了。

当这个参数的值为1的时候，你提交事务的时候，就必须把redo log从内存刷入到磁盘文件里去，只要事务提交成功，那么redo log就必然在磁盘里了。此时mysql宕机重启之后，他可以根据redo日志去恢复之前做过的修改。

参数值为2时，提交事务的时候，把redo日志写入磁盘文件对应的os cache缓存里去，而不是直接进入磁盘文件，可能1秒后才会把os cache里的数据写入到磁盘文件里去。

这种模式下，你提交事务之后，redo log可能仅仅停留在os cache内存缓存里，没实际进入磁盘文件，万一此时你要是机器宕机了，那么os cache里的redo log就会丢失，同样会让你感觉提交事务了，结果数据丢了。

binlog是什么？

binlog叫做归档日志，他里面记录的是偏向于逻辑性的日志，类似于“对users表中的id=10的一行数据做了更新操作，更新以后的值是什么”，binlog不是InnoDB存储引擎特有的日志文件，是属于mysql server自己的日志文件。

在我们提交事务的时候，会把redo log日志写入磁盘文件中去。同时还会把这次更新对应的binlog日志写入到磁盘文件中去。

binlog日志的刷盘策略

有一个**sync_binlog**参数可以控制binlog的刷盘策略，他的默认值是0，此时你把binlog写入磁盘的时候，其实不是直接进入磁盘文件，而是进入os cache内存缓存。

设置为1的话，那么此时会强制在提交事务的时候，把binlog直接写入到磁盘文件里去，那么这样提交事务之后，哪怕机器宕机，磁盘上的binlog是不会丢失的。

基于binlog和redo log完成事务的提交

当我们把binlog写入磁盘文件之后，接着就会完成最终的事务提交，此时会把本次更新对应的binlog文件名称和这次更新的binlog日志在文件里的位置，都写入到redo log日志文件里去，同时在redo log日志文件里写入一个commit标记。

在完成这个事情之后，才算最终完成了事务的提交。

在redo日志中写入commit标记的意义是什么？

用来保持redo log日志与binlog日志一致的。

必须是在redo log中写入最终的事务commit标记了，然后此时事务提交成功，而且redo log里有本次更新对应的日志，binlog里也有本次更新对应的日志 ，redo log和binlog完全是一致的。

后台IO线程随机将内存更新后的脏数据刷回磁盘

MySQL有一个后台的IO线程，会在之后某个时间里，随机的把内存buffer pool中的修改后的脏数据给刷回到磁盘上的数据文件里去。

在你IO线程把脏数据刷回磁盘之前，哪怕mysql宕机崩溃也没关系，因为重启之后，会根据redo日志恢复之前提交事务做过的修改到内存里去。

总结

InnoDB存储引擎主要就是包含了一些buffer pool、redo log buffer等内存里的缓存数据，同时还包含了一些undo日志文件，redo日志文件等东西，同时mysql server自己还有binlog日志文件。

在你执行更新的时候，每条SQL语句，都会对应修改buffer pool里的缓存数据、写undo日志、写redo log buffer几个步骤；

但是当你提交事务的时候，一定会把redo log刷入磁盘，binlog刷入磁盘，完成redo log中的事务commit标记；最后后台的IO线程会随机的把buffer pool里的脏数据刷入磁盘里去。

### 1.MySQL 基础架构

MySQL 基本架构概览

- **连接器：** 身份认证和权限相关(登录 MySQL 的时候)。
- **查询缓存:** 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。
- **分析器:** 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。
- **优化器：** 按照 MySQL 认为最优的方案去执行。
- **执行器:** 执行语句，然后从存储引擎返回数据。

简单来说 MySQL 主要分为 Server 层和存储引擎层：

- **Server 层**：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。
- **存储引擎**： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。**现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。**

Server 层基本组件介绍

1) 连接器

连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。

主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。

2) 查询缓存(MySQL 8.0 版本后移除)

查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。

连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。

MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。

所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。

MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。

3) 分析器

MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步：

**第一步，词法分析**，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。

**第二步，语法分析**，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。

完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。

4) 优化器

优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。

可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。

5) 执行器

当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。

## SQL

### 1.SQL 简介

1 . 数据查询语言（ DQL )

```html
select list [ INTO new table ] FROM table source
[ WHERE search_condition ] [ GROUPBY group_by_expression］
[ HAVING search_condition ] [ ORDERBY order expression [ ASC | DESC ] ]
```

2.数据操作语言（ DML )

DML 指在SQL 语句中负责修改存储数据但不修改数据库和数据定义的语句。常用的DML 语句有insert 、update 和delete，标准语法如下。

- insert: INSERT INTO table (column1 [, column2, column3 ... ]) VALUES (value1[,value2 ，value3 ... ］） 。
- update: UPDATE table name SET column_name =value [, column_name =value ... ] [WHERE condition ］ 。
- delete: DELETE FROM table_narne ［WHERE condition ］ 。

3.事务处理语言(TPL)

TPL 用于与数据库交互管理实务，比如BEGIN TRANSACTION 、COMMIT 和ROLLBACK 语句。

4.数据控制语言（ DCL)

用于授权数据库对象，比如GRAND 语句。

5.数据定义语言（ DDL)

用于定义数据库对象、表、字段、索引等，比如CREATE TABLE 。

6.指针控制语言（ CCL)

用于定义游标， 主要在脚本、存储过程和触发器中使用。

### 2.SQL调优的常用手段

如果是应付面试，我们实在是不可能深入讲mysql的SQL优化，以后架构班里都会深入讲解，但是这里给大家说一句，互联网公司的系统，一般很少需要复杂的SQL优化，为啥呢？因为我说过很多次了，保持SQL简单，一般90%的SQL都建议是单表查询，join等逻辑放java代码里实现，不要放SQL里。

既然是单表查询了，你觉得还能有什么性能问题么？对吧

如果某个线上SQL跑的慢，十有八九就是因为那个SQL没有用索引，所以这个时候，第一步就是去看MySQL的执行计划，看看那个SQL有没有用到索引，如果没有，那么就改写一下SQL让他用上索引，或者是额外加个索引。

我的面试突击课里就讲这种互联网公司最经典和常用的SQL优化手段，其他的大家为了面试准备，可以临时去网上搜个帖子，MySQL SQL优化，随便记住一些到时候说说即可。

我这里其实主要就是讲下怎么看SQL的执行计划，这个是码农必备能力，必须能看懂执行计划，一般其实就是看SQL有没有走索引，你倒是可以在这个环节重点说下你对执行计划这块的理解就ok

explain select * from table，就ok了

table | type | possible_keys | key | key_len | ref | rows | Extra

table：哪个表

type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）

possible_keys：显示可能使用的索引

key：实际使用的索引

key_len：使用索引的长度

ref：联合索引的哪一列被用了

rows：一共扫描和返回了多少行

extra：using filesort（需要额外进行排序），using temporary（mysql构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据where来过滤出了结果）

## 安全性

### 1.如何提高 MySQL 的安全性

1.如果 M y SQL 客户端和服务器端的连接需要跨越并通过不可信任的网络，那么需要使用 ssh 隧道来加密该连接的通信。 2.使用 set password 语句来修改用户的密码，先“ mysql u root ”登陆数据库系统，然后 mysql> update mysql.user set password=password( newpwd ’’))”，最后执行 flush privileges ”。 3.MySQL 需要提防的攻击有，防偷听、篡改、回放、拒绝服务等，不涉及可用性和容错方面。对所有的连接、查询、其他操作使用基于 ACL ACL （访问控制列表）是一种路由器配置和控制网络访问的一种有力的工具，它可控制路由器应该允许或拒绝数据包通过，可监控流量，可自上向下检查网络的安全性，可检查和过滤数据和限制不必要的路由更新，因此让网络资源节约成本的 ACL 配置技术在生活中越来越广泛应用。） 即访问控制列表的安全措施来完成。 4.设置除了 root 用户外的其他任何用户不允许访问 mysql 主数据库中的 user 表 5.使用 grant 和 revoke 语句来进行用户访问控制的工作 6.不要使用明文密码，而是使用 md5() 和 sha1() 等单向的哈系函数来设置密码

7.不要选用字典中的字来做密码 8.采用防火墙可以去掉 50% 的外部危险，让数据库系统躲在防火墙后面工作，或放置在 DMZ DMZ 是英文demilitarized zone ”的缩写，隔离区，它是为了解决安装防火墙后外部网络的访问用户不能访问内部网络服务器的问题，而设立的一个非安全系统与安全系统之间的缓冲区） 区域中 9.从因特网上用 nmap 来扫描 3306 端口，也可用 telnet server_host 3306 的方法测试，不允许从非信任网络中访问数据库服务器的 3306 号 tcp 端口，需要在防火墙或路由器上做设定 10.服务端要对 SQL 进行预编译，避免 SQL 注入攻击， 例如 where id=234 ，别人却输入 where id=234 or 1=1 。 11.在传递数据给 mysql 时检查一下大小 12.应用程序连接到数据库 时 应该使用一般的用户帐号，开放少数必要的权限给该用户 13.学会使用 tcpdump 和 strings 工具来查看传输数据的安全性，例如 tcpdump l i eth0 w src or dst port 3306 strings 。以普通用户来启动 mysql 数据库服务

14.确信在 mysql 目录中只有启动数据库服务的用户才可以对文件有读和写的权限

15.不许将 process 或 super 权限付给非管理用户，该 mysqladmin processlist 可以列举出当前执行的查询文本 ;super 权限可用于切断客户端连接、改变服务器运行参数状态、控制拷贝复制数据库的服务器 16.如果不相信 dns 服务公司的服务，可以在主机名称允许表中只设置 ip 数字地址 17.使用 max _user_connections 变量来使 mysqld 服务进程，对一个指定帐户限定连接数 18.grant 语句也支持资源控制选项 19.启动 mysqld 服务进程的安全选项开关， local infile=0 或 1 若是 0 则客户端程序就无法使用 local load data 了，赋权的一个例子 grant insert(user) on mysql.user to user_name ’’@'host_name 若使用 skip grant tables 系统将对任何用户的访问不做任何访问控制，但可以用 mysq ladmin flush privileges 或 mysqladmin reload来开启访问控制 默认情况是 show databases 语句对所有用户开放，可以用 skip show databases 来关闭掉。

## 数据存储

### 1.Buffer Pool

Buffer Pool这个内存数据结构到底长个什么样子？

如何配置你的Buffer Pool的大小？

Buffer Pool默认情况下是128MB。还是有一点偏小了，我们实际生产环境下完全可以对Buffer Pool进行调整。

```html
[server]
innodb_buffer_pool_size = 2147483648
```

数据页：MySQL中抽象出来的数据单位

MySQL对数据抽象出来了一个数据页的概念，他是把很多行数据放在了一个数据页里，也就是说我们的磁盘文件中就是会有很多的数据页，每一页数据里放了很多行数据。

假设我们要更新一行数据，此时数据库会找到这行数据所在的数据页，然后从磁盘文件里把这行数据所在的数据页直接给加载到Buffer Pool里去。就是说Buffer Pool中存放的是一个一个的数据页。

磁盘上的数据页和Buffer Pool中的缓存页是如何对应起来的？

实际上默认情况下，磁盘中存放的数据页的大小是16KB，也就是说，一页数据包含了16KB的内容。

而Buffer Pool中存放的一个一个的数据页，我们通常叫做缓存页，因为毕竟Buffer Pool是一个缓冲池，里面的数据都是从磁盘缓存到内存去的。

而Buffer Pool中默认情况下，一个缓存页的大小和磁盘上的一个数据页的大小是一一对应起来的，都是16KB。

缓存页对应的描述信息是什么？

对于每个缓存页，他实际上都会有一个描述信息，这个描述信息大体可以认为是用来描述这个缓存页的.

比如包含如下的一些东西：这个数据页所属的表空间、数据页的编号、这个缓存页在Buffer Pool中的地址以及别的一些杂七杂八的东西。

每个缓存页都会对应一个描述信息，这个描述信息本身也是一块数据，在Buffer Pool中，每个缓存页的描述数据放在最前面，然后各个缓存页放在后面。

Buffer Pool中的描述数据大概相当于缓存页大小的5%左右，也就是每个描述数据大概是800个字节左右的大小，然后假设你设置的buffer pool大小是128MB，实际上Buffer Pool真正的最终大小会超出一些，可能有个130多MB的样子，因为他里面还要存放每个缓存页的描述数据。

从磁盘读取数据页到Buffer Pool的时候，free链表有什么用？

数据库启动的时候，是如何初始化Buffer Pool的？

数据库只要一启动，就会按照你设置的Buffer Pool大小，稍微再加大一点，去找操作系统申请一块内存区域，作为Buffer Pool的内存区域。

然后当内存区域申请完毕之后，数据库就会按照默认的缓存页的16KB的大小以及对应的800个字节左右的描述数据的大小，在Buffer Pool中划分出来一个一个的缓存页和一个一个的他们对应的描述数据。

只不过这个时候，Buffer Pool中的一个一个的缓存页都是空的，里面什么都没有，要等数据库运行起来之后，当我们要对数据执行增删改查的操作的时候，才会把数据对应的页从磁盘文件里读取出来，放入Buffer Pool中的缓存页中。

我们怎么知道哪些缓存页是空闲的呢？

数据库会为Buffer Pool设计一个**free链表**，他是一个双向链表数据结构，这个free链表里，每个节点就是一个空闲的缓存页的描述数据块的地址，每个节点都会双向链接自己的前后节点，组成一个双向链表。也就是说，只要你一个缓存页是空闲的，那么他的描述数据块就会被放入这个free链表中。

刚开始数据库启动的时候，可能所有的缓存页都是空闲的，因为此时可能是一个空的数据库，一条数据都没有，所以此时所有缓存页的描述数据块，都会被放入这个free链表中。

除此之外，这个free链表有一个基础节点，他会引用链表的头节点和尾节点，里面还存储了链表中有多少个描述数据块的节点，也就是有多少个空闲的缓存页。

free链表占用多少内存空间？

这个free链表，他本身其实就是由Buffer Pool里的描述数据块组成的，你可以认为是每个描述数据块里都有两个指针，一个是free_pre，一个是free_next，分别指向自己的上一个free链表的节点，以及下一个free链表的节点。

通过Buffer Pool中的描述数据块的free_pre和free_next两个指针，就可以把所有的描述数据块串成一个free链表。

对于free链表而言，只有一个基础节点是不属于Buffer Pool的，他是40字节大小的一个节点，里面就存放了free链表的头节点的地址，尾节点的地址，还有free链表里当前有多少个节点。

如何将磁盘上的页读取到Buffer Pool的缓存页中去？

首先，我们需要从free链表里获取一个描述数据块，然后就可以对应的获取到这个描述数据块对应的空闲缓存页。

接着我们就可以把磁盘上的数据页读取到对应的缓存页里去，同时把相关的一些描述数据写入缓存页的描述数据块里去，比如这个数据页所属的表空间之类的信息，最后把那个描述数据块从free链表里去除就可以了。

怎么知道数据页有没有被缓存？

如果数据页已经被缓存了，那么就会直接使用了。

所以其实**数据库还会有一个哈希表数据结构，他会用表空间号+数据页号，作为一个key，然后缓存页的地址作为value。**

当你要使用一个数据页的时候，通过“表空间号+数据页号”作为key去这个哈希表里查一下，如果没有就读取数据页，如果已经有了，就说明数据页已经被缓存了。

每次你读取一个数据页到缓存之后，都会在这个哈希表中写入一个key-value对，key就是表空间号+数据页号，value就是缓存页的地址，那么下次如果你再使用这个数据页，就可以从哈希表里直接读取出来他已经被放入一个缓存页了。

当我们更新Buffer Pool中的数据时，flush链表有什么用？

Buffer Pool中会不会有内存碎片？

当然有。

因为Buffer Pool大小是你自己定的，很可能Buffer Pool划分完全部的缓存页和描述数据块之后，还剩一点点的内存，这一点点的内存放不下任何一个缓存页了，所以这点内存就只能放着不能用，这就是内存碎片。

数据库在Buffer Pool中划分缓存页的时候，会让所有的缓存页和描述数据块都紧密的挨在一起，这样尽可能减少内存浪费，就可以尽可能的减少内存碎片的产生了。

脏数据页到底为什么会脏？

一旦你更新了缓存页中的数据，那么缓存页里的数据和磁盘上的数据页里的数据，是不是就不一致了。

哪些缓存页是脏页呢？

数据库在这里引入了另外一个跟free链表类似的**flush链表**，这个flush链表本质也是通过缓存页的描述数据块中的两个指针，让被修改过的缓存页的描述数据块，组成一个双向链表。

凡是被修改过的缓存页，都会把他的描述数据块加入到flush链表中去，flush的意思就是这些都是脏页，后续都是要flush刷新到磁盘上去的。

当你更新缓存页的时候，通过变换缓存页中的描述数据块的flush链表的指针，就可以把脏页的描述数据块组成一个双向链表，也就是flush链表，而且flush链表的基础节点会指向起始节点和尾巴节点。

对于LRU链表中尾部的缓存页，是如何淘汰他们刷入磁盘的？

定时把LRU尾部的部分缓存页刷入磁盘

首先第一个时机，并不是在缓存页满的时候，才会挑选LRU冷数据区域尾部的几个缓存页刷入磁盘，而是有一个后台线程，他会运行一个定时任务，这个定时任务每隔一段时间就会把LRU链表的冷数据区域的尾部的一些缓存页，刷入磁盘里去，清空这几个缓存页，把他们加入回free链表去！

把flush链表中的一些缓存页定时刷入磁盘

这个后台线程同时也会在MySQL不怎么繁忙的时候，找个时间把flush链表中的缓存页都刷入磁盘中，这样被你修改过的数据，迟早都会刷入磁盘的！

只要flush链表中的一波缓存页被刷入了磁盘，那么这些缓存页也会从flush链表和lru链表中移除，然后加入到free链表中去！

实在没有空闲缓存页了怎么办？

从LRU链表的冷数据区域的尾部找到一个缓存页，他一定是最不经常使用的缓存页！然后把他刷入磁盘和清空，然后把数据页加载到这个腾出来的空闲缓存页里去！

### 2.LRU算法

当Buffer Pool中的缓存页不够的时候，如何基于LRU算法淘汰部分缓存？

如果要淘汰掉一些缓存数据，淘汰谁？

当你不停的把磁盘上的数据页加载到空闲缓存页里去，free链表中不停的移除空闲缓存页，迟早有那么一瞬间，你会发现free链表中已经没有空闲缓存页了。

你必须把一个缓存页里被修改过的数据，给他刷到磁盘上的数据页里去，然后这个缓存页就可以清空了，让他重新变成一个空闲的缓存页。

缓存命中率

假设现在有两个缓存页，一个缓存页的数据，经常会被修改和查询，比如在100次请求中，有30次都是在查询和修改这个缓存页里的数据。那么此时我们可以说这种情况下，缓存命中率很高。

为什么呢？因为100次请求中，30次都可以操作缓存，不需要从磁盘加载数据，这个缓存命中率就比较高了。

另外一个缓存页里的数据，就是刚从磁盘加载到缓存页之后，被修改和查询过1次，之后100次请求中没有一次是修改和查询这个缓存页的数据的，那么此时我们就说缓存命中率有点低，因为大部分请求可能还需要走磁盘查询数据，他们要操作的数据不在缓存中。

此时要选择的话，当然是选择第二个缓存页刷入磁盘中了！

引入LRU链表来判断哪些缓存页是不常用的

通过这个LRU链表，我们可以知道哪些缓存页是最近最少被使用的，那么当你缓存页需要腾出来一个刷入磁盘的时候，不就可以选择那个LRU链表中最近最少被使用的缓存页了么？

假设我们从磁盘加载一个数据页到缓存页的时候，就把这个缓存页的描述数据块放到LRU链表头部去，那么只要有数据的缓存页，他都会在LRU里了，而且最近被加载数据的缓存页，都会放到LRU链表的头部去。

然后假设某个缓存页的描述数据块本来在LRU链表的尾部，后续你只要查询或者修改了这个缓存页的数据，也要把这个缓存页挪动到LRU链表的头部去，也就是说最近被访问过的缓存页，一定在LRU链表的头部。

LRU链表导致的问题

预读带来的一个巨大问题

预读机制，说的就是当你从磁盘上加载一个数据页的时候，他可能会连带着把这个数据页相邻的其他数据页，也加载到缓存里去！

通过预读机制加载进来的缓存页有可能比较少被访问，但是却在LRU链表的头部，导致真正使用多的缓存页跑到尾部去了被删除！

哪些情况下会触发MySQL的预读机制？

**（1）**有一个参数是innodb_read_ahead_threshold，他的默认值是56，意思就是如果顺序的访问了一个区里的多个数据页，访问的数据页的数量超过了这个阈值，此时就会触发预读机制，把下一个相邻区中的所有数据页都加载到缓存里去。

**（2）**如果Buffer Pool里缓存了一个区里的13个连续的数据页，而且这些数据页都是比较频繁会被访问的，此时就会直接触发预读机制，把这个区里的其他的数据页都加载到缓存里去

这个机制是通过参数innodb_random_read_ahead来控制的，他默认是OFF，也就是这个规则是关闭的。

另外一种可能导致频繁被访问的缓存页被淘汰的场景

全表扫描。

比如，SELECT * FROM USERS，会导致他直接一下子把这个表里所有的数据页，都从磁盘加载到Buffer Pool里去。

MySQL是如何基于冷热数据分离的方案，来优化LRU算法的？

基于冷热数据分离的思想设计LRU链表

真正的LRU链表，会被拆分为两个部分，一部分是热数据，一部分是冷数据，这个冷热数据的比例是由innodb_old_blocks_pct参数控制的，他默认是37，也就是说冷数据占比37%。

数据页第一次被加载到缓存的时候

首先数据页第一次被加载到缓存的时候，缓存页会被放在冷数据区域的链表头部。

冷数据区域的缓存页什么时候会被放入到热数据区域？

MySQL设定了一个规则，他设计了一个innodb_old_blocks_time参数，默认值1000，也就是1000毫秒。

也就是说，必须是一个数据页被加载到缓存页之后，在1s之后，你访问这个缓存页，他才会被挪动到热数据区域的链表头部去。

基于冷热数据分离方案优化后的LRU链表，是如何解决之前的问题的？

预读机制和全表扫描加载进来的缓存页，能进热数据区域吗？

除非你在冷数据区域里的缓存页，在1s之后还被人访问了，那么此时他们就会判定为未来可能会被频繁访问的缓存页，然后移动到热数据区域的链表头部去！

如果此时缓存页不够了，需要淘汰一些缓存，会怎么样？

直接淘汰冷数据区域的尾部的缓存页，刷入磁盘，就可以了。

MySQL是如何将LRU链表的使用性能优化到极致的？

LRU链表的热数据区域是如何进行优化的？

LRU链表的热数据区域的访问规则被优化了一下，即你只有在热数据区域的后3/4部分的缓存页被访问了，才会给你移动到链表头部去。

如果你是热数据区域的前面1/4的缓存页被访问，他是不会移动到链表头部去的。

这样的话，他就可以尽可能的减少链表中的节点移动了。

### 3.数据在磁盘上是怎么存储的？

我们写入数据库的一行数据，在磁盘上是怎么存储的？

MySQL为什么要引入数据页这个概念？

innodb存储引擎在这里引入了一个**数据页**的概念，也就是把数据组织成一页一页的概念，每一页有16kb，然后每次加载磁盘的数据到内存里的时候，是至少加载一页数据进去，甚至是多页数据进去。

要更新一条id=1的数据，此时他会把id=1这条数据所在的一页数据都加载到内存里去，这一页数据里，可能还包含了id=2，id=3等其他数据。接着更新id=2的数据，那么此时是不是就不用再次读取磁盘里的数据了。

磁盘和内存之间的数据交换通过数据页来执行，包括内存里更新后的脏数据，刷回磁盘的时候，也是至少一个数据页刷回去。

一行数据在磁盘上是如何存储的？

涉及到一个概念，就是行格式。

CREATE TABLE table_name (columns) ROW_FORMAT=COMPACT

ALTER TABLE table_name ROW_FORMAT=COMPACT

你可以在建表的时候，就指定一个行存储的格式，也可以后续修改行存储的格式。这里指定了一个COMPACT行存储格式，在这种格式下，每一行数据他实际存储的时候，大概格式类似下面这样：

变长字段的长度列表，null值列表，数据头，column01的值，column02的值，column0n的值......

对于每一行数据，他其实存储的时候都会有一些头字段对这行数据进行一定的描述，然后再放上他这一行数据每一列的具体的值，这就是所谓的行格式。除了COMPACT以外，还有其他几种行存储格式，基本都大同小异。

对于VARCHAR这种变长字段，在磁盘上到底是如何存储的？

变长字段在磁盘中是怎么存储的？

比如现在有一行数据，他的几个字段的类型为VRACHAR(10)，CHAR(1)，CHAR(1)，这一行数据：hello a a，另一行数据是：hi a a，那么这个时候在一个磁盘文件里可能有下面的两行数据：

hello a a hi a a

表里的很多行数据，最终落地到磁盘里的时候，都是这样一大坨数据放在一个磁盘文件里都挨着存储的。

存储在磁盘文件里的变长字段，为什么难以读取？

因为这个表里的字段如果是变长字段的，字段的长度是多少我们是不知道的！

引入变长字段的长度列表，解决一行数据的读取问题

要在存储每一行数据的时候，都保存一下他的变长字段的长度列表，这样才能解决一行数据的读取问题。

“hello”的长度是5，十六进制就是0x05，此时会在“hello a a”前面补充一些额外信息，首先就是变长字段的长度列表，你会看到这行数据在磁盘文件里存储的时候，其实是类似如下的格式：0x05 null值列表 数据头 hello a a。

此时磁盘文件上的数据可能是这样：

0x05 null值列表 数据头 hello a a 0x02 null值列表 数据头 hi a a。

引入变长字段长度列表后，如何解决变长字段的读取问题？

你会发现第一行数据的开头有一个变长字段的长度列表，里面会读取到一个0x05这个十六进制的数字，发现第一个变长字段的长度是5，于是按照长度为5，读取出来第一个字段的值，就是“hello”。

接着你知道后续两个字段都是CHAR(1)，长度都是固定的1个字符，于是此时就依次按照长度为1读取出来后续两个字段的值，分别是“a”“a”，于是最终你会读取出来“hello a a”这一行数据！

如果有多个变长字段，如何存放他们的长度？

比如一行数据有VARCHAR(10) VARCHAR(5) VARCHAR(20) CHAR(1) CHAR(1)，一共5个字段，其中三个是变长字段，此时假设一行数据是这样的：hello hi hao a a。

此时在磁盘中存储的，必须在他开头的变长字段长度列表中存储几个变长字段的长度，一定要注意一点，他这里是逆序存储的！

也就是说先存放VARCHAR(20)这个字段的长度，然后存放VARCHAR(5)这个字段的长度，最后存放VARCHAR(10)这个字段的长度。

hello hi hao三个字段的长度分别是0x05 0x02 0x03。

一行数据实际存储可能是下面这样的：

0x03 0x02 0x05 null值列表 头字段 hello hi hao a a

一行数据中的多个NULL字段值在磁盘上怎么存储？

为什么一行数据里的NULL值不能直接存储？

NULL值列表，顾名思义，说的就是你一行数据里可能有的字段值是NULL，比如你有一个name字段，他是允许为NULL的，那么实际上在存储的时候，如果你没给他赋值，他这个字段的值就是NULL。

假设这个字段的NULL值我们在磁盘上存储的时候，是按照“NULL”这么个字符串来存储，是不是很浪费存储空间。

NULL值是以二进制bit位来存储的

对所有的NULL值，不通过字符串在磁盘上存储，而是通过二进制的bit位来存储，一行数据里假设有多个字段的值都是NULL，那么这多个字段的NULL，就会以bit位的形式存放在NULL值列表中。

比如有5个字段，分别为name、address、gender、job、school，数据为“jack NULL m NULL xx_school”。4个变长字段，gender是定长字段。name不允许为NULL，其他允许为NULL。

只有name和school两个变长字段是有值的，把他们的长度按照逆序放在变长字段长度列表中就可以了

0x09 0x04 NULL值列表 头信息 column1=value1 column2=value2 ... columnN=valueN

每个字段都有一个二进制bit位的值，如果bit值是1说明是NULL，如果bit值是0说明不是NULL。

比如上面4个字段都允许为NULL，每个人都会有一个bit位，实际放在NULL值列表的时候，他是按逆序放的，所以在NULL值列表里，放的是：0101。

0x09 0x04 0101 头信息 column1=value1 column2=value2 ... columnN=valueN

实际NULL值列表存放的时候，不会说仅仅是4个bit位，他一般起码是8个bit位的倍数，如果不足8个bit位就高位补0。

0x09 0x04 00000101 头信息 column1=value1 column2=value2 ... columnN=valueN

磁盘文件中， 40个bit位的数据头以及真实数据是如何存储的？

这40个bit位里，第一个bit位和第二个bit位，都是预留位，是没任何含义的。

然后接下来有一个bit位是**delete_mask**，他标识的是这行数据是否被删除了，在MySQL里删除一行数据的时候，未必是立马把他从磁盘上清理掉，而是给他在数据头里搞1个bit标记他已经被删了。

然后下一个bit位是**min_rec_mask**，他其实就是说在B+树里每一层的非叶子节点里的最小值都有这个标记。

接下来有4个bit位是**n_owned**，他其实就是记录了一个记录数，这个记录数的作用。

接着有13个bit位是**heap_no**，他代表的是当前这行数据在记录堆里的位置。

然后是3个bit位的record_type，这就是说这行数据的类型。

0代表的是普通类型，1代表的是B+树非叶子节点，2代表的是最小值数据，3代表的是最大值数据。

最后是16个bit的next_record，这个是指向他下一条数据的指针。

我们每一行的实际数据在磁盘上是如何存储的？

一行数据在磁盘文件里存储的时候，实际上首先会包含自己的变长字段的长度列表，然后是NULL值列表，接着是数据头，然后接着才是真实数据。

上面有一行数据是“jack NULL m NULL xx_school”，那么他真实存储大致如下所示：

0x09 0x04 00000101 0000000000000000000010000000000000011001 jack m xx_school

刚开始先是他的变长字段的长度，用十六进制来存储，然后是NULL值列表，指出了谁是NULL，接着是40个bit位的数据头，然后是真实的数据值，就放在后面。

实际上字符串这些东西都是根据我们数据库指定的字符集编码，进行编码之后再存储的。

0x09 0x04 00000101 0000000000000000000010000000000000011001 616161 636320 6262626262

在实际存储一行数据的时候，会在他的真实数据部分，加入一些隐藏字段。

首先有一个DB_ROW_ID字段，这就是一个行的唯一标识，是他数据库内部给你搞的一个标识，不是你的主键ID字段。如果我们没有指定主键和unique key唯一索引的时候，他就内部自动加一个ROW_ID作为主键。

接着是一个DB_TRX_ID字段，这是跟事务相关的，他是说这是哪个事务更新的数据，这是事务ID。

最后是DB_ROLL_PTR字段，这是回滚指针，是用来进行事务回滚的。

所以如果你加上这几个隐藏字段之后，实际一行数据可能看起来如下所示：

0x09 0x04 00000101 0000000000000000000010000000000000011001 00000000094C（DB_ROW_ID）00000000032D（DB_TRX_ID） EA000010078E（DB_ROL_PTR） 616161 636320 6262626262

### 4.多行数据的数据页

行溢出是什么东西？

比如有一个表的字段类型是VARCHAR(65532)，意思就是最大可以包含65532个字符，那也就是65532个字节，这就远大于16kb的大小了，也就是说这一行数据的这个字段都远超一个数据页的大小了！

这个时候实际上会在那一页里存储你这行数据，然后在那个字段中，仅仅包含他一部分数据，同时包含一个20个字节的指针，指向了其他的一些数据页，那些数据页用链表串联起来，存放这个VARCHAR(65532)超大字段里的数据。

这个过程就叫**行溢出**，就是说一行数据存储的内容太多了，一个数据页都放不下了，此时只能溢出这个数据页，把数据溢出存放到其他数据页里去，那些数据页就叫做溢出页。

包括其他的一些字段类型都是一样的，比如TEXT、BLOB这种类型的字段，都有可能出现溢出，然后一行数据就会存储在多个数据页里。

用于存放磁盘上的多行数据的数据页到底长个什么样子？

其实一个数据页拆分成了很多个部分，大体上来说包含了文件头、数据页头、最小记录和最大记录、多个数据行、空闲空间、数据页目录、文件尾部。

其中文件头占据了38个字节，数据页头占据了56个字节，最大记录和最小记录占据了26个字节，数据行区域的大小是不固定的，空闲区域的大小也是不固定的，数据页目录的大小也是不固定的，然后文件尾部占据8个字节。

把数据插入数据页的过程

刚开始一个数据页可能是空的，没有一行数据的，此时这个数据页实际上是没有数据行那个区域的。

假设我们现在要插入一行数据，先是从磁盘上加载一个空的数据页到缓存页里去，缓存页跟数据页是一 一对应的，他在磁盘上的时候就是数据页，数据页加载到缓存页里了，我们就叫他缓存页了！

所以此时在缓存页里插入一条数据，实际上就是在数据行那个区域里插入一行数据，然后空闲区域的空间会减少一些。

接着你就可以不停的插入数据到这个缓存页里去，直到他的空闲区域都耗尽了，就是这个页满了，此时数据行区域内可能有很多行数据。

在更新缓存页的同时，其实他在lru链表里的位置会不停的变动，而且肯定会在flush链表里，所以最终他一定会通过后台IO线程根据lru链表和flush链表，把这个脏的缓存页刷到磁盘上去。

### 5.表空间以及划分多个数据页的数据区，又是什么概念？

什么是表空间？

简单来说，就是我们平时创建的那些表，其实都是有一个表空间的概念，在磁盘上都会对应着“表名.ibd”这样的一个磁盘数据文件。所以其实在物理层面，表空间就是对应一些磁盘上的数据文件。

数据区，extent

一个数据区对应着连续的64个数据页，每个数据页是16kb，所以一个数据区是1mb，然后256个数据区被划分为了一组。

对于表空间而言，他的第一组数据区的第一个数据区的前3个数据页，都是固定的，里面存放了一些描述性的数据。比如FSP_HDR这个数据页，他里面就存放了表空间和这一组数据区的一些属性。

IBUF_BITMAP数据页，里面存放的是这一组数据页的所有insert buffer的一些信息。

INODE数据页，这里也是存放了一些特殊的信息。

表空间里的其他各组数据区，每一组数据区的第一个数据区的头两个数据页，都是存放特殊信息的，其实就是很多描述这组数据区的东西。

总结

**我们平时创建的那些表都是有对应的表空间的，每个表空间就是对应了磁盘上的数据文件，在表空间里有很多组数据区，一组数据区是256个数据区，每个数据区包含了64个数据页，是1mb。然后表空间的第一组数据区的第一个数据区的头三个数据页，都是存放特殊信息的；表空间的其他组数据区的第一个数据区的头两个数据页，也都是存放特殊信息的。**

### 6.MySQL数据库的日志顺序读写以及数据文件随机读写的原理

一种是对redo log、binlog这种日志进行的磁盘顺序读写，一种是对表空间的磁盘文件里的数据页进行的磁盘随机读写。

MySQL在工作的时候，尤其是执行增删改操作的时候，肯定会先从表空间的磁盘文件里读取数据页出来，这个过程其实就是典型的磁盘随机读操作。

磁盘随机读的性能是比较差的，所以不可能每次更新数据都进行磁盘随机读，必须是读取一个数据页之后放到Buffer Pool的缓存里去，下次要更新的时候直接更新Buffer Pool里的缓存页。

对于磁盘随机读来说，主要关注的性能指标是IOPS和响应延迟。

当你在Buffer Pool的缓存页里更新了数据之后，必须要写一条redo log日志，这个redo log日志，其实就是走的顺序写。

所谓顺序写，就是说在一个磁盘日志文件里，一直在末尾追加日志。

磁盘顺序写的性能其实是很高的，某种程度上来说，几乎可以跟内存随机读写的性能差不多，尤其是在数据库里其实也用了os cache机制，就是redo log顺序写入磁盘之前，先是进入os cache，就是操作系统管理的内存缓存里。

对于这个写磁盘日志文件而言，最核心关注的是磁盘每秒读写多少数据量的吞吐量指标。

## 执行计划

### 1.执行计划

每次你提交一个SQL给MySQL，他内核里的查询优化器，都会针对这个SQL语句的语义去生成一个执行计划，这个执行计划就代表了，他会怎么查各个表，用哪些索引，如何做排序和分组。

执行计划包含哪些内容

const

根据索引直接可以快速查找数据的过程，在执行计划里称之为const，意思就是性能超高的常量级的。

你的二级索引必须是唯一索引，才是属于const方式的，也就是说你必须建立unique key唯一索引，保证一个二级索引的每一个值都是唯一的，才可以。

ref

如果你是一个普通的二级索引呢，就是个普通的KEY索引，那么此时这种查询速度也是很快的，他在执行计划里叫做ref。

如果你是包含多个列的普通索引的话，那么必须是从索引最左侧开始连续多个列都是等值比较才可以是属于ref方式，就是类似于select * from table where name=x and age=x and xx=xx，然后索引可能是个KEY(name,age,xx)。

如果你用name IS NULL这种语法的话，即使name是主键或者唯一索引，还是只能走ref方式。但是如果你是针对一个二级索引同时比较了一个值还有限定了IS NULL，类似于select * from table where name=x and name IS NULL，那么此时在执行计划里就叫做ref_or_null。

range

比如写一个SQL是select * from table where age>=x and age <=x，假设age就是一个普通索引，此时就必然利用索引来进行范围筛选，一旦利用索引做了范围筛选，那么这种方式就是range。

index

针对这种只要遍历二级索引就可以拿到你想要的数据，而不需要回源到聚簇索引的访问方式，就叫做index访问方式！

联合索引是KEY(x1,x2,x3)，SQL语句是select x1,x2,x3 from table where x2=xxx，此时针对这个SQL，会直接遍历KEY(x1,x2,x3)索引树的叶子节点的那些页，一个接一个的遍历，然后找到 x2=xxx 的那个数据，就把里面的x1，x2，x3三个字段的值直接提取出来就可以了！这个遍历二级索引的过程，要比遍历聚簇索引快多了。

all

all意思就是直接全表扫描。

总结

const、ref和range本质都是基于索引查询，只要你索引查出来的数据量不是特别大，一般性能都极为高效，index稍微次一点，需要遍历某个二级索引，但是因为二级索引比较小，所以遍历性能也还可以的。

执行计划

索引的设计，必然尽可能是要让x1=xx这种条件在索引树里查找出来的数据量比较少，才能保证后续的性能比较高。

在执行SQL语句的时候，有可能是会同时查多个索引树取个交集，再回表到聚簇索引的。

### 2.多表关联的SQL语句到底是如何执行的？

select * from t1,t2 where t1.x1=xxx and t1.x2=t2.x2 and t2.x3=xxx。

如果你在FROM字句后直接来了两个表名，这意思就是要针对两个表进行查询了，而且会把两个表的数据给关联起来，假设你要是没有限定什么多表连接条件，那么可能会搞出一个笛卡尔积的东西。

举个例子，假设t1表有10条数据，t2表有5条数据，那么此时select * from t1,t2，其实会查出来50条数据，因为t1表里的每条数据都会跟t2表里的每条数据连接起来返回给你，那么不就是会查出来10 * 5 = 50条数据吗？这就是笛卡尔积。

他可能是先从一个表里查一波数据，这个表叫做“驱动表”，再根据这波数据去另外一个表里查一波数据进行关联，另外一个表叫做“被驱动表”。

内连接inner join

要求两个表里的数据必须是完全能关联上的，才能返回回来，这就是内连接。

外连接outer join

左外连接

在左侧的表里的某条数据，如果在右侧的表里关联不到任何数据，也得把左侧表这个数据给返回出来

右外连接

在右侧的表里如果关联不到左侧表里的任何数据，得把右侧表的数据返回出来。

内连接，连接条件是可以放在where语句里的，但是外连接一般是把连接条件放在ON字句里的。

嵌套循环关联（nested-loop join）

假设有两个表要一起执行关联，此时会先在一个驱动表里根据他的where筛选条件找出一波数据，比如说找出10条数据吧。就对这10条数据走一个循环，用每条数据都到另外一个被驱动表里去根据ON连接条件和WHERE里的被驱动表筛选条件去查找数据，找出来的数据就进行关联。依次类推，假设驱动表里找出来10条数据，那么就要到被驱动表里去查询10次！

通常而言，针对多表查询的语句，我们要尽量给两个表都加上索引，索引要确保从驱动表里查询也是通过索引去查找，接着对被驱动表查询也通过索引去查找。如果能做到这一点，你的多表关联语句性能就会很高！

### 3.MySQL是如何根据成本优化选择执行计划的？

全表扫描的成本计算方法

show table status like "表名"。

可以拿到你的表的统计信息，你在对表进行增删改的时候，MySQL会给你维护这个表的一些统计信息，比如这里可以看到rows和data_length两个信息，不过对于innodb来说，这个rows是估计值。

rows就是表里的记录数，data_length就是表的聚簇索引的字节数大小，此时用data_length除以1024就是kb为单位的大小，然后再除以16kb（默认一页的大小），就是有多少页，此时知道数据页的数量和rows记录数，就可以计算全表扫描的成本了。

IO成本就是：数据页数量 * 1.0 + 微调值，CPU成本就是：行记录数 * 0.2 + 微调值，他们俩相加，就是一个总的成本值，比如你有数据页100个，记录数有2万条，此时总成本值大致就是100 + 4000 = 4100，在这个左右。

索引的成本计算方法

除非你直接根据主键查，那就直接走一个聚簇索引就ok了，否则普通索引，一般都是两步走，先从二级索引查询一波数据，再根据这波数据的主键去聚簇索引回表查询。

首先，在二级索引里根据条件查一波数据的IO成本，一般是看你的查询条件涉及到几个范围，比如说name值在25100，250350两个区间，那么就是两个范围，否则name=xx就仅仅是一个范围区间。

一般一个范围区间就粗暴的认为等同于一个数据页，所以此时可能一般根据二级索引查询的时候，这个IO成本都会预估的很小，可能就是1 * 1.0 = 1，或者是n * 1.0 = n，基本就是个位数这个级别。

二级索引数据页到内存里以后，还得根据搜索条件去拿出来一波数据，拿这波数据的过程就是根据搜索条件在二级索引里搜索的过程。

这需要估算一下在二级索引里会查出多少条数据，会根据一个不是怎么太准确的算法去估算一下根据查询条件可能会在二级索引里查出多少条数据来。估算出来之后，比如估算可能会查到100条数据，此时从二级索引里查询数据的CPU成本就是100 * 0.2 + 微调值，总之就是20左右而已。

接着你拿到100条数据之后，就得回表到聚簇索引里去查询完整数据，此时先估算回表到聚簇索引的IO成本，这里比较粗暴的直接默认1条数据就得回表到聚簇索引查询一个数据页，所以100条数据就是100个数据页的IO成本，也就是100 * 1.0 + 微调值，大致是100左右。

接着因为在二级索引里搜索到的数据是100条，然后通过IO成本最多回表到聚簇索引访问100个数据页之后，就可以拿到这100条数据的完整值了，此时就可以针对这100条数据去判断，他们是否符合其他查询条件了，这里耗费的CPU成本就是100 * 0.2 + 微调值，就是20左右。

把上面的所有成本都加起来，就是1 + 20 + 100 + 20 = 141，这就是使用一个索引进行查询的成本的计算方法。

多表关联查询是如何选择执行计划的

select * from t1 join t2 on t1.x1=t2.x1 where t1.x2=xxx and t1.x3=xxx and t2.x4=xxx and t2.x5=xxx

会按照之前讲的那套方法来计算针对t1表查询的全表扫描和不同索引的成本，选择一个针对t1表的最佳访问方式，用最低成本从t1表里查出符合条件的数据来，接着就根据这波数据得去t2表里查数据，按照连接条件t1.x1=t2.x1去查，同时要符合t2.x4=xxx和t2.x5=xxx这两个条件。

此时一样会根据之前讲解的办法去估算，针对t2表的全表扫描以及基于x4、x5、x1几个字段不同索引的访问的成本，挑选一个成本最低的方法，然后从t2表里把数据给查找出来，就可以，这就完成了多表关联！

### 4.MySQL是如何基于各种规则去优化执行计划的

MySQL有时候可能会觉得你写的SQL一点都不好，直接按你的SQL生成的执行计划效率还是不够高，需要自动帮你改改。

基于IN语句的子查询执行方式

select * from t1 where x1 in (select x2 from t2 where x3=xxx)

执行计划会被优化为，先执行子查询，也就是select x2 from t2 where x3=xxx这条SQL语句，把查出来的数据都写入一个临时表里，也可以叫做物化表，意思就是说，把这个中间结果集进行物化。

假设t1表的数据量是10万条，而物化表的数据量只有500条，那么此时完全可以改成全表扫描物化表，对每个数据值都到t1表里根据x1这个字段的索引进行查找，查找物化表的这个值是否在t1表的x1索引树里，如果在的话，那么就符合条件了。

半连接

假设你有一个子查询语句：select * from t1 where x1 in (select x2 from t2 where x3=xxx)，此时其实可能会在底层把他转化为一个半连接，有点类似于下面的样子：

select t1.* from t1 semi join t2 on t1.x1=t2.x2 and t2.x3=xxx

当然，其实并没有提供semi join这种语法，这是MySQL内核里面使用的一种方式。

### 5.通过explain命令得到的SQL执行计划

所谓的执行计划，落实到底层，无非就是先访问哪个表，用哪个索引还是全表扫描，拿到数据之后如何去聚簇索引回表，是否要基于临时磁盘文件做分组聚合或者排序

我们只要用类似于：explain select * from table，这种SQL前面加一个explain命令，就可以轻松拿到这个SQL语句的执行计划。

当你执行explain命令之后，拿到的执行计划可能是类似下面这样的东西：

id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra

|+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+|

1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used |

1. id：每个SELECT都会对应一个id，一个复杂的SQL里可能会有很多个SELECT，也可能会包含多条执行计划，每一条执行计划都会有一个唯一的id。
2. select_type：这一条执行计划对应的查询是个什么查询类型。
3. table：表名。
4. partitions：表分区。
5. type：针对当前这个表的访问方法，比如说const、ref、range、index、all之类的，分别代表了使用聚簇索引、二级索引、全表扫描之类的访问方式。
6. possible_keys：他是跟type结合起来的，意思就是说你type确定访问方式了，那么到底有哪些索引是可供选择，可以使用的呢，这都会放这里。
7. key：就是在possible_keys里实际选择的那个索引。
8. key_len：索引的长度。
9. ref：使用某个字段的索引进行等值匹配搜索的时候，跟索引列进行等值匹配的那个目标值的一些信息。
10. rows：预估通过索引或者别的方式访问这个表的时候，大概可能会读取多少条数据。
11. filtered：就是经过搜索条件过滤之后的剩余数据的百分比。
12. extra：一些额外的信息，不是太重要。

select_type

有SIMPLE的，还有primary和subquery。

一般如果单表查询或者是多表连接查询，其实他们的select_type都是SIMPLE。

如果是union语句的话，就类似于select * from t1 union select * from t2，那么会对应两条执行计划，第一条执行计划是针对t1表的，select_type是PRIMARY，第二条执行计划是针对t2表的，select_type是UNION。

在使用union语句的时候，会有第三条执行计划，这个第三条执行计划意思是针对两个查询的结果依托一个临时表进行去重，这个第三条执行计划的select_type就是union_result。

如果是在SQL里有子查询，类似于select * from t1 where x1 in (select x1 ffrom t2) or x3='xxx'，此时其实会有两条执行计划，第一条执行计划的select_type是PRIMARY，第二条执行计划的select_type是SUBQUERY。

select_type是derived，意思就是说，针对子查询执行后的结果集会物化为一个内部临时表，然后外层查询是针对这个临时的物化表执行的。

select_type是DEPENDENT UNION，因为执行计划是在执行union后的查询。

type是index，意思就是直接扫描偶了某个索引树的所有叶子节点，把某个相同值的个数都统计出来就可以了。

type

直接决定了对某个表是如何从里面查询数据的，包括了const、ref、range、index、all这几种方式，分别是根据主键/唯一索引查询，根据二级索引查询，对二级索引进行全索引扫描，对聚簇索引进行全表扫描。

在连接查询时，针对被驱动表如果基于主键进行等值匹配，那么他的查询方式就是eq_ref了。

而如果要是正常基于某个二级索引进行等值匹配的时候，type就会是ref，而如果基于二级索引查询的时候允许值为null，那么查询方式就会是ref_or_null。

有一些特殊场景下针对单表查询可能会基于多个索引提取数据后进行合并，此时查询方式会是index_merge这种。

而查询方式是range的话就是基于二级索引进行范围查询，查询方式是index的时候是直接扫描二级索引的叶子节点，也就是扫描二级索引里的每条数据，最后如果是all的话就是全表扫描，也就是对聚簇索引的叶子节点扫描每条数据。

possible_keys

possible_keys，其实就是在针对一个表进行查询的时候有哪些潜在可以使用的索引。

此时就需要通过我们之前讲解的成本优化方法，去估算使用两个索引进行查询的成本，看使用哪个索引的成本更低，那么就选择用那个索引，最终选择的索引，就是执行计划里的key这个字段的值了。

key_len

其实就是当你在key里选择使用某个索引之后，那个索引里的最大值的长度是多少，这个就是给你一个参考，大概知道那个索引里的值最大能有多长。

ref

当你的查询方式是索引等值匹配的时候，比如const、ref、eq_ref、ref_or_null这些方式的时候，此时执行计划的ref字段告诉你的就是：你跟索引列等值匹配的是什么？是等值匹配一个常量值？还是等值匹配另外一个字段的值？

ref的值是const，意思就是说，是使用一个常量值跟索引里的值进行等值匹配的。

rows

你使用指定的查询方式，会查出来多少条数据。

filtered

在查询方式查出来的这波数据里再用上其他的不在索引范围里的查询条件，又会过滤出来百分之几的数据。

Extra

如果没有回表操作，仅仅在二级索引里执行，那么extra里会告诉in是Using index。

如果有个SQL语句是：

SELECT * FROM t1 WHERE x1 > 'xxx' AND x1 LIKE '%xxx'

此时他会先在二级索引index_x1里查找，查找出来的结果还会额外的跟x1 LIKE '%xxx'条件做比对，如果满足条件的才会被筛选出来，这种情况下，extra显示的是Using index condition。

join buffer

在多表关联的时候，有的时候你的关联条件并不是索引，此时就会用一种叫做**join buffer**的内存技术来提升关联的性能.

EXPLAIN SELECT * FROM t1 INNER JOIN t2 ON t1.x2 = t2.x2

要执行join，那么肯定是先得查询t1表的数据，此时是对t1表直接全表查询，查出来4578条数据，接着似乎很明确了，就是对每条数据的x2字段的值，跑到t2表里去查对应的数据，进行关联。

但是此时因为 t2 表也没法根据索引来查，也是属于全表扫描，所以每次都得对t2表全表扫描一下，根据extra提示的Using where，就是根据t1表每条数据的x2字段的值去t2表查找对应的数据了，然后此时会用join buffer技术，在内存里做一些特殊优化，减少t2表的全表扫描次数。

Using filesort

有的时候我们在SQL语句里进行排序的时候，如果排序字段是有索引的，那么其实是直接可以从索引里按照排序顺序去查找数据的.

EXPLAIN SELECT * FROM t1 ORDER BY x1 LIMIT 10

Using temporary

如果我们用group by、union、distinct之类的语法的时候，万一你要是没法直接利用索引来进行分组聚合，那么他会直接基于临时表来完成，也会有大量的磁盘操作，性能其实也是极低的。

EXPLAIN SELECT x2, COUNT(*) AS amount FROM t1 GROUP BY x2

这个SQL里只能对全表数据放到临时表里做大量的磁盘文件操作，然后才能完成对x2字段的不同的值去分组，分组完了以后对不同x2值的分组去做聚合操作，这个过程也是相当的耗时的，性能是极低的。

### 6.**profiling**工具

这个工具可以对SQL语句的执行耗时进行非常深入和细致的分析.

首先要打开这个profiling，使用set profiling=1这个命令，接着MySQL就会自动记录查询语句的profiling信息了。

此时如果执行show profiles命令，就会给你列出各种查询语句的profiling信息.他会记录下来每个查询语句的query id，所以你要针对你需要分析的query找到对他的query id。

然后就可以针对单个查询语句，看一下他的profiling具体信息，使用show profile cpu, block io for query xx，这里的xx是数字，此时就可以看到具体的profile信息了。

除了cpu以及block io以外，你还可以指定去看这个SQL语句执行时候的其他各项负载和耗时。

会给你展示出来SQL语句执行时候的各种耗时，比如磁盘IO的耗时，CPU等待耗时，发送数据耗时，拷贝数据到临时表的耗时，等等。

Sending Data：为一个SELECT语句读取和处理数据行，同时发送数据给客户端的过程，简单来说就是为你的SELECT语句把数据读出来，同时发送给客户端。

show engine innodb status。

看一下innodb存储引擎的一些状态

## 生产经验

### 1.生产经验：真实生产环境下的数据库机器配置如何规划？

普通的Java应用系统部署在机器上能抗多少并发？

Java应用系统部署的时候常选用的机器配置大致是2核4G和4核8G的较多一些，数据库部署的时候常选用的机器配置最低在8核16G以上，正常在16核32G。

一般Java应用系统部署在4核8G的机器上，每秒钟抗下500左右的并发访问量，差不多是比较合适的。

一台机器能抗下每秒多少请求，往往是跟你每个请求处理耗费多长时间是关联的。

高并发场景下，数据库应该用什么样的机器？

通常推荐的数据库至少是选用8核16G以的机器，甚至是16核32G的机器更加合适一些。

常说你有一个Java系统压力很大，负载很高，这个Java系统其实主要的压力和复杂都是集中在你依赖的那个MySQL数据库上的！

一般8核16G的机器部署的MySQL数据库，每秒抗个一两千并发请求是没问题的。

对于16核32G的机器部署的MySQL数据库而言，每秒抗个两三千，甚至三四千的并发请求也都是可以的。

另外对于数据库而言，如果可以的话，最好是采用SSD固态硬盘而不是普通的机械硬盘，因为数据库最大的复杂就在于大量的磁盘IO，他需要大量的读写磁盘文件，所以如果能使用SSD固态硬盘，那么你的数据库每秒能抗的并发请求量就会更高一些。

### 2.生产经验：互联网公司的生产环境数据库是如何进行性能测试的？

让专业的DBA部署MySQL

DBA这个时候会按照他过往的经验，用自己的MySQL生产调优参数模板，直接放到MySQL里去，然后用一个参数模板去启动这个MySQL，往往这里很多参数都是调优过的。

而且DBA还可能会对linux机器的一些OS内核参数进行一定的调整，比如说最大文件句柄之类的参数，这些参数往往也都是需要调整的。

有了数据库之后，还需要先进行压测

你得基于一些工具模拟一个系统每秒发出1000个请求到数据库上去，观察一下他的CPU负载、磁盘IO负载、网络IO负载、内存复杂，然后数据库能否每秒处理掉这1000个请求，还是每秒只能处理500个请求？这个过程，就是压测。

还可以模拟每秒发送2000个请求，甚至3000个请求，逐步的测试出来，这个数据库在目前的机器配置之下，他大致的一个负载压力如何，性能表现如何，每秒最多可以抗多少请求。

QPS和TPS到底有什么区别？

QPS：Query Per Second。你的这个数据库每秒可以处理多少个请求，你大致可以理解为，一次请求就是一条SQL语句，也就是说这个数据库每秒可以处理多少个SQL语句。

TPS：Transaction Per Second。其实就是每秒可处理的事务量，就是说数据库每秒会处理多少次事务提交或者回滚。

IO相关的压测性能指标

**（1）IOPS：**这个指的是机器的随机IO并发处理的能力，比如机器可以达到200 IOPS，意思就是说每秒可以执行200个随机IO读写请求。

如果说IOPS指标太低了，那么会导致你内存里的脏数据刷回磁盘的效率就会不高。

**（2）吞吐量：**这个指的是机器的磁盘存储每秒可以读写多少字节的数据量。

一台机器他的存储每秒可以读写多少字节的数据量，就决定了他每秒可以把多少redo log之类的日志写入到磁盘里去。一般来说我们写redo log之类的日志，都是对磁盘文件进行顺序写入的，也就是一行接着一行的写，不会说进行随机的读写，那么一般普通磁盘的顺序写入的吞吐量每秒都可以达到200MB左右。

通常而言，机器的磁盘吞吐量都是足够承载高并发请求的。

**（3）latency：**这个指标说的是往磁盘里写入一条数据的延迟。

一般来说，当然是你的磁盘读写延迟越低，那么你的数据库性能就越高，你执行每个SQL语句和事务的时候速度就会越快。

压测的时候要关注的其他性能指标

**（1）CPU负载：**CPU负载是一个很重要的性能指标，因为假设你数据库压测到了每秒处理3000请求了，可能其他的性能指标都还正常，但是此时CPU负载特别高，那么也说明你的数据库不能继续往下压测更高的QPS了，否则CPU是吃不消的。

**（2）网络负载：**这个主要是要看看你的机器带宽情况下，在压测到一定的QPS和TPS的时候，每秒钟机器的网卡会输入多少MB数据，会输出多少MB数据，因为有可能你的网络带宽最多每秒传输100MB的数据，那么可能你的QPS到1000的时候，网卡就打满了，已经每秒传输100MB的数据了，此时即使其他指标都还算正常，但是你也不能继续压测下去了。

**（3）内存负载：**这个就是看看在压测到一定情况下的时候，你的机器内存耗费了多少，如果说机器内存耗费过高了，说明也不能继续压测下去了。

### 3.生产经验：如何对生产环境中的数据库进行360度无死角压测？

数据库压测工具

**sysbench**，这个工具可以自动帮你在数据库里构造出来大量的数据，你想要多少数据，他就自动给你构造出来多少条数据。

然后这个工具接着可以模拟几千个线程并发的访问你的数据库，模拟使用各种各样的SQL语句来访问你的数据库，包括模拟出来各种事务提交到你的数据库里去，甚至可以模拟出几十万的TPS去压测你的数据库。

基于sysbench构造测试表和测试数据

```html
sysbench --db-driver=mysql --time=300 --threads=10 --report-interval=1 --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=test_user --mysql-password=test_user --mysql-db=test_db --tables=20 --table_size=1000000 oltp_read_write --db-ps-mode=disable prepare
```

- --db-driver=mysql：这个很简单，就是说他基于mysql的驱动去连接mysql数据库，你要是oracle，或者sqlserver，那自然就是其他的数据库的驱动了
- --time=300：这个就是说连续访问300秒
- --threads=10：这个就是说用10个线程模拟并发访问
- --report-interval=1：这个就是说每隔1秒输出一下压测情况
- --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=test_user --mysql-password=test_user：这一大串，就是说连接到哪台机器的哪个端口上的MySQL库，他的用户名和密码是什么
- --mysql-db=test_db --tables=20 --table_size=1000000：这一串的意思，就是说在test_db这个库里，构造20个测试表，每个测试表里构造100万条测试数据，测试表的名字会是类似于sbtest1，sbtest2这个样子的
- oltp_read_write：这个就是说，执行oltp数据库的读写测试
- --db-ps-mode=disable：这个就是禁止ps模式
- 最后一个prepare，意思是参照这个命令的设置去构造出来我们需要的数据库里的数据

对数据库进行360度的全方位测试

测试数据库的综合读写TPS，使用的是oltp_read_write模式。

测试数据库的只读性能，使用的是oltp_read_only模式。

测试数据库的删除性能，使用的是oltp_delete模式。

测试数据库的更新索引字段的性能，使用的是oltp_update_index模式。

测试数据库的更新非索引字段的性能，使用的是oltp_update_non_index模式。

测试数据库的插入性能，使用的是oltp_insert模式。

测试数据库的写入性能，使用的是oltp_write_only模式。

使用上面的命令，sysbench工具会根据你的指令构造出各种各样的SQL语句去更新或者查询你的20张测试表里的数据，同时监测出你的数据库的压测性能指标，最后完成压测之后，可以执行下面的cleanup命令，清理数据。

压测结果分析

每隔1秒都会输出一次压测报告的，此时他每隔一秒会输出类似下面的一段东西：

```html
[ 22s ] thds: 10 tps: 380.99 qps: 7312.66 (r/w/o: 5132.99/1155.86/1321.35) lat (ms, 95%): 21.33 err/s: 0.00 reconn/s: 0.00
```

- thds: 10，这个意思就是有10个线程在压测
- tps: 380.99，这个意思就是每秒执行了380.99个事务
- qps: 7610.20，这个意思就是每秒可以执行7610.20个请求
- (r/w/o: 5132.99/1155.86/1321.35)，这个意思就是说，在每秒7610.20个请求中，有5132.99个请求是读请求，1155.86个请求是写请求，1321.35个请求是其他的请求，就是对QPS进行了拆解
- lat (ms, 95%): 21.33，这个意思就是说，95%的请求的延迟都在21.33毫秒以下
- err/s: 0.00 reconn/s: 0.00，这两个的意思就是说，每秒有0个请求是失败的，发生了0次网络重连

在完成压测之后，最后会显示一个总的压测报告。

### 4.生产经验：在数据库的压测过程中，如何360度无死角观察机器性能？

观察机器的性能

可以在sysbench中不停的增加线程的数量，比如使用20个线程，甚至100个线程去并发的访问数据库，直到发现数据库的QPS和TPS上不去了。

不停的提高线程数量，不停的让数据库承载更高的QPS的过程，还需要配合我们对机器性能表现的观察来做，不能盲目的不停的增加线程去压测数据库。

为什么在不停增加线程数量的时候，要密切关注机器性能？

在压测的过程中，必须是不停的增加sysbench的线程数量，持续的让数据库承载更高的QPS，同时密切关注机器的CPU、内存、磁盘和网络的负载情况，在硬件负载情况比较正常的范围内，哪怕负载相对较高一些，也还是可以继续增加线程数量和提高数据库的QPS的。

然后当你不停的增加线程数量，发现在数据库抗下一个QPS的数值的同时，机器的CPU、内存、网络和磁盘的负载已经比较高了，到了一个有一定风险的临界值的了，此时就不能继续增加线程数量和提高数据库抗下的QPS了。

压测时如何观察机器的CPU负载情况？

```html
top - 15:52:00 up 42:35, 1 user, load average: 0.15, 0.05, 0.01
```

首先15:52:00指的是当前时间，up 42:35指的是机器已经运行了多长时间，1 user就是说当前机器有1个用户在使用。最重要的是load average: 0.15, 0.05, 0.01这行信息，他说的是CPU在1分钟、5分钟、15分钟内的负载情况。

如果你的CPU负载是1，那说明4核CPU中有一个核已经被使用的比较繁忙了，另外3个核还是比较空闲一些。要是CPU负载是1.5，说明有一个核被使用繁忙，另外一个核也在使用，但是没那么繁忙，还有2个核可能还是空闲的。

压测时如何观察机器的内存负载情况？

```html
Mem: 33554432k total, 20971520k used, 12268339 free, 307200k buffers
```

这里说的就是当前机器的内存使用情况，这个其实很简单，明显可以看出来就是总内存大概有32GB，已经使用了20GB左右的内存，还有10多G的内存是空闲的，然后有大概300MB左右的内存用作OS内核的缓冲区了。

对于内存而言，同样是要在压测的过程中紧密的观察，一般来说，如果内存的使用率在80%以内，基本都还能接受，在正常范围内，但是如果你的机器的内存使用率到了70%~80%了，就说明有点危险了，此时就不要继续增加压测的线程数量和QPS了，差不多就可以了。

压测时如何观察机器的磁盘IO情况？

使用dstat -d命令，会看到如下的东西：

```html
-dsk/total -
read writ
103k 211k
  0   11k
```

在上面可以清晰看到，存储的IO吞吐量是每秒钟读取103kb的数据，每秒写入211kb的数据。

使用命令：dstat -r，可以看到如下的信息

```html
--io/total-
read writ
0.25 31.9
  0   253
  0   39.0
```

意思就是读IOPS和写IOPS分别是多少，也就是说随机磁盘读取每秒钟多少次，随机磁盘写入每秒钟执行多少次，大概就是这个意思，一般来说，随机磁盘读写每秒在两三百次都是可以承受的。

我们需要在压测的时候密切观察机器的磁盘IO情况，如果磁盘IO吞吐量已经太高了，都达到极限的每秒上百MB了，或者随机磁盘读写每秒都到极限的两三百次了，此时就不要继续增加线程数量了，否则磁盘IO负载就太高了。

压测时观察网卡的流量情况

使用dstat -n命令，可以看到如下的信息：

```html
-net/total-
recv send
16k  17k
```

这个说的就是每秒钟网卡接收到流量有多少kb，每秒钟通过网卡发送出去的流量有多少kb，通常来说，如果你的机器使用的是千兆网卡，那么每秒钟网卡的总流量也就在100MB左右，甚至更低一些。

所以我们在压测的时候也得观察好网卡的流量情况，如果网卡传输流量已经到了极限值了，那么此时你再怎么提高sysbench线程数量，数据库的QPS也上不去了，因为这台机器每秒钟无法通过网卡传输更多的数据了。

总结

你必须不停的增加sysbench的线程数量，增加数据库抗下的QPS，同时通过各种命令观察机器的CPU、内存、磁盘和网络的负载情况，如果你发现某个硬件负载已经很高了，此时就可以不再提高数据库的QPS了。

**在硬件的一定合理的负载范围内，把数据库的QPS提高到最大，这就是数据库压测的时候最合理的一个极限QPS值**，而不是不管机器的各个硬件的负载，盲目的不停的增加sysbench的线程数量，不停的让数据库增加可以抗下的QPS的数值。

### 5.生产经验：如何为生产环境中的数据库部署监控系统？

Prometheus和Grafana是什么

Prometheus其实就是一个监控数据采集和存储系统，他可以利用监控数据采集组件（比如mysql_exporter）从你指定的MySQL数据库中采集他需要的监控数据，然后他自己有一个时序数据库，他会把采集到的监控数据放入自己的时序数据库中，其实本质就是存储在磁盘文件里。

我们采集到了MySQL的监控数据还不够，现在我们还要去看这些数据组成的一些报表，所以此时就需要使用Grafana了，Grafana就是一个可视化的监控数据展示系统，他可以把Prometheus采集到的大量的MySQL监控数据展示成各种精美的报表，让我们可以直观的看到MySQL的监控情况。

安装和启动Prometheus

prometheus-2.1.0.linux-amd64.tar.gz

node_exporter-0.15.2.linux-amd64.tar.gz

mysqld_exporter-0.10.0.linux-amd64.tar.gz

这里prometheus就是用来部署监控系统自己的，然后node_exporter是用来采集MySQL数据库所在机器的CPU、内存、网络、磁盘之类的监控数据的，mysqld_exporter就是用来采集MySQL数据库自己的一些监控数据的，比如SQL性能、连接数量之类的。

vi prometheus.yml，修改prometheus的配置文件，其实主要是在scrape_configs下面加入一大段自定义的配置，因为他需要去采集MySQL数据库本身和MySQL所在机器的监控数据。

接着必须要在/data/prometheus目录中，去执行启动命令：

/data/prometheus/prometheus --storage.tsdb.retention=30d &，这里的30d是说你的监控数据保留30天的。启动之后，就可以在浏览器中访问9090端口号去查看prometheus的主页了。

### 6.生产经验：如何为数据库的监控系统部署可视化报表系统？

部署Grafana

grafana-4.6.3.linux-x64.tar.gz

```html
tar xf grafana-4.6.3.linux-x64.tar.gz -C /data/prometheus
cd /data/prometheus
mv grafana-4.6.3 grafana
cd /data/prometheus/grafana
./bin/grafana-server &
```

接着就完成了grafana的启动，然后可以通过浏览器访问3000端口，默认的用户名和密码是admin/admin。

grafana-dashboards-1.6.1.tar.gz

Grafana的仪表盘组件。

添加MySQL机器的监控

首先我们如果想要让Prometheus去采集MySQL机器的监控数据（CPU、内存、磁盘、网络，等等），然后让Grafana可以展示出来，那么就必须先添加Prometheus对MySQL机器的监控。

首先必须要在MySQL机器上解压缩和启动node_exporter，这启动之后是个linux进程，他会自动采集这台linux机器上的CPU、磁盘、内存、网络之类的各种监控数据。

之前在Prometheus的yml配置文件中，我们已经定义了一个host监控项，他就是用来监控机器的，他的配置文件是host.yml，此时我们可以编辑一下这个host.yml配置文件，加入mysql所在机器的地址就可以了。

接着Prometheus就会跟MySQL机器上部署的node_exporter进行通信，源源不断的获取到这台机器的监控数据，写入自己的时序数据库中进行存储。

添加MySQL数据库的监控

需要在MySQL所在机器上再启动一个mysqld_exporter的组件，他负责去采集MySQL数据库自己的一些监控数据。

接着需要配置一些环境变量，去设置mysqld_exporter要监控的数据库的地址信息。

接着启动mysqld_exporter。

接着这个mysqld_exporter进程就会自动采集MySQL自己的监控数据了，然后我们还需要在Prometheus里配置一下他去跟mysqld_exporter通信获取数据以及存储，然后Grafana才能看到对应的报表。

### 7.生产经验：如何通过多个Buffer Pool来优化数据库的并发性能？

Buffer Pool在访问的时候需要加锁吗？

Buffer Pool其实本质就是一大块内存数据结构，由一大堆的缓存页和描述数据块组成的，然后加上了各种链表（free、flush、lru）来辅助他的运行。

多线程并发访问一个Buffer Pool，必然是要加锁的，然后让一个线程先完成一系列的操作，比如说加载数据页到缓存页，更新free链表，更新lru链表，然后释放锁，接着下一个线程再执行一系列的操作。

多线程并发访问加锁，数据库的性能还能好吗？

大部分情况下，每个线程都是查询或者更新缓存页里的数据，这个操作是发生在内存里的，基本都是微秒级的，很快很快，包括更新free、flush、lru这些链表，他因为都是基于链表进行一些指针操作，性能也是极高的。

所以即使每个线程排队加锁，然后执行一系列操作，数据库的性能倒也是还可以的。

MySQL的生产优化经验：多个Buffer Pool优化并发能力

MySQL默认的规则是，如果你给Buffer Pool分配的内存小于1GB，那么最多就只会给你一个Buffer Pool。

但是如果你的机器内存很大，那么你必然会给Buffer Pool分配较大的内存，比如给他个8G内存，那么此时你是同时可以设置多个Buffer Pool的。

```html
[server]
innodb_buffer_pool_size = 8589934592
innodb_buffer_pool_instances = 4
```

给buffer pool设置了8GB的总内存，然后设置了他应该有4个Buffer Pool，此时就是说，每个buffer pool的大小就是2GB。

### 8.生产经验：如何通过chunk来支持数据库运行期间的Buffer Pool动态调整？

buffer pool这种大块头，能在运行期间动态调整大小吗？

原则上说，buffer pool在运行期间是不能动态的调整自己的大小的。

因为动态调整buffer pool大小，就是需要这个时候向操作系统申请一块新的16GB的连续内存，然后把现在的buffer pool中的所有缓存页、描述数据块、各种链表，都拷贝到新的16GB的内存中去。这个过程是极为耗时的。

如何基于chunk机制把buffer pool给拆小呢？

buffer pool是由很多chunk组成的，他的大小是innodb_buffer_pool_chunk_size参数控制的，默认值就是128MB。

比如现在我们给buffer pool设置一个总大小是8GB，然后有4个buffer pool，那么每个buffer pool就是2GB，此时每个buffer pool是由一系列的128MB的chunk组成的，也就是说每个buffer pool会有16个chunk。

每个chunk就是一系列的描述数据块和缓存页，这样的话，就是把buffer pool按照chunk为单位，拆分为了一系列的小数据块，但是每个buffer pool是共用一套free、flush、lru的链表的。

基于chunk机制是如何支持运行期间，动态调整buffer pool大小的？

比如我们buffer pool现在总大小是8GB，现在要动态加到16GB，那么此时只要申请一系列的128MB大小的chunk就可以了，只要每个chunk是连续的128MB内存就行了。然后把这些申请到的chunk内存分配给buffer pool就行了。

有个这个chunk机制，此时并不需要额外申请16GB的连续内存空间，然后还要把已有的数据进行拷贝。

避免缓存页频繁的被使用完毕

如果你的缓存页使用的很快，然后后台线程释放缓存页的速度很慢，那么必然导致你频繁发现缓存页被使用完了。但是缓存页被使用的速度你是没法控制的，因为那是由你的Java系统访问数据库的并发程度来决定的，你高并发访问数据库，缓存页必然使用的很快了！

然后你后台线程定时释放一批缓存页，这个过程也很难去优化，因为你要是释放的过于频繁了，那么后台线程执行磁盘IO过于频繁，也会影响数据库的性能。

所以这里的关键点就在于，你的buffer pool有多大！

因为你的buffer pool内存很大，所以空闲缓存页是很多很多的，即使你的空闲缓存页逐步的减少，也可能需要较长时间才会发现缓存页用完了，此时才会出现一次crud操作执行的时候，先刷缓存页到磁盘，再读取数据页到缓存页来，这种情况是不会出现的太频繁的！

而一旦你的数据库高峰过去，此时缓存页被使用的速率下降了很多很多，然后后台线程会定是基于flush链表和lru链表不停的释放缓存页，那么你的空闲缓存页的数量又会在数据库低峰的时候慢慢的增加了。

### 9.生产经验：在生产环境中，如何基于机器配置来合理设置Buffer Pool？

生产环境中应该给buffer pool设置多少内存？

建议一个比较合理的、健康的比例，是给buffer pool设置你的机器内存的50%~60%左右。

比如你有32GB的机器，那么给buffer设置个20GB的内存，剩下的留给OS和其他人来用，这样比较合理一些。

buffer pool总大小=(chunk大小 * buffer pool数量)的2倍数

比如默认的chunk大小是128MB，那么此时如果你的机器的内存是32GB，你打算给buffer pool总大小在20GB左右，那么你得算一下，此时你的buffer pool的数量应该是多少个呢？

假设你的buffer pool的数量是16个，这是没问题的，那么此时chunk大小 * buffer pool的数量 = 16 * 128MB = 2048MB，然后buffer pool总大小如果是20GB，此时buffer pool总大小就是2048MB的10倍，这就符合规则了。

当然，此时你可以设置多一些buffer pool数量，比如设置32个buffer pool，那么此时buffer pool总大小（20GB）就是（chunk大小128MB * 32个buffer pool）的5倍，也是可以的。

那么此时你的buffer pool大小就是20GB，然后buffer pool数量是32个，每个buffer pool的大小是640MB，然后每个buffer pool包含5个128MB的chunk，算下来就是这么一个结果了。

SHOW ENGINE INNODB STATUS

你可能会看到如下一系列的东西：

```html
Total memory allocated xxxx;
Dictionary memory allocated xxx
Buffer pool size   xxxx
Free buffers       xxx
Database pages     xxx
Old database pages xxxx
Modified db pages  xx
Pending reads 0
Pending writes: LRU 0, flush list 0, single page 0
Pages made young xxxx, not young xxx
xx youngs/s, xx non-youngs/s
Pages read xxxx, created xxx, written xxx
xx reads/s, xx creates/s, 1xx writes/s
Buffer pool hit rate xxx / 1000, young-making rate xxx / 1000 not xx / 1000
Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s
LRU len: xxxx, unzip_LRU len: xxx
I/O sum[xxx]:cur[xx], unzip sum[16xx:cur[0]
```

- Total memory allocated，这就是说buffer pool最终的总大小是多少
- Buffer pool size，这就是说buffer pool一共能容纳多少个缓存页Free buffers，这就是说free链表中一共有多少个空闲的缓存页是可用的
- Database pages和Old database pages，就是说lru链表中一共有多少个缓存页，以及冷数据区域里的缓存页数量
- Modified db pages，这就是flush链表中的缓存页数量
- Pending reads和Pending writes，等待从磁盘上加载进缓存页的数量，还有就是即将从lru链表中刷入磁盘的数量、即将从flush链表中刷入磁盘的数量
- Pages made young和not young，这就是说已经lru冷数据区域里访问之后转移到热数据区域的缓存页的数量，以及在lru冷数据区域里1s内被访问了没进入热数据区域的缓存页的数量
- youngs/s和not youngs/s，这就是说每秒从冷数据区域进入热数据区域的缓存页的数量，以及每秒在冷数据区域里被访问了但是不能进入热数据区域的缓存页的数量
- Pages read xxxx, created xxx, written xxx，xx reads/s, xx creates/s, 1xx writes/s，这里就是说已经读取、创建和写入了多少个缓存页，以及每秒钟读取、创建和写入的缓存页数量
- Buffer pool hit rate xxx / 1000，这就是说每1000次访问，有多少次是直接命中了buffer pool里的缓存的
- young-making rate xxx / 1000 not xx / 1000，每1000次访问，有多少次访问让缓存页从冷数据区域移动到了热数据区域，以及没移动的缓存页数量
- LRU len：这就是lru链表里的缓存页的数量
- I/O sum：最近50s读取磁盘页的总数
- I/O cur：现在正在读取磁盘页的数量

尤为关注的是free、lru、flush几个链表的数量的情况，然后就是lru链表的冷热数据转移的情况，然后你的缓存页的读写情况，这些代表了你当前buffer pool的使用情况。

最关键的是两个东西，一个是你的buffer pool的千次访问缓存命中率，这个命中率越高，说明你大量的操作都是直接基于缓存来执行的，性能越高。

第二个是你的磁盘IO的情况，这个磁盘IO越多，说明你数据库性能越差。

### 10.生产经验：Linux操作系统的存储系统软件层原理剖析以及IO调度优化原理

Linux的存储系统分为VFS层、文件系统层、Page Cache缓存层、通用Block层、IO调度层、Block设备驱动层、Block设备层。

当MySQL发起一次数据页的随机读写，或者是一次redo log日志文件的顺序读写的时候，实际上会把磁盘IO请求交给Linux操作系统的VFS层。

这一层的作用，就是根据你是对哪个目录中的文件执行的磁盘IO操作，把IO请求交给具体的文件系统。

举个例子，在linux中，有的目录比如/xx1/xx2里的文件其实是由NFS文件系统管理的，有的目录比如/xx3/xx4里的文件其实是由Ext3文件系统管理的，那么这个时候VFS层需要根据你是对哪个目录下的文件发起的读写IO请求，把请求转交给对应的文件系统。

接着文件系统会先在Page Cache这个基于内存的缓存里找你要的数据在不在里面，如果有就基于内存缓存来执行读写，如果没有就继续往下一层走，此时这个请求会交给通用Block层，在这一层会把你对文件的IO请求转换为Block IO请求。

接着IO请求转换为Block IO请求之后，会把这个Block IO请求交给IO调度层，在这一层里默认是用CFQ公平调度算法的。

可能假设此时你数据库发起了多个SQL语句同时在执行IO操作。

有一个SQL语句可能非常简单，比如update xxx set xx1=xx2 where id=1，他其实可能就只要更新磁盘上的一个block里的数据就可以了。

但是有的SQL语句，比如说select * from xx where xx1 like "%xx%"可能需要IO读取磁盘上的大量数据。

那么此时如果基于公平调度算法，就会导致他先执行第二个SQL语句的读取大量数据的IO操作，耗时很久，然后第一个仅仅更新少量数据的SQL语句的IO操作，就一直在等待他，得不到执行的机会。

一般建议MySQL的生产环境，需要调整为deadline IO调度算法，他的核心思想就是，任何一个IO操作都不能一直不停的等待，在指定时间范围内，都必须让他去执行。

所以基于deadline算法，上面第一个SQL语句的更新少量数据的IO操作可能在等待一会儿之后，就会得到执行的机会，这也是一个生产环境的IO调度优化经验。

最后IO完成调度之后，就会决定哪个IO请求先执行，哪个IO请求后执行，此时可以执行的IO请求就会交给Block设备驱动层，然后最后经过驱动把IO请求发送给真正的存储硬件，也就是Block设备层。

然后硬件设备完成了IO读写操作之后，要不然是写，要不然是读，最后就把响应经过上面的层级反向依次返回，最终MySQL可以得到本次IO读写操作的结果。

### 11.生产经验：数据库服务器使用的RAID存储架构初步介绍

很多数据库部署在机器上的时候，存储都是搭建的RAID存储架构，RAID就是一个磁盘冗余阵列。

在存储层面往往会在机器里搞多块磁盘，然后引入RAID这个技术，大致理解为用来管理机器里的多块磁盘的一种磁盘阵列技术！有了他以后，你在往磁盘里读写数据的时候，他会告诉你应该在哪块磁盘上读写数据。

除此之外，RAID技术很重要的一个作用，就是他还可以实现**数据冗余机制**。

有的RAID磁盘冗余阵列技术里，是可以把你写入的同样一份数据，在两块磁盘上都写入的，这样可以让两块磁盘上的数据一样，作为冗余备份，然后当你一块磁盘坏掉的时候，可以从另外一块磁盘读取冗余数据出来，这一切都是RAID技术自动帮你管理的，不需要你操心。

具体来说，RAID还可以分成不同的技术方案，比如RAID 0、RAID 1、RAID 0+1、RAID2，等等，一直到RAID 10，很多种不同的多磁盘管理技术方案。

### 12.生产经验：数据库服务器上的RAID存储架构的电池充放电原理

服务器使用多块磁盘组成的RAID阵列的时候，一般会有一个RAID卡，这个RAID卡是带有一个缓存的，这个缓存不是直接用我们的服务器的主内存的那种模式，他是一种跟内存类似的SDRAM，当然，你大致就认为他也是基于内存来存储的吧！

然后我们可以把RAID的缓存模式设置为write back，这样的话，所有写入到磁盘阵列的数据，先会缓存在RAID卡的缓存里，后续慢慢再写入到磁盘阵列里去，这种写缓冲机制，可以大幅度提升我们的数据库磁盘写的性能。

RAID卡一般都配置有自己独立的锂电池或者是电容，如果服务器突然掉电了，无法接通电源了，RAID卡自己是基于锂电池来供电运行的，然后他会赶紧把缓存里的数据写入到阵列中的磁盘上去。

但是锂电池是存在性能衰减问题的，所以一般来说锂电池都是要配置定时充放电的，也就是说每隔30天~90天（不同的锂电池厂商是不一样的），就会自动对锂电池充放电一次，这可以延长锂电池的寿命和校准电池容量。

在锂电池充放电的过程中，RAID的缓存级别会从write back变成write through，我们通过RAID写数据的时候，IO就直接写磁盘了，如果写内存的话，性能也就是0.1ms这个级别，但是直接写磁盘，就性能退化10倍到毫秒级了！

优化

对于RAID锂电池充放电问题导致的存储性能抖动，一般有**三种解决方案：**

1. 给RAID卡把锂电池换成电容，电容是不用频繁充放电的，不会导致充放电的性能抖动，还有就是电容可以支持透明充放电，就是自动检查电量，自动进行充电，不会说在充放电的时候让写IO直接走磁盘，但是更换电容很麻烦，而且电容比较容易老化，这个其实一般不常用。
2. 手动充放电，这个比较常用，包括一些大家知道的顶尖互联网大厂的数据库服务器的RAID就是用了这个方案避免性能抖动，就是关闭RAID自动充放电，然后写一个脚本，脚本每隔一段时间自动在晚上凌晨的业务低峰时期，脚本手动触发充放电，这样可以避免业务高峰期的时候RAID自动充放电引起性能抖动。
3. 充放电的时候不要关闭write back，就是设置一下，锂电池充放电的时候不要把缓存级别从write back修改为write through，这个也是可以做到的，可以和第二个策略配合起来使用。

## 实战

### 1.案例实战：数据库无法连接故障的定位，Too many connections

异常信息：“ERROR 1040(HY000): Too many connections”，这个时候就是说数据库的连接池里已经有太多的连接了，不能再跟你建立新的连接了！

Java系统部署在2台机器上，Java系统设置的连接池的最大大小是200，一共最多建立400个连接。

检查了一下MySQL的配置文件，my.cnf，里面有一个关键的参数是max_connections，就是MySQL能建立的最大连接数，设置的是800。

可以用命令行或者一些管理工具登录到MySQL去，可以执行下面的命令看一下：

show variables like 'max_connections'

看到，当前MySQL仅仅只是建立了214个连接而已！

检查一下MySQL的启动日志，可以看到如下的字样：

Could not increase number of max_open_files to more than mysqld (request: 65535)

Changed limits: max_connections: 214 (requested 2000)

Changed limits: table_open_cache: 400 (requested 4096)

所以说，看看日志就很清楚了，MySQL发现自己无法设置max_connections为我们期望的800，只能强行限制为214了！

这是为什么呢？简单来说，就是因为底层的linux操作系统把进程可以打开的文件句柄数限制为了1024了，导致MySQL最大连接数是214！

如何解决经典的Too many connections故障？

其实核心就是一行命令：

ulimit -HSn 65535

然后就可以用如下命令检查最大文件句柄数是否被修改了

cat /etc/security/limits.conf

cat /etc/rc.local

这个linux的ulimit命令是干嘛用的，其实说白了，linux的话是默认会限制你每个进程对机器资源的使用的，包括可以打开的文件句柄的限制，可以打开的子进程数的限制，网络缓存的限制，最大可以锁定的内存大小。

因为linux操作系统设计的初衷，就是要尽量避免你某个进程一下子耗尽机器上的所有资源，所以他默认都是会做限制的。

如果linux限制你一个进程的文件句柄太少的话，那么就会导致我们没办法创建大量的网络连接，此时我们的系统进程就没法正常工作了。

往往你在生产环境部署了一个系统，比如数据库系统、消息中间件系统、存储系统、缓存系统之后，都需要调整一下linux的一些内核参数，这个文件句柄的数量是一定要调整的，通常都得设置为65535。

我们平时可以用ulimit命令来设置每个进程被限制使用的资源量，用ulimit -a就可以看到进程被限制使用的各种资源的量

比如 core file size 代表的进程崩溃时候的转储文件的大小限制，max locked memory就是最大锁定内存大小，open files就是最大可以打开的文件句柄数量，max user processes就是最多可以拥有的子进程数量。

设置之后，我们要确保变更落地到/etc/security/limits.conf文件里，永久性的设置进程的资源限制。

所以执行ulimit -HSn 65535命令后，要用如下命令检查一下是否落地到配置文件里去了。

cat /etc/security/limits.conf

cat /etc/rc.local

### 2.案例实战：线上数据库不确定性的性能抖动优化实践

导致线上数据库的查询和更新语句莫名其妙出现性能抖动情况

- 可能buffer pool的缓存页都满了，此时你执行一个SQL查询很多数据，一下子要把很多缓存页flush到磁盘上去，刷磁盘太慢了，就会导致你的查询语句执行的很慢。因为你必须等很多缓存页都flush到磁盘了，你才能执行查询从磁盘把你需要的数据页加载到buffer pool的缓存页里来。
- 可能你执行更新语句的时候，redo log在磁盘上的所有文件都写满了，此时需要回到第一个redo log文件覆盖写，覆盖写的时候可能就涉及到第一个redo log文件里有很多redo log日志对应的更新操作改动了缓存页，那些缓存页还没flush到磁盘，此时就必须把那些缓存页flush到磁盘，才能执行后续的更新语句，那你这么一等待，必然会导致更新执行的很慢了。

上述两个场景导致的大量缓存页flush到磁盘，就会导致莫名其妙的SQL语句性能抖动了。

如果要尽量避免缓存页flush到磁盘可能带来的性能抖动问题，那么核心的就两点。

第一个是尽量减少缓存页flush到磁盘的频率，第二个是尽量提升缓存页flush到磁盘的速度。

优化方案一

给你的数据库采用大内存机器，给buffer pool分配的内存空间大一些，那么他缓存页填满的速率低一些，flush磁盘的频率也会比较低。

优化方案二

尽可能减少flush缓存页到磁盘的时间开销到最小。

建议大家对数据库部署机器的SSD固态硬盘能承载的最大随机IO速率做一个测试，这个可以使用fio工具来测试。查出来SSD固态硬盘的最大随机IO速率之后，就知道他每秒可以执行多少次随机IO，此时你把这个数值设置给数据库的innodb_io_capacity这个参数就可以了，尽可能的让数据库用最大速率去flush缓存页到磁盘。

但是实际flush的时候，其实他会按照innodb_io_capacity乘以一个百分比来进行刷磁盘，这个百分比就是脏页的比例，是innodb_max_dirty_pages_pct参数控制的，默认是75%，这个一般不用动。

把innodb_io_capacity设置为SSD固态硬盘的IOPS，让他刷缓存页尽量快，同时设置innodb_flush_neighbors为0，让他每次别刷临近缓存页，减少要刷缓存页的数量，这样就可以把刷缓存页的性能提升到最高。

同时也可以尽可能降低每次刷缓存页对执行SQL语句的影响。

### 3.实战

select xx from user_info where age between 20 and 25 order by score

假设你就一个联合索引，age在最左侧，那你的where是可以用上索引来筛选的，但是排序是基于score字段，那就不可以用索引了。那假设你针对age和score分别设计了两个索引，但是在你的SQL里假设基于age索引进行了筛选，是没法利用另外一个score索引进行排序的。

往往在类似这种SQL里，你的where筛选和order by排序实际上大部分情况下是没法都用到索引的！

一般这种时候往往都是让where条件去使用索引来快速筛选出来一部分指定的数据，接着再进行排序，最后针对排序后的数据拿出来一页数据。

因为基于索引进行where筛选往往可以最快速度筛选出你要的少部分数据，如果筛选出来的数据量不是太大的话，那么后续排序和分页的成本往往不会太大！

针对一些频繁使用的包含枚举值范围的一些字段，也完全可以加入到联合索引里去。

写成where province=xx and city=xx and sex in(xx, xx) and hobby in (xx, xx, xx, xx) and character=xx and age>=xx and age<=xx

即使你不需要按性别和爱好进行筛选，但是在SQL里你可以对这两个字段用in语句，把他们所有的枚举值都放进去。

在SQL里，一旦你的一个字段做范围查询用到了索引，那么这个字段接下来的条件都不能用索引了，这就是规则！

实际设计索引的时候，必须把经常用做范围查询的字段放在联合索引的最后一个，才能保证你SQL里每个字段都能基于索引去查询。

**假设在查询的时候还有一个条件**，是要根据用户最近登录时间在7天之内来进行筛选，筛选最近7天登录过APP的用户。此时你完全可以设计一个字段为：does_login_in_latest_7_days，也就是说，这个人是否在最近7天内登录过APP。假设在7天内登录了这个APP，那么这个字段就是1，否则超过7天没登录，这个字段就是0！这样就把一个时间字段转换为了一个枚举值的字段。

核心重点就是，尽量利用一两个复杂的多字段联合索引，抗下你80%以上的 查询，然后用一两个辅助索引抗下剩余20%的非典型查询，保证你99%以上的查询都能充分利用索引，就能保证你的查询速度和性能！

### 4.**mysql数据库cpu飙升到500%的话怎么处理？**

当cpu飙升到500%时，先用操作系统命令top命令观察是不是mysqld占用导致的，如果不是，找出占用高的进程，并进行相关处理。如果是mysqld造成的，show processlist，看看里面跑的session情况，是不是有消耗资源的sql在运行。找出消耗高的sql，看看执行计划是否准确，index是否缺失，或者实在是数据量太大造成。一般来说，肯定要kill掉这些线程(同时观察cpu使用率是否下降)，等进行相应的调整(比如说加索引、改sql、改内存参数)之后，再重新跑这些SQL。也有可能是每个sql消耗资源并不多，但是突然之间，有大量的session连进来导致cpu飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等。

### 5.**500**台db，在最快时间之内重启

可以使用批量ssh工具pssh来对需要重启的机器执行重启命令。也可以使用salt（前提是客户端有安装salt）或者ansible（ansible只需要ssh免登通了就行）等多线程工具同时操作多台服务器

## 分库分表

### 1.分库分表

建议MySQL单表数据量不要超过1000万，最好是在500万以内，如果能控制在100万以内，那是最佳的选择了，基本单表100万以内的数据，性能上不会有太大的问题，前提是，只要你建好索引就行。

以选择把这个用户大表拆分为比如100张表，100张表均匀分散在2台服务器上就可以了，分的时候需要指定一个字段来分，一般来说会指定userid，根据用户id进行hash后，对表进行取模，路由到一个表里去，这样可以让数据均匀分散。

只要给系统加上数据库中间件技术，设置好路由规则，就可以轻松的对2个分库上的100张表进行增删改查的操作了。平时针对某个用户增删改查，直接对他的userid进行hash，然后对表取模，做一个路由，就知道到哪个表里去找这个用户的数据了。

用户在登录的时候，可能不是根据userid登陆的，可能是根据username之类的用户名，手机号之类的来登录的，此时你又没有userid，怎么知道去哪个表里找这个用户的数据判断是否能登录呢？

关于这个问题，一般来说常规方案是建立一个索引映射表，就是说搞一个表结构为（username, userid）的索引映射表，把username和userid一一映射，然后针对username再做一次分库分表，把这个索引映射表可以拆分为比如100个表分散在两台服务器里。

然后用户登录的时候，就可以根据username先去索引映射表里查找对应的userid，比如对username进行hash然后取模路由到一个表里去，找到username对应的userid，接着根据userid进行hash再取模，然后路由到按照userid分库分表的一个表里去，找到用户的完整数据即可。

把你要搜索的所有字段同步到Elasticsearch里去，建立好搜索的索引。然后就可以通过Elasticsearch去进行复杂的多条件搜索，ES是适合干这个事儿的，然后定位到一批userid，通过userid回到分库分表环境里去找出具体的用户数据，在页面上展示出来即可。

分库分表的玩法基本都是这套思路，按业务id分库分表，建立索引映射表同时进行分库分表，数据同步到ES做复杂搜索，基本这套玩法就可以保证你的分库分表场景下，各种业务功能都可以支撑了。

### 2.如果需要进行垮库的分页操作，应该怎么来做？

当你觉得似乎必须要跨库和表查询和分页的时候，我建议你，第一，你考虑一下是不是可以把你查询里按照某个主要的业务id进行分库分表建立一个索引映射表，第二是不是可以把这个查询里要的条件都放到索引映射表里去，第三，是不是可以通过ES来搞定这个需求。

### 3.数据库扩容

刚开始拆分，表数量可以多一些，避免后续要增加表。然后数据库服务器要扩容是没问题的，直接把表做一下迁移就行了，然后修改路由规则。

### 4.分库分表之后,id 主键如何处理

因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个全局唯一的 id 来支持。 生成全局 id 有下面这几种方式： UUID：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。 数据库自增 id : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。 利用 redis 生成 id : 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。 Twitter的snowflake算法 美团的Leaf分布式ID生成系统 ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。

### 5.说说MySQL分区

InnoDB逻辑存储结构

首先要先介绍一下InnoDB逻辑存储结构和区的概念，它的所有数据都被逻辑地存放在表空间，表空间又由段，区，页组成。

段

段就是上图的segment区域，常见的段有数据段、索引段、回滚段等，在InnoDB存储引擎中，对段的管理都是由引擎自身所完成的。

区

区就是上图的extent区域，区是由连续的页组成的空间，无论页的大小怎么变，区的大小默认总是为1MB。为了保证区中的页的连续性，InnoDB存储引擎一次从磁盘申请4-5个区，InnoDB页的大小默认为16kb，即一个区一共有64（1MB/16kb=16）个连续的页。

每个段开始，先用32页（page）大小的碎片页来存放数据，在使用完这些页之后才是64个连续页的申请。这样做的目的是，对于一些小表或者是undo类的段，可以开始申请较小的空间，节约磁盘开销。

页

页就是上图的page区域，也可以叫块。页是InnoDB磁盘管理的最小单位。默认大小为16KB，可以通过参数innodb_page_size来设置。

常见的页类型有：数据页，undo页，系统页，事务数据页，插入缓冲位图页，插入缓冲空闲列表页，未压缩的二进制大对象页，压缩的二进制大对象页等。

分区概述

分区

这里讲的分区，此“区”非彼“区”，这里讲的分区的意思是指将同一表中不同行的记录分配到不同的物理文件中，几个分区就有几个.idb文件，不是我们刚刚说的区。

MySQL在5.1时添加了对水平分区的支持。分区是将一个表或索引分解成多个更小，更可管理的部分。每个区都是独立的，可以独立处理，也可以作为一个更大对象的一部分进行处理。这个是MySQL支持的功能，业务代码无需改动。要知道MySQL是面向OLTP的数据，它不像TIDB等其他DB。那么对于分区的使用应该非常小心，如果不清楚如何使用分区可能会对性能产生负面的影响。

MySQL数据库的分区是局部分区索引，一个分区中既存了数据，又放了索引。也就是说，每个区的聚集索引和非聚集索引都放在各自区的（不同的物理文件）。目前MySQL数据库还不支持全局分区。

无论哪种类型的分区，如果表中存在主键或唯一索引时，分区列必须是唯一索引的一个组成部分。

分区类型

目前MySQL支持一下几种类型的分区，RANGE分区，LIST分区，HASH分区，KEY分区。如果表存在主键或者唯一索引时，分区列必须是唯一索引的一个组成部分。实战十有八九都是用RANGE分区。

RANGE分区

RANGE分区是实战最常用的一种分区类型，行数据基于属于一个给定的连续区间的列值被放入分区。但是记住，当插入的数据不在一个分区中定义的值的时候，会抛异常。

RANGE分区主要用于日期列的分区，比如交易表啊，销售表啊等。可以根据年月来存放数据。如果你分区走的唯一索引中date类型的数据，那么注意了，优化器只能对YEAR(),TO_DAYS(),TO_SECONDS(),UNIX_TIMESTAMP()这类函数进行优化选择。实战中可以用int类型，那么只用存yyyyMM就好了。也不用关心函数了。

LIST分区

LIST分区和RANGE分区很相似，只是分区列的值是离散的，不是连续的。LIST分区使用VALUES IN，因为每个分区的值是离散的，因此只能定义值。

HASH分区

说到哈希，那么目的很明显了，将数据均匀的分布到预先定义的各个分区中，保证每个分区的数量大致相同。

KEY分区

KEY分区和HASH分区相似，不同之处在于HASH分区使用用户定义的函数进行分区，KEY分区使用数据库提供的函数进行分区。

分区和性能

一项技术，不是用了就一定带来益处。比如显式锁功能比内置锁强大，你没玩好可能导致很不好的情况。分区也是一样，不是启动了分区数据库就会运行的更快，分区可能会给某些sql语句性能提高，但是分区主要用于数据库高可用性的管理。

数据库应用分为2类，一类是OLTP（在线事务处理），一类是OLAP（在线分析处理）。对于OLAP应用分区的确可以很好的提高查询性能，因为一般分析都需要返回大量的数据，如果按时间分区，比如一个月用户行为等数据，则只需扫描响应的分区即可。

在OLTP应用中，分区更加要小心，通常不会获取一张大表的10%的数据，大部分是通过索引返回几条数据即可。

比如一张表1000w数据量，如果一句select语句走辅助索引，但是没有走分区键。那么结果会很尴尬。如果1000w的B+树的高度是3，现在有10个分区。那么不是要`(3+3)*`10次的逻辑IO？（3次聚集索引，3次辅助索引，10个分区）。所以在OLTP应用中请小心使用分区表。

在日常开发中，如果想查看sql语句的分区查询结果可以使用explain partitions + select sql来获取，partitions标识走了哪几个分区。

### 6.说说有哪些分库分表的思路和技巧？

分表

分表，最直白的意思，就是将一个表结构分为多个表，然后，可以在同一个库里，也可以放到不同的库。当然，首先要知道什么情况下，才需要分表。个人觉得单表记录条数达到百万到千万级别时就要使用分表了。

分表的分类

1.纵向分表 将本来可以在同一个表的内容，人为划分为多个表。（所谓的本来，是指按照关系型数据库的第三范式要求，是应该在同一个表的。） 分表技巧： 根据数据的活跃度进行分离，（因为不同活跃的数据，处理方式是不同的） 案例： 对于一个博客系统，文章标题，作者，分类，创建时间等，是变化频率慢，查询次数多，而且最好有很好的实时性的数据，我们把它叫做冷数据。而博客的浏览量，回复数等，类似的统计信息，或者别的变化频率比较高的数据，我们把它叫做活跃数据。所以，在进行数据库结构设计的时候，就应该考虑分表，首先是纵向分表的处理。 这样纵向分表后： （1）首先，存储引擎的使用不同，冷数据使用MyIsam 可以有更好的查询数据。活跃数据，可以使用Innodb ,可以有更好的更新速度。 （2）其次，对冷数据进行更多的从库配置，因为更多的操作是查询，这样来加快查询速度。对热数据，可以相对有更多的主库的横向分表处理。 其实，对于一些特殊的活跃数据，也可以考虑使用memcache ,redis之类的缓存，等累计到一定量再去更新数据库。或者mongodb 一类的nosql 数据库，这里只是举例，就先不说这个。 2.横向分表 字面意思，就可以看出来，是把大的表结构，横向切割为同样结构的不同表，如，用户信息表，user_1,user_2 等。表结构是完全一样，但是，根据某些特定的规则来划分的表，如根据用户ID来取模划分。 分表技巧： 根据数据量的规模来划分，保证单表的容量不会太大，从而来保证单表的查询等处理能力。 案例： 同上面的例子，博客系统。当博客的量达到很大的时候，就应该采取横向分割来降低每个单表的压力，来提升性能。例如博客的冷数据表，假如分为100个表，当同时有100万个用户在浏览时，如果是单表的话，会进行100万次请求，而现在分表后，就可能是每个表进行1万个数据的请求（因为，不可能绝对的平均，只是假设），这样压力就降低了很多。 注意：数据库的复制能解决访问问题，并不能解决大规模的并发写入问题，要解决这个问题就要考虑MySQL数据切分了。

数据切分

顾名思义，就是数据分散，将一台主机上的数据分摊到多台，减轻单台主机的负载压力，有两种切分方式，一种是分库，即按照业务模块分多个库，每个库中的表不一样，还有一种就是分表，按照一定的业务规则或者逻辑将数据拆分到不同的主机上，每个主机上的表是一样的，这个有点类似于Oracle的表分区。

分区

分库又叫垂直分区，这种方式实现起来比较简单，重要的是对业务要细化，分库时候要想清楚各个模块业务之间的交互情况，避免将来写程序时出现过多的跨库读写操作。 分表又叫水平分区，这种方式实现起来就比垂直分区复杂些，但是它能解决垂直分区所不能解决的问题，即单张表的访问及写入很频繁，这时候就可以根据一定的业务规则（PS：如互联网BBS论坛的会员等级概念，根据会员等级来分表）来分表，这样就能减轻单表压力，并且还能解决各个模块的之间的频繁交互问题。 分库的优点是： 实现简单，库与库之间界限分明，便于维护，缺点是不利于频繁跨库操作，不能解决单表数据量大的问题。 分表的优点是： 能解决分库的不足点，但是缺点却恰恰是分库的优点，分表实现起来比较复杂，特别是分表规则的划分，程序的编写，以及后期的数据库拆分移植维护。

## 操作数据语句

### 1.INSERT语句

基本：INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), …]

如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。

优化策略：

（1）当我们需要批量插入数据的时候，这样的语句却会出现性能问题。例如说，如果有需要插入100000条数据，那么就需要有100000条insert语句，每一句都需要提交到关系引擎去解析，优化，然后才能够到达存储引擎做真的插入工作。上述所说的同时插入多条就是一种优化。（经测试，大概10条同时插入是最高效的）

（2）将进程/线程数控制在2倍于CPU数目相对合适 （3）采用顺序主键策略（例如自增主键，或者修改业务逻辑，让插入的记录尽可能顺序主键） （4）考虑使用replace 语句代替insert语句。

### 2.DELETE语句

DELETE FROM 表名[ 删除条件子句]（没有条件子句，则会删除全部）

补充：Mysql中的truncate table和delete语句都可以删除表里面所有数据，但是在一些情况下有些不同！

（1）truncate table删除速度更快，，但truncate table删除后不记录mysql日志，不可以恢复数据。（谨慎使用） （2）如果没有外键关联，innodb执行truncate是先drop table(原始表),再创建一个跟原始表一样空表,速度要远远快于delete逐条删除行记录。（思考：删除百万级数据的时候是否可用truncate table） （3）如果使用innodb_file_per_table参数，truncate table 能重新利用释放的硬盘空间,在InnoDBPlugin中，truncate table为自动回收，如果不是用InnoDB Plugin,那么需要使用optimize table来优化表，释放空间。

truncate table删除表后，optimize table尤其重要，特别是大数据数据库，表空间可以得到释放！ （4）表有外键关联，truncate table删除表数据为逐行删除，如果外键指定级联删除(deletecascade)，关联的子表也会会被删除所有表数据。如果外键未指定级联(cascde),truncate table逐行删除数据，如果是父行关联子表行数据，将会报错。

注意： 一个大的 DELETE 或 INSERT 操作，要非常小心，因为这两个操作是会锁表的，表一锁住，其他操作就进不来了。因此，我们要交给DBA去拆分，重整数据库策略，比如限制处理1000条。

由于索引需要额外的维护成本；因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟），然后删除其中无用数据，此过程需要不到两分钟，删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

### 3.UPDATE语句

UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件]

(1). 尽量不要修改主键字段。 (2). 当修改VARCHAR型字段时，尽量使用相同长度内容的值代替。 (3). 尽量最小化对于含有UPDATE触发器的表的UPDATE操作。 (4). 避免UPDATE将要复制到其他数据库的列。 (5). 避免UPDATE建有很多索引的列。 (6). 避免UPDATE在WHERE子句条件中的列。

### 4.REPLACE语句

根据应用情况可以使用replace 语句代替insert/update语句。例如：如果一个表在一个字段上建立了唯一索引，当向这个表中使用已经存在的键值插入一条记录，将会抛出一个主键冲突的错误。如果我们想用新记录的值来覆盖原来的记录值时，就可以使用REPLACE语句。

使用REPLACE插入记录时，如果记录不重复（或往表里插新记录），REPLACE功能与INSERT一样，如果存在重复记录，REPLACE就使用新记录的值来替换原来的记录值。使用REPLACE的最大好处就是可以将DELETE和INSERT合二为一，形成一个原子操作。这样就可以不必考虑同时使用DELETE和INSERT时添加事务等复杂操作了。

在使用REPLACE时，表中必须有唯一有一个PRIMARY KEY或UNIQUE索引，否则，使用一个REPLACE语句没有意义。

UPDATE和REPLACE的区别： 1）UPDATE在没有匹配记录时什么都不做，而REPLACE在有重复记录时更新，在没有重复记录时插入。 2）UPDATE可以选择性地更新记录的一部分字段。而REPLACE在发现有重复记录时就将这条记录彻底删除，再插入新的记录。也就是说，将所有的字段都更新了。

### 5.UNION

UNION是会把结果排序的！！！ union查询：它可以把需要使用临时表的两条或更多的select查询合并的一个查询中（即把两次或多次查询结果合并起来。）。在客户端的查询会话结束的时候，临时表会被自动删除，从而保证数据库整齐、高效。使用union来创建查询的时候，我们只需要用UNION作为关键字把多个select语句连接起来就可以了，要注意的是所有select语句中的字段数目要想同。 要求：两次查询的列数必须一致（列的类型可以不一样，但推荐查询的每一列，相对应的类型要一样） 可以来自多张表的数据：多次sql语句取出的列名可以不一致，此时以第一个sql语句的列名为准。 如果不同的语句中取出的行，有完全相同(这里表示的是每个列的值都相同)，那么union会将相同的行合并，最终只保留一行。也可以这样理解，union会去掉重复的行。 如果不想去掉重复的行，可以使用union all。 如果子句中有order by,limit，需用括号()包起来。推荐放到所有子句之后，即对最终合并的结果来排序或筛选。

注意： 1、UNION 结果集中的列名总是等于第一个 SELECT 语句中的列名 2、UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条SELECT 语句中的列的顺序必须相同 UNION ALL的作用和语法： 默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。当 ALL 随 UNION 一起使用时（即 UNION ALL），不消除重复行。

### 6.JOIN

（1）对于要求全面的结果时，我们需要使用连接操作（LEFT JOIN / RIGHT JOIN / FULL JOIN）； （2）应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描

（3）in 和 not in 也要慎用，否则会导致全表扫描

很多时候用 exists 代替 in 是一个好的选择

（4）尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连 接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。

（5）尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 （6）不要以为使用MySQL的一些连接操作对查询有多么大的改善，其实核心是索引

### 7.DISTINCT

排除重复的行

1. 单独的 DISTINCT 关键字只能放在开头，放在其它位置会报错
2. 但如果是配合其它的函数使用，比如 COUNT() 则可以任意位置

DISTINCT 中的小括号 ()

「 SQL 解析器会忽略 DISTINCT 关键字后面的小括号，而把 DISTINCT 关键字后面的所有列都作为唯一条件 」

基本用法

1、单独获取某一列不重复的值

这种情况下，有无小括号的结果是一样的

2、单独获取某一列不重复值的数量

3、以多列作为条件获取不同的值 一定要记住，当你使用多列时，并不仅仅时使用小括号内的列，而是全部列

如果你真需要这样的结果，有两种方法 1、附带 SQL WHERE 子句 子句

2、使用 GROUP BY 子句

## 日志

### 1.日志

a、错误日志：记录启动、运行或停止mysqld时出现的问题。 b、通用日志：记录建立的客户端连接和执行的语句。 c、更新日志：记录更改数据的语句。该日志在MySQL 5.1中已不再使用。 d、二进制日志：记录所有更改数据的语句。还用于复制。 e、慢查询日志：记录所有执行时间超过long_query_time秒的所有查询或不使用索引的查询。 f、Innodb日志：innodb redo log

缺省情况下，所有日志创建于mysqld数据目录中。 可以通过刷新日志，来强制mysqld来关闭和重新打开日志文件（或者在某些情况下切换到一个新的日志）。 当你执行一个FLUSH LOGS语句或执行mysqladmin flush-logs或mysqladmin refresh时，则日志被老化。 对于存在MySQL复制的情形下，从复制服务器将维护更多日志文件，被称为接替日志。

### 2.慢查询日志

何谓慢查询日志？MySQL会记录下查询超过指定时间的语句，我们将超过指定时间的SQL语句查询称为慢查询，都记在慢查询日志里，我们开启后可以查看究竟是哪些语句在慢查询。

开启慢查询日志

mysql>show variables like “%slow%”; 查看慢查询配置，没有则在my.cnf中添加

### 3.**innodb的**日志

（1） 有多少种日志

redo/undo

（2） 日志的存放形式

redo：在页修改的时候，先写到redo log buffer 里面，然后写到redo log 的文件系统缓存里面(fwrite)，然后再同步到磁盘文件（fsync）。

Undo：在MySQL5.5之前，undo只能存放在ibdata*文件里面，5.6之后，可以通过设置innodb_undo_tablespaces参数把undo log存放在ibdata*之外。

（3） 事务是如何通过日志来实现的，说得越深入越好。

基本流程如下：

因为事务在修改页时，要先记undo，在记undo之前要记undo的redo，然后修改数据页，再记数据页修改的redo。Redo（里面包括undo的修改）一定要比数据页先持久化到磁盘。当事务需要回滚时，因为有undo，可以把数据页回滚到前镜像的状态，崩溃恢复时，如果redo log中事务没有对应的commit记录，那么需要用undo把该事务的修改回滚到事务开始之前。如果有commit记录，就用redo前滚到该事务完成时并提交掉。



## 面试题

### 1.为什么Mysql的常用引擎都默认使用B+树作为索引？

索引的本质

在一次查询中，磁盘io占用了大部分的时间。更进一步地说，一次查询的效率取绝于磁盘io的次数，如果我们能够在一次查询中尽可能地降低磁盘io的次数，那么我们就能加快查询的速度。

二叉树

最普通的二叉树的问题在于他不能保证O(logN)的查询时间复杂度，由于插入的元素逐渐增大，元素始终在右边进行插入，好好的一棵二叉树最终变成了一条“链表”。在这种极端的情况下，二叉树的查询时间复杂度不再是O(logN)，而是退化为O(N)，这样显然不符合索引的要求。

平衡二叉树（红黑树）

像红黑树这样的平衡二叉树，无论如何插入元素，他都可以通过一些旋转的方法调整树的高度，使得整棵树的查询效率维持在O(logN)，这么来说他已经符合了成为索引的必备条件，但是最终没有选择他作为索引说明还有不足的地方。仔细看看可以发现平衡二叉树的每个节点只有两个孩子节点，如果一张表的数据量特别大，整棵树的高度也会随之上升。一个千万级别的表如果用平衡二叉树作为索引的话，树高将会达到二十多层。这也就意味着做一次查询需要二十多次磁盘io，这是一个不小的开销。

B树和B+树

平衡二叉树的瓶颈在于一个节点只有两个孩子节点，而B树一个节点可以存放N个孩子节点，这就完美解决了树高的问题，我们可以把B树称为平衡多叉树。

B+树在B树的基础上作了哪些改进

- B树的节点中既存储索引，也存储表对应的数据；而B+树的非叶子节点是不存储数据的，只存储索引，数据全部存储在叶子节点上。在B树的情况下，由于非叶子节点使用了大量空间存储数据，存放的索引指针肯定就少，最终整棵树如果想要存储和B+树一样多的数据就必须要增加树高，这样一来就增加了磁盘io，所以说B+树作为索引的性能比B树高。
- 叶子节点之间使用指针连接，提高区间访问效率。如果我们要进行范围查询，可以轻松通过B+树叶子节点之间的指针进行遍历，减少了不必要的磁盘io。

为什么不常用哈希表和数组作为索引

哈希表虽然单一个值的查询效率很高，但是撑不住范围查询，哪个公司的业务还没个范围查询呢？ 而数组虽然查询的效率高，但是增加和删除的效率低，由于记录在增加和删除的时候索引也得跟着维护，这会导致大数据量的情况 下，增加或删除一条记录效率较低。

### 2.如何以最高的效率从MySQL中随机查询一条记录？

```html
SELECT *
FROM `table` AS t1 JOIN (SELECT ROUND(RAND() * ((SELECT MAX(id) FROM
`table`)-(SELECT MIN(id) FROM `table`))+(SELECT MIN(id) FROM `table`)) AS id)
AS t2
WHERE t1.id >= t2.id
ORDER BY t1.id LIMIT 1;
```

### 3.如何实现向MySQL中插入数据时，存在则忽略，不存在就插入？

INSERT语句和ON DUPLICATE KEY UPDATE语句实现

如果指定了ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行UPDATE。例如，如果列a被定义为UNIQUE，并且包含值1，则以下两个语句具有相同的效果：

```html
INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1;
UPDATE table SET c=c+1 WHERE a=1;
```

如果行作为新记录被插入，则受影响行的值为1；如果原有的记录被更新，则受影响行的值为2。

REPLACE语句实现

使用REPLACE的最大好处就是可以将DELETE和INSERT合二为一，形成一个原子操作。这样就可以不必考虑在同时使用DELETE和INSERT时添加事务等复杂操作了。在使用REPLACE时，表中必须有唯一索引，而且这个索引所在的字段不能允许空值，否则REPLACE就和INSERT完全一样的。在执行REPLACE后，系统返回了所影响的行数，如果返回1，说明在表中并没有重复的记录，如果返回2，说明有一条重复记录，系统自动先调用了DELETE删除这条记录，然后再记录用INSERT来插入这条记录。

语法和INSERT非常的相似，如下面的REPLACE语句是插入或更新一条记录。

```html
REPLACE INTO users (id,name,age) VALUES(1, 'binghe', 18);
```

## 一、索引

### B+ Tree 原理

\1. 数据结构

B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。

B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。

在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。

\2. 操作

进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。

插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。

\3. 与红黑树的比较

红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。

（一）B+ 树有更低的树高

平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。

（二）磁盘访问原理

操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。

如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。

（三）磁盘预读特性

为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。

### MySQL 索引

索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。

\1. B+Tree 索引

是大多数 MySQL 存储引擎的默认索引类型。

因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。

因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。

可以指定多个列作为索引列，多个索引列共同组成键。

适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

InnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。

辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。

\2. 哈希索引

哈希索引能以 O(1) 时间进行查找，但是失去了有序性：

- 无法用于排序与分组；
- 只支持精确查找，无法用于部分查找和范围查找。

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

\3. 全文索引

MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。

查找条件使用 MATCH AGAINST，而不是普通的 WHERE。

全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。

InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。

\4. 空间数据索引

MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。

必须使用 GIS 相关的函数来维护数据。

### 索引优化

\1. 独立的列

在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。

例如下面的查询不能使用 actor_id 列的索引：

```html
SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;
```

\2. 多列索引

在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。

```html
SELECT film_id, actor_ id FROM sakila.film_actor
WHERE actor_id = 1 AND film_id = 1;
```

\3. 索引列的顺序

让选择性最强的索引列放在前面。

索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。

例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。

```html
SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,
COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,
COUNT(*)
FROM payment;
   staff_id_selectivity: 0.0001
customer_id_selectivity: 0.0373
               COUNT(*): 16049
```

\4. 前缀索引

对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。

前缀长度的选取需要根据索引选择性来确定。

\5. 覆盖索引

索引包含所有需要查询的字段的值。

具有以下优点：

- 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。
- 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。
- 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。

### 索引的优点

- 大大减少了服务器需要扫描的数据行数。
- 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。
- 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。

### 索引的使用条件

- 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；
- 对于中到大型的表，索引就非常有效；
- 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。

## 二、查询性能优化

### 使用 Explain 进行分析

Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。

比较重要的字段有：

- select_type : 查询类型，有简单查询、联合查询、子查询等
- key : 使用的索引
- rows : 扫描的行数

### 优化数据访问

\1. 减少请求的数据量

- 只返回必要的列：最好不要使用 SELECT * 语句。
- 只返回必要的行：使用 LIMIT 语句来限制返回的数据。
- 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。

\2. 减少服务器端扫描的行数

最有效的方式是使用索引来覆盖查询。

### 重构查询方式

\1. 切分大查询

一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。

```html
DELETE FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);
rows_affected = 0
do {
    rows_affected = do_query(
    "DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")
} while rows_affected > 0
```

\2. 分解大连接查询

将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：

- 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。
- 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。
- 减少锁竞争；
- 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。
- 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。

```html
SELECT * FROM tag
JOIN tag_post ON tag_post.tag_id=tag.id
JOIN post ON tag_post.post_id=post.id
WHERE tag.tag='mysql';
SELECT * FROM tag WHERE tag='mysql';
SELECT * FROM tag_post WHERE tag_id=1234;
SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);
```

## 五、切分

### 水平切分

水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。

当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。

### 垂直切分

垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。

在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。

### Sharding 策略

- 哈希取模：hash(key) % N；
- 范围：可以是 ID 范围也可以是时间范围；
- 映射表：使用单独的一个数据库来存储映射关系。

### Sharding 存在的问题

\1. 事务问题

使用分布式事务来解决，比如 XA 接口。

\2. 连接

可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。

\3. ID 唯一性

- 使用全局唯一 ID（GUID）
- 为每个分片指定一个 ID 范围
- 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)

## 六、复制

### 主从复制

主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

- **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
- **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。
- **SQL 线程** ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。

### 读写分离

主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

## 存储引擎

### 1.存储引擎

数据库的存储引擎是数据库的底层软件组织，数据库管理系统（DBMS）使用存储引擎创建、查询、更新和删除数据。不同的存储引擎提供了不同的存储机制、索引技巧、锁定水平等功能，都有其特定的功能。现在，许多数据库管理系统都支持多种存储引擎，常用的存储引擎主要有MyISAM、InnoDB、Memory、Archive和Federated。

MyIASM

MyIASM是MySQL默认的存储引擎，不支持数据库事务、行级锁和外键，因此在INSERT（插入）或UPDATE（更新）数据即写操作时需要锁定整个表，效率较低。 MyIASM的特点是执行读取操作的速度快，且占用的内存和存储资源较少。它在设计之初就假设数据被组织成固定长度的记录，并且是按顺序存储的。在查找数据时，MyIASM直接查找文件的OFFSET，定位比InnoDB要快（InnoDB寻址时要先映射到块，再映射到行）。 总体来说，MyIASM的缺点是更新数据慢且不支持事务处理，优点是查询速度快。

InnoDB

InnoDB 为MySQL 提供了事务（ Transaction ） 支持、回滚（Rollback）、崩溃修复能力（Crash Recovery Capabilities）、多版本并发控制（Multi-versioned Concurrency Control）、事务安全（Transaction-safe）的操作。InnoDB的底层存储结构为B+树，B+树的每个节点都对应InnoDB的一个Page，Page大小是固定的，一般被设为16KB。其中，非叶子节点只有键值，叶子节点包含完整的数据。 InnoDB适用于有以下需求的场景。

- 经常有数据更新的表，适合处理多重并发更新请求。
- 支持事务。
- 支持灾难恢复（通过bin-log日志等）。
- 支持外键约束，只有InnoDB支持外键。
- 支持自动增加列属性auto_increment。

TokuDB

TokuDB的底层存储结构为Fractal Tree。Fractal Tree的结构与B+树有些类似，只是在Fractal Tree中除了每一个指针（key），都需要指向一个child（孩子）节点，child节点带一个Message Buffer，这个Message Buffer是一个先进先出队列，用来缓存更新操作。这样，每一次插入操作都只需落在某节点的Message Buffer上，就可以马上返回，并不需要搜索到叶子节点。这些缓存的更新操作会在后台异步合并并更新到对应的节点上。 TokuDB在线添加索引，不影响读写操作，有非常高的写入性能，主要适用于要求写入速度快、访问频率不高的数据或历史数据归档。

Memory

Memory表使用内存空间创建。每个Memory表实际上都对应一个磁盘文件用于持久化。Memory表因为数据是存放在内存中的，因此访问速度非常快，通常使用Hash索引来实现数据索引。Memory表的缺点是一旦服务关闭，表中的数据就会丢失。 Memory还支持散列索引和B树索引。B树索引可以使用部分查询和通配查询，也可以使用不等于和大于等于等操作符方便批量数据访问，散列索引相对于B树索引来说，基于Key的查询效率特别高，但是基于范围的查询效率不是很高。

### 2.MyISAM和InnoDB区别

1. **是否支持行级锁** : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。
2. **是否支持事务和崩溃后的安全恢复： MyISAM** 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是**InnoDB** 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。
3. **是否支持外键：** MyISAM不支持，而InnoDB支持。
4. **是否支持MVCC** ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 `READ COMMITTED` 和 `REPEATABLE READ` 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。

## 事务

### 1.事务的四大特性(ACID)

1. **原子性（Atomicity）：** 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. **一致性（Consistency）：** 执行事务后，数据库从一个正确的状态变化到另一个正确的状态；
3. **隔离性（Isolation）：** 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
4. **持久性（Durability）：** 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

### 2.并发事务带来哪些问题?

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。
- **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

**不可重复读和幻读区别：**

不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。

### 3.事务隔离级别有哪些?

**SQL 标准定义了四个隔离级别：**

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

### 4.数据库三范式

范式是具有最小冗余的表结构，三范式的概念如下所述。

1.第一范式

如果每列都是不可再分的最小数据单元（也叫作最小的原子单元），则满足第一范式，第一范式的目标是确保每列的原子性。

2.第二范式

第二范式在第一范式的基础上，规定表中的非主键列不存在对主键的部分依赖，即第二范式要求每个表只描述一件事情。

3.第三范式

第三范式的定义为：满足第一范式和第二范式，并且表中的列不存在对非主键列的传递依赖。

## 锁

### 1.锁机制与InnoDB锁算法

**MyISAM和InnoDB存储引擎使用的锁：**

- MyISAM采用表级锁(table-level locking)。
- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁

**表级锁和行级锁对比：**

- **表级锁：** MySQL中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。
- **行级锁：** MySQL中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

**InnoDB存储引擎的锁的算法有三种：**

- Record lock：单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record+gap 锁定一个范围，包含记录本身

**相关知识点：**

1. innodb对于行的查询使用next-key lock
2. Next-locking keying为了解决Phantom Problem幻读问题
3. 当查询的索引含有唯一属性时，将next-key lock降级为record key
4. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
5. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

### 2.共享锁（s）和排他锁（X）

表级锁和行级锁可以进一步划分为共享锁（s）和排他锁（X）。

共享锁（s）

共享锁（Share Locks，简记为S）又被称为读锁，其他用户可以并发读取数据，但任何事务都不能获取数据上的排他锁，直到已释放所有共享锁。若事务T对数据对象A加上S锁，则事务T只能读A；其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这就保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。

排他锁（X）

排它锁（(Exclusive lock,简记为X锁)）又称为写锁，若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁。它防止任何其它事务获取资源上的锁，直到在事务的末尾将资源上的原始锁释放为止。在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁。

两者之间的区别

共享锁（S锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不 能加排他锁。获取共享锁的事务只能读数据，不能修改数据。

排他锁（X锁）：如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获取排他锁的事务既能读数据，又能修改数据。 另外两个表级锁：IS和IX

当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。

### 3.表锁

首先，从锁的粒度，我们可以分成两大类：

- 表锁
  - 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低
- 行锁
  - 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高

不同的存储引擎支持的锁粒度是不一样的：

- **InnoDB行锁和表锁都支持**！
- **MyISAM只支持表锁**！

InnoDB只有通过**索引条件**检索数据**才使用行级锁**，否则，InnoDB将使用**表锁**

- 也就是说，**InnoDB的行锁是基于索引的**！

**表锁下又分为两种模式**：

- 表读锁（Table Read Lock）
- 表写锁（Table Write Lock）
- 从下图可以清晰看到，在表读锁和表写锁的环境下：读读不阻塞，读写阻塞，写写阻塞！
  - 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁
  - 读写阻塞：当前用户在读数据，其他的用户**不能修改当前用户读的数据**，会加锁！
  - 写写阻塞：当前用户在修改数据，其他的用户**不能修改当前用户正在修改的数据**，会加锁！

**MyISAM可以**支持查询和插入操作的**并发**进行。可以通过系统变量`concurrent_insert`来指定哪种模式，在**MyISAM**中它默认是：如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从**表尾**插入记录。

但是**InnoDB存储引擎是不支持的**！

### 4.行锁

我们使用Mysql一般是使用InnoDB存储引擎的。InnoDB和MyISAM有两个本质的区别：

- InnoDB支持行锁
- InnoDB支持事务

从上面也说了：我们是**很少手动加表锁**的。表锁对我们程序员来说几乎是透明的，即使InnoDB不走索引，加的表锁也是自动的！

我们应该**更加关注行锁的内容**，因为InnoDB一大特性就是支持行锁！

InnoDB实现了以下**两种**类型的行锁。

- 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。
  - 也叫做**读锁**：读锁是**共享**的，多个客户可以**同时读取同一个**资源，但**不允许其他客户修改**。
- 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。
  - 也叫做**写锁**：写锁是排他的，**写锁会阻塞其他的写锁和读锁**。

另外，**为了允许行锁和表锁共存，实现多粒度锁机制**，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是**表锁**：

- 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。
- 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。
- 意向锁也是数据库隐式帮我们做了，**不需要程序员操心**！

### 5.数据库的并发策略

数据库的并发控制一般采用三种方法实现，分别是乐观锁、悲观锁及时间戳。

乐观锁

乐观锁在读数据时，认为别人不会去写其所读的数据；悲观锁就刚好相反，觉得自己读数据时，别人可能刚好在写自己刚读的数据，态度比较保守；时间戳在操作数据时不加锁，而是通过时间戳来控制并发出现的问题。

悲观锁

悲观锁指在其修改某条数据时，不允许别人读取该数据，直到自己的整个事务都提交并释放锁，其他用户才能访问该数据。悲观锁又可分为排它锁（写锁）和共享锁（读锁）。

时间戳

时间戳指在数据库表中额外加一个时间戳列TimeStamp。每次读数据时，都把时间戳也读出来，在更新数据时把时间戳加 1，在提交之前跟数据库的该字段比较一次，如果比数据库的值大，就允许保存，否则不允许保存。这种处理方法虽然不使用数据库系统提供的锁机制，但是可以大大提高数据库处理的并发量。

### 6.数据库锁

1.行级锁

行级锁指对某行数据加锁，是一种排他锁，防止其他事务修改此行。在执行以下数据库操作时，数据库会自动应用行级锁。

- INSERT 、UPDATE 、DELETE 、SELECT … FOR UPDATE [OF columns] [WAIT n|NOWAIT]。
- SELECT … FOR UPDATE语句允许用户一次针对多条记录执行更新。
- 使用COMMIT或ROLLBACK语句释放锁。

2.表级锁

表级锁指对当前操作的整张表加锁，它的实现简单，资源消耗较少，被大部分存储引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。

3.页级锁

页级锁的锁定粒度介于行级锁和表级锁之间。表级锁的加锁速度快，但冲突多，行级冲突少，但加锁速度慢。页级锁在二者之间做了平衡，一次锁定相邻的一组记录。

4.基于Redis的分布式锁

数据库锁是基于单个数据库实现的，在我们的业务跨多个数据库时，就要使用分布式锁来保证数据的一致性。下面介绍使用Redis实现一个分布式锁的流程。Redis实现的分布式锁以Redis setnx命令为中心实现， setnx 是Redis 的写入操作命令， 具体语法为setnx(key val)。在且仅在key不存在时，则插入一个key为val的字符串，返回1；若key存在，则什么都不做，返回0。通过setnx实现分布式锁的思路如下。

- 获取锁：在获取锁时调用setnx，如果返回 0，则该锁正在被别人使用；如果返回1，则成功获取锁。
- 释放锁：在释放锁时，判断锁是否存在，如果存在，则执行Redis的delete操作释放锁。

注意，如果锁并发比较大，则可以设置一个锁的超时时间，在超时时间到后，Redis会自动释放锁给其他线程使用。

## 索引

### 1.为什么要使用索引

通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

可以大大加快数据的检索速度（大大减少检索的数据库），这也是创建索引的最主要的原因。

帮助服务器避免排序和临时表。

将随机IO变为顺序IO。

可以加速表与表之间的连接，特别是在实现数据的参考完整性方面特别有意义。

### 2.索引这么多优点，为什么不对表中的每一列创建一个索引呢？

当对表中的数据增加、删除和修改的时候，索引也需要动态的维护，这样就降低了索引的维护速度。

索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间更大。

创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。

### 3.索引是如何提高查询速度的？

将无序的数据变为相对有序的数据（就像查目录一样）。

很明显的是：**没有用索引**我们是需要**遍历双向链表**来定位对应的页，现在通过**“目录”**就可以很快地定位到对应的页上了！

其实底层结构就是**B+树**，B+树作为树的一种实现，能够让我们**很快地**查找出对应的记录。

### 4.使用索引的注意事项

1. 在经常需要搜索的列上，可以加快搜索的速度。
2. 在经常使用where子句的列上面创建索引，加快条件的判断速度。
3. 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。
4. 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引。
5. 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度。
6. 避免where子句中对字段施加函数，这会造成无法命中索引。
7. 在使用Innodb时使用与业务无关的自增主键作为索引，即使用逻辑主键，而不要使用业务主键。
8. 将打算加索引的列设置为NOT NULL，否则将导致引擎放弃使用索引而进行全表扫描。
9. 删除长期未使用的索引，不用的索引存在会造成不必要的性能损耗，MYSQL5.7可以通过查询sys库的chema_unused_indexes视图来查询哪些索引未被使用。
10. 在使用limit offset查询缓慢时，可以借助索引来提高性能。

### 5.MYSQL索引主要使用的两种数据结构

- 哈希索引：对于哈希索引来说，底层的数据结构就是哈希表，因为在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快，其余大部分场景，建议选择BTree索引。
- BTree索引：mysql的BTree索引使用的是B树中的B+ Tree。但是对于主要的两种存储引擎（MyISAM和InnoDB）的实现方式是不一样的。

### 6.MyISAM和InnoDB实现BTree索引方式的区别

- MyISAM：B+ Tree叶节点的data域存放的是数据记录的地址，在索引检索的时候，首先按照B+ Tree搜索算法搜索索引，如果指定的key存在，则取出其data域的值，然后以data域的值为地址读取相应的数据记录，这被称为“非聚簇索引”。
- InnoDB：其数据文件本身就是索引文件，相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+ Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引，这被称为“聚簇索引（或聚集索引）”，而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。在根据主索引搜索时，直接找到key所在的节点即可取出数据，在根据辅助索引查找时，则需要先取出主键的值，再走一遍索引。因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。

### 7.覆盖索引

- 如果不是聚集索引，叶子节点存储的是主键+列值
- 最终还是要“回表”，也就是要通过主键**再**查找一次。这样就会比较慢
- 覆盖索引就是把要**查询出的列和索引是对应的**，不做回表操作！

比如说：

- 现在我创建了索引`(username,age)`，在查询数据的时候：`select username , age from user where username = 'Java3y' and age = 20`。
- 很明显地知道，我们上边的查询是走索引的，并且，**要查询出的列在叶子节点都存在**！所以，就不用回表了~
- 所以，能使用覆盖索引就尽量使用吧~

### 8.索引的基础知识

Mysql的基本存储结构是**页**(记录都存在页里边)。

- **各个数据页**可以组成一个**双向链表**
- 而每个数据页中的记录又可以组成一个单向链表
  - 每个数据页都会为存储在它里边儿的记录生成一个**页目录**，在通过**主键**查找某条记录的时候可以在页目录中使用**二分法快速定位**到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录
  - 以**其他列**(非主键)作为搜索条件：只能从最小记录开始**依次遍历单链表中的每条记录**。

所以说，如果我们写`select * from user where username = 'Java3y'`这样没有进行任何优化的sql语句，默认会这样做：

- 定位到记录所在的页
  - 需要遍历双向链表，找到所在的页
- 从所在的页内中查找相应的记录
  - 由于不是根据主键查询，只能遍历所在页的单链表了

很明显，在数据量很大的情况下这样查找会**很慢**！

### 9.索引降低增删改的速度

B+树是**平衡树**的一种。

> 平衡树：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。

如果一棵普通的树在**极端**的情况下，是能**退化成链表**的(树的优点就不复存在了)

B+树是平衡树的一种，是不会退化成链表的，树的高度都是相对比较低的(基本符合**矮矮胖胖(均衡)的结构**)【这样一来我们检索的时间复杂度就是O(logn)】！

- B+树是一颗平衡树，如果我们对这颗树增删改的话，那肯定会**破坏它的原有结构**。
- **要维持平衡树，就必须做额外的工作**。正因为这些额外的工作**开销**，导致索引会降低增删改的速度

### 10.聚集和非聚集索引

简单概括：

- 聚集索引就是以**主键**创建的索引
- 非聚集索引就是以**非主键**创建的索引

区别：

- 聚集索引在叶子节点存储的是**表中的数据**
- 非聚集索引在叶子节点存储的是**主键和索引列**
- 使用非聚集索引查询出数据时，**拿到叶子上的主键再去查到想要查找的数据**。(拿到主键再查找这个过程叫做**回表**)

**非聚集索引也叫做二级索引**

非聚集索引在建立的时候也**未必是单列**的，可以多个列来创建索引。

- 此时就涉及到了哪个列会走索引，哪个列不走索引的问题了(最左匹配原则-->后面有说)
- **创建多个单列(非聚集)索引的时候，会生成多个索引树**(所以过多创建索引会占用磁盘空间)

### 11.什么是最左前缀原则？

**最左匹配原则**：

- 索引可以简单如一个列`(a)`，也可以复杂如多个列`(a, b, c, d)`，即**联合索引**。
- 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否**存在（相等）**，遇到范围查询`(>、<、between、like`左匹配)等就**不能进一步匹配**了，后续退化为线性查找。
- 因此，**列的排列顺序决定了可命中索引的列数**。

例子：

- 如有索引`(a, b, c, d)`，查询条件`a = 1 and b = 2 and c > 3 and d = 4`，则会在每个节点依次命中a、b、c，无法命中d。(很简单：索引命中只能是**相等**的情况，不能是范围匹配)

### 12.Mysql如何为表字段添加索引？

1.添加PRIMARY KEY（主键索引）

```html
ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 
```

2.添加UNIQUE(唯一索引)

```html
ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 
```

3.添加INDEX(普通索引)

```html
ALTER TABLE `table_name` ADD INDEX index_name ( `column` )
```

4.添加FULLTEXT(全文索引)

```html
ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 
```

5.添加多列索引

```html
ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` )
```

### 13.索引总结

索引在数据库中是一个**非常**重要的知识点！上面谈的其实就是索引**最基本**的东西，要创建出好的索引要顾及到很多的方面：

- 1，**最左前缀匹配原则**。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询`（>,<,BETWEEN,LIKE）`就停止匹配。
- 3，尽量选择**区分度高的列作为索引**，区分度的公式是 `COUNT(DISTINCT col) / COUNT(*)`。表示字段不重复的比率，比率越大我们扫描的记录数就越少。
- 4，**索引列不能参与计算，尽量保持列“干净”**。比如，`FROM_UNIXTIME(create_time) = '2016-06-06'` 就不能使用索引，原因很简单，**B+树中存储的都是数据表中的字段值**，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： `create_time = UNIX_TIMESTAMP('2016-06-06')`。
- 5，尽可能的**扩展索引**，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。
- 6，单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，**MySQL只能使用一个索引**，会从多个单列索引中选择一个限制最为严格的索引。

### 14.Hash 索引和 B+树索引优劣分析

**Hash 索引定位快**

Hash 索引指的就是 Hash 表，最大的优点就是能够在很短的时间内，根据 Hash 函数定位到数据所在的位置，这是 B+树所不能比的。

**Hash 冲突问题**

知道 HashMap 或 HashTable 的同学，相信都知道它们最大的缺点就是 Hash 冲突了。不过对于数据库来说这还不算最大的缺点。

**Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点。**

试想一种情况:

```html
SELECT * FROM tb1 WHERE id < 500;
```

B+树是有序的，在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。

### 15.索引类型

主键索引(Primary Key)

**数据表的主键列使用的就是主键索引。**

**一张数据表有只能有一个主键，并且主键不能为 null，不能重复。**

**在 mysql 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。**

二级索引(辅助索引)

**二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

**PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。**

1. **唯一索引(Unique Key)** ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
2. **普通索引(Index)** ：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**
3. **前缀索引(Prefix)** ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
4. **全文索引(Full Text)** ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

### 16.聚集索引与非聚集索引

聚集索引

**聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。**

在 Mysql 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

聚集索引的优点

聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

聚集索引的缺点

1. **依赖于有序的数据** ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
2. **更新代价大** ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

非聚集索引

**非聚集索引即索引结构和数据分开存放的索引。**

**二级索引属于非聚集索引。**

> MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。
>
> **非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。**

非聚集索引的优点

**更新代价比聚集索引要小** 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的

非聚集索引的缺点

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. **可能会二次查询(回表)** :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

### 17.非聚集索引一定回表查询吗(覆盖索引)?

**非聚集索引不一定回表查询。**

> 试想一种情况，用户准备使用 SQL 查询用户名，而用户名字段正好建立了索引。

```html
 SELECT name FROM table WHERE name='guang19';
```

> 那么这个索引的 key 本身就是 name，查到对应的 name 直接返回就行了，无需回表查询。

**即使是 MYISAM 也是这样，虽然 MYISAM 的主键索引确实需要回表， 因为它的主键索引的叶子节点存放的是指针。但是如果 SQL 查的就是主键呢?**

```html
SELECT id FROM table WHERE id=1;
```

主键索引本身的 key 就是主键，查到返回就行了。这种情况就称之为覆盖索引了。

### 18.覆盖索引

如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！

**覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。**

> 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。
>
> 再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。

### 19.索引创建原则

单列索引

单列索引即由一列属性组成的索引。

联合索引(多列索引)

联合索引即由多列属性组成索引。

最左前缀原则

假设创建的联合索引由三个字段组成:

```html
ALTER TABLE table ADD INDEX index_name (num,name,age)
```

那么当查询的条件有为:num / (num AND name) / (num AND name AND age)时，索引才生效。所以在创建联合索引时，尽量把查询最频繁的那个字段作为最左(第一个)字段。查询的时候也尽量以这个字段为第一条件。

### 20.索引创建注意点

最左前缀原则

虽然我目前的 Mysql 版本较高，好像不遵守最左前缀原则，索引也会生效。 但是我们仍应遵守最左前缀原则，以免版本更迭带来的麻烦。

选择合适的字段

1.不为 NULL 的字段

索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。

2.被频繁查询的字段

我们创建索引的字段应该是查询操作非常频繁的字段。

3.被作为条件查询的字段

被作为 WHERE 条件查询的字段，应该被考虑建立索引。

4.被经常频繁用于连接的字段

经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。

不合适创建索引的字段

1.被频繁更新的字段应该慎重建立索引

虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。

2.不被经常查询的字段没有必要建立索引

3.尽可能的考虑建立联合索引而不是单列索引

因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。

4.注意避免冗余索引

冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。

5.考虑在字符串类型的字段上使用前缀索引代替普通索引

前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。

使用索引一定能提高查询性能吗?

大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。

### 21.创建索引的原则

创建索引是我们提高数据库查询数据效率最常用的办法，也是很重要的办法。下面是常见的创建索引的原则。

- 选择唯一性索引：唯一性索引一般基于Hash算法实现，可以快速、唯一地定位某条数据。
- 为经常需要排序、分组和联合操作的字段建立索引。
- 为常作为查询条件的字段建立索引。
- 限制索引的数量：索引越多，数据更新表越慢，因为在数据更新时会不断计算和添加索引。
- 尽量使用数据量少的索引：如果索引的值很长，则占用的磁盘变大，查询速度会受到影响。
- 尽量使用前缀来索引：如果索引字段的值过长，则不但影响索引的大小，而且会降低索引的执行效率，这时需要使用字段的部分前缀来作为索引。
- 删除不再使用或者很少使用的索引。
- 尽量选择区分度高的列作为索引：区分度表示字段值不重复的比例。
- 索引列不能参与计算：带函数的查询不建议参与索引。
- 尽量扩展现有索引：联合索引的查询效率比多个独立索引高。

## 优化

### 1.MySQL大表优化方案

单表优化

除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在`千万级`以下，字符串为主的表在`五百万`以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量：

字段

- 尽量使用`TINYINT`、`SMALLINT`、`MEDIUM_INT`作为整数类型而非`INT`，如果非负则加上`UNSIGNED`
- `VARCHAR`的长度只分配真正需要的空间
- 使用枚举或整数代替字符串类型
- 尽量使用`TIMESTAMP`而非`DATETIME`，
- 单表不要有太多字段，建议在20以内
- 避免使用NULL字段，很难查询优化且占用额外索引空间
- 用整型来存IP

索引

- 索引并不是越多越好，要根据查询有针对性的创建，考虑在`WHERE`和`ORDER BY`命令上涉及的列建立索引，可根据`EXPLAIN`来查看是否用了索引还是全表扫描
- 应尽量避免在`WHERE`子句中对字段进行`NULL`值判断，否则将导致引擎放弃使用索引而进行全表扫描
- 值分布很稀少的字段不适合建索引，例如"性别"这种只有两三个值的字段
- 字符字段只建前缀索引
- 字符字段最好不要做主键
- 不用外键，由程序保证约束
- 尽量不用`UNIQUE`，由程序保证约束
- 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引

查询SQL

- 可通过开启慢查询日志来找出较慢的SQL
- 不做列运算：`SELECT id WHERE age + 1 = 10`，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边
- sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库
- 不用`SELECT *`
- `OR`改写成`IN`：`OR`的效率是n级别，`IN`的效率是log(n)级别，in的个数建议控制在200以内
- 不用函数和触发器，在应用程序实现
- 避免`%xxx`式查询
- 少用`JOIN`
- 使用同类型进行比较，比如用`'123'`和`'123'`比，`123`和`123`比
- 尽量避免在`WHERE`子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
- 对于连续数值，使用`BETWEEN`不用`IN`：`SELECT id FROM t WHERE num BETWEEN 1 AND 5`
- 列表数据不要拿全表，要使用`LIMIT`来分页，每页数量也不要太大

引擎

目前广泛使用的是MyISAM和InnoDB两种引擎：

MyISAM

MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：

- 不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁
- 不支持事务
- 不支持外键
- 不支持崩溃后的安全恢复
- 在表有读取查询的同时，支持往表中插入新纪录
- 支持`BLOB`和`TEXT`的前500个字符索引，支持全文索引
- 支持延迟更新索引，极大提升写入性能
- 对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用

InnoDB

InnoDB在MySQL 5.5后成为默认索引，它的特点是：

- 支持行锁，采用MVCC来支持高并发
- 支持事务
- 支持外键
- 支持崩溃后的安全恢复
- 不支持全文索引

总体来讲，MyISAM适合`SELECT`密集型的表，而InnoDB适合`INSERT`和`UPDATE`密集型的表

系统调优参数

可以使用下面几个工具来做基准测试：

- [sysbench](https://github.com/akopytov/sysbench)：一个模块化，跨平台以及多线程的性能测试工具
- [iibench-mysql](https://github.com/tmcallaghan/iibench-mysql)：基于 Java 的 MySQL/Percona/MariaDB 索引进行插入性能测试工具
- [tpcc-mysql](https://github.com/Percona-Lab/tpcc-mysql)：Percona开发的TPC-C测试工具

具体的调优参数内容较多，具体可参考官方文档，这里介绍一些比较重要的参数：

- back_log：back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。可以从默认的50升至500
- wait_timeout：数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时
- max_user_connection: 最大连接数，默认为0无上限，最好设一个合理上限
- thread_concurrency：并发线程数，设为CPU核数的两倍
- skip_name_resolve：禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问
- key_buffer_size：索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询`show status like 'key_read%'`，保证`key_reads / key_read_requests`在0.1%以下最好
- innodb_buffer_pool_size：缓存数据块和索引块，对InnoDB表性能影响最大。通过查询`show status like 'Innodb_buffer_pool_read%'`，保证`(Innodb_buffer_pool_read_requests – Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests`越高越好
- innodb_additional_mem_pool_size：InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小
- innodb_log_buffer_size：InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB
- query_cache_size：缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失。根据命中率`(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))`进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令`show status like 'Qcache_%'`查看目前系统Query catch使用大小
- read_buffer_size：MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能
- sort_buffer_size：MySql执行排序使用的缓冲大小。如果想要增加`ORDER BY`的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sort_buffer_size变量的大小
- read_rnd_buffer_size：MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。
- record_buffer：每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值
- thread_cache_size：保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的
- table_cache：类似于thread_cache_size，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM

升级硬件

Scale up，这个不多说了，根据MySQL是CPU密集型还是I/O密集型，通过提升CPU和内存、使用SSD，都能显著提升MySQL性能

读写分离

也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离

缓存

缓存可以发生在这些层次：

- MySQL内部：在系统调优参数介绍了相关设置
- 数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象`Persistence Object`
- 应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象`Data Transfer Object`
- Web层：针对web页面做缓存
- 浏览器客户端：用户端的缓存

可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式：

- 直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。
- 回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。

表分区

MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码

对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引

用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过`EXPLAIN PARTITIONS`来查看某条SQL语句会落在那些分区上，从而进行SQL优化

分区的好处是：

- 可以让单表存储更多的数据
- 分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作
- 部分查询能够从查询条件确定只落在少数分区上，速度会很快
- 分区表的数据还可以分布在不同的物理设备上，从而搞笑利用多个硬件设备
- 可以使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争
- 可以备份和恢复单个分区

分区的限制和缺点：

- 一个表最多只能有1024个分区
- 如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来
- 分区表无法使用外键约束
- NULL值会使分区过滤无效
- 所有分区必须使用相同的存储引擎

分区的类型：

- RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区
- LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择
- HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式
- KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值

分区适合的场景有：

- 最适合的场景数据的时间序列性比较强，则可以按时间来分区

查询时加上时间范围条件效率会非常高，同时对于不需要的历史数据能很容的批量删除。

- 如果数据有明显的热点，而且除了这部分数据，其他数据很少被访问到，那么可以将热点数据单独放在一个分区，让这个分区的数据能够有机会都缓存在内存中，查询时只访问一个很小的分区表，能够有效使用索引和缓存

另外MySQL有一种早期的简单的分区实现 - 合并表（merge table），限制较多且缺乏优化，不建议使用，应该用新的分区机制来替代

垂直拆分

垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联

垂直拆分的优点是：

- 可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少)
- 可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起
- 数据维护简单

缺点是：

- 主键出现冗余，需要管理冗余列
- 会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力
- 依然存在单表数据量过大的问题（需要水平拆分）
- 事务处理复杂

水平拆分

概述

水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。前面的表分区本质上也是一种特殊的库内分表

库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决

水平拆分的优点是:

- 不存在单库大数据和高并发的性能瓶颈
- 应用端改造较少
- 提高了系统的稳定性和负载能力

缺点是：

- 分片事务一致性难以解决
- 跨节点Join性能差，逻辑复杂
- 数据多次扩展难度跟维护量极大

分片原则

- 能不分就不分，参考单表优化
- 分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量
- 分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容
- 尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题
- 查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。
- 通过数据冗余和表分区赖降低跨库Join的可能

这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。

总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。

解决方案

由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。

客户端架构

通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现

客户端架构的优点是：

- 应用直连数据库，降低外围系统依赖所带来的宕机风险
- 集成成本低，无需额外运维的组件

缺点是：

- 限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心
- 将分片逻辑的压力放在应用服务器上，造成额外风险

代理架构

通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件

代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理

代理架构的优点是：

- 能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强
- 对于应用服务器透明且没有增加任何额外负载

缺点是：

- 需部署和运维独立的代理中间件，成本高
- 应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险

### 2.MySQL高性能优化规范建议

数据库命令规范

- 所有数据库对象名称必须使用小写字母并用下划线分割
- 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来）
- 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符
- 临时库表必须以 tmp*为前缀并以日期为后缀，备份表必须以 bak*为前缀并以日期 (时间戳) 为后缀
- 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）

数据库基本设计规范

\1. 所有表必须使用 Innodb 存储引擎

没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。

Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。

\2. 数据库和表的字符集统一使用 UTF8

兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。

\3. 所有表和字段都需要添加注释

使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护

\4. 尽量控制单表数据量的大小,建议控制在 500 万以内。

500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。

可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小

\5. 谨慎使用 MySQL 分区表

分区表在物理上表现为多个文件，在逻辑上表现为一个表；

谨慎选择分区键，跨分区查询效率可能更低；

建议采用物理分表的方式管理大数据。

6.尽量做到冷热数据分离,减小表的宽度

> MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。

减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）；

更有效的利用缓存，避免读入无用的冷数据；

经常一起使用的列放到一个表中（避免更多的关联操作）。

\7. 禁止在表中建立预留字段

预留字段的命名很难做到见名识义。

预留字段无法确认存储的数据类型，所以无法选择合适的类型。

对预留字段类型的修改，会对表进行锁定。

\8. 禁止在数据库中存储图片,文件等大的二进制数据

通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。

通常存储于文件服务器，数据库只存储文件地址信息

\9. 禁止在线上做数据库压力测试

\10. 禁止从开发环境,测试环境直接连接生成环境数据库

数据库字段设计规范

\1. 优先选择符合存储需要的最小的数据类型

**原因：**

列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。

**方法：**

**a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据**

MySQL 提供了两个方法来处理 ip 地址

- inet_aton 把 ip 转为无符号整型 (4-8 位)
- inet_ntoa 把整型的 ip 转为地址

插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。

**b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储**

**原因：**

无符号相对于有符号可以多出一倍的存储空间

```html
SIGNED INT -2147483648~2147483647
UNSIGNED INT 0~4294967295
```

VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。**过大的长度会消耗更多的内存。**

\2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据

**a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中**

MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。

如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。

**2、TEXT 或 BLOB 类型只能使用前缀索引**

因为MySQL[1] 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的

\3. 避免使用 ENUM 类型

修改 ENUM 值需要使用 ALTER 语句

ENUM 类型的 ORDER BY 操作效率低，需要额外操作

禁止使用数值作为 ENUM 的枚举值

\4. 尽可能把所有列定义为 NOT NULL

**原因：**

索引 NULL 列需要额外的空间来保存，所以要占用更多的空间

进行比较和计算时要对 NULL 值做特别的处理

\5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间

TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07

TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高

超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储

**经常会有人用字符串存储日期型的数据（不正确的做法）**

•缺点 1：无法用日期函数进行计算和比较•缺点 2：用字符串存储日期要占用更多的空间

\6. 同财务相关的金额类数据必须使用 decimal 类型

•非精准浮点：float,double•精准浮点：decimal

Decimal 类型为精准浮点数，在计算时不会丢失精度

占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节

可用于存储比 bigint 更大的整型数据

索引设计规范

\1. 限制每张表上的索引数量,建议单张表索引不超过 5 个

索引并不是越多越好！索引可以提高效率同样可以降低效率。

索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。

因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。

\2. 禁止给表中的每一列都建立单独的索引

5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。

\3. 每个 Innodb 表必须有个主键

Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。

Innodb 是按照主键索引的顺序来组织表的

- 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引）
- 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长）
- 主键建议使用自增 ID 值

\4. 常见索引列建议

- 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列
- 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段
- 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好
- 多表 join 的关联列

5.如何选择索引列的顺序

建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。

- 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）
- 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）
- 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）

\6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）

- 重复索引示例：primary key(id)、index(id)、unique index(id)
- 冗余索引示例：index(a,b,c)、index(a,b)、index(a)

\7. 对于频繁的查询优先考虑使用覆盖索引

> 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引

**覆盖索引的好处：**

- **避免 Innodb 表进行索引的二次查询:** Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。
- **可以把随机 IO 变成顺序 IO 加快查询效率:** 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。

8.索引 SET 规范

**尽量避免使用外键约束**

- 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引
- 外键可用于保证数据的参照完整性，但建议在业务端实现
- 外键会影响父表和子表的写操作从而降低性能

数据库 SQL 开发规范

\1. 建议使用预编译语句进行数据库操作

预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。

只传参数，比传递 SQL 语句更高效。

相同语句可以一次解析，多次使用，提高处理效率。

\2. 避免数据类型的隐式转换

隐式转换会导致索引失效如:

```html
select name,phone from customer where id = '111';
```

\3. 充分利用表上已经存在的索引

避免使用双%号的查询条件。如：`a like '%123%'`，（如果无前置%,只有后置%，是可以用到列上的索引的）

一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。

在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。

\4. 数据库设计时，应该要对以后扩展进行考虑

\5. 程序连接不同的数据库使用不同的账号，进制跨库查询

- 为数据库迁移和分库分表留出余地
- 降低业务耦合度
- 避免权限过大而产生的安全风险

\6. 禁止使用 SELECT * 必须使用 SELECT <字段列表> 查询

**原因：**

- 消耗更多的 CPU 和 IO 以网络带宽资源
- 无法使用覆盖索引
- 可减少表结构变更带来的影响

\7. 禁止使用不含字段列表的 INSERT 语句

如：

```html
insert into values ('a','b','c');
```

应使用：

```html
insert into t(c1,c2,c3) values ('a','b','c');
```

\8. 避免使用子查询，可以把子查询优化为 join 操作

通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。

**子查询性能差的原因：**

子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。

由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。

\9. 避免使用 JOIN 关联太多的表

对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。

在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。

如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。

同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。

\10. 减少同数据库的交互次数

数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。

\11. 对应同一列进行 or 判断时，使用 in 代替 or

in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。

\12. 禁止使用 order by rand() 进行随机排序

order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。

推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。

\13. WHERE 从句中禁止对列进行函数转换和计算

对列进行函数转换或计算时会导致无法使用索引

**不推荐：**

```html
where date(create_time)='20190101'
```

**推荐：**

```html
where create_time >= '20190101' and create_time < '20190102'
```

\14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION

- UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作
- UNION ALL 不会再对结果集进行去重操作

\15. 拆分复杂的大 SQL 为多个小 SQL

- 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL
- MySQL 中，一个 SQL 只能使用一个 CPU 进行计算
- SQL 拆分后可以通过并行执行来提高处理效率

数据库操作行为规范

\1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作

**大批量操作可能会造成严重的主从延迟**

主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况

**binlog 日志为 row 格式时会产生大量的日志**

大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因

**避免产生大事务操作**

大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。

特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批

\2. 对于大表使用 pt-online-schema-change 修改表结构

- 避免大表修改产生的主从延迟
- 避免在对表字段进行修改时进行锁表

对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。

pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。

\3. 禁止为程序使用的账号赋予 super 权限

- 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接
- super 权限只能留给 DBA 处理问题的账号使用

\4. 对于程序连接数据库账号,遵循权限最小原则

- 程序使用数据库账号只能在一个 DB 下使用，不准跨库
- 程序使用的账号原则上不准有 drop 权限





# **Netty**

## 1.Netty

Netty是一个高性能、异步事件驱动的NIO框架，它基于JavaNIO提供的API实现，提供了对TCP( Transmission Control Protocol，传输控制协议）、UDP(User Datagram Protocol，用户数据包协议）和文件传输的支持。作为一个异步NIO框架，Netty的所有I/O 操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便地主动获取或者通过通知机制获取I/O操作结果。

Netty是一个高性能的异步事件驱动的网络应用程序框架，主要用于客户端和服务端网络应用程序的快速搭建和开发。基于Netty API可以非常快速、方便地构建一个高性能的TCP或UDP的网络应用，不但极大地简化了网络编程的复杂度，还提高了网络应用程序的性能和稳定性。Netty支持多种网络通信协议，包括TCP、UDP、FTP、HTTP、SMTP等。

## 2.Netty 高性能

在IO编程过程中，当需要同时处理多个客户端接入请求时，可以利用多线程或者IO多路复用技术进行处理。IO多路复用技术通过把多个IO的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源。

与Socket类和ServerSocket类相对应，NIO也提供了SocketChannel和ServerSocketChannel两种不同的套接字通道实现。

### 多路复用通讯方式

Netty的IO线程NioEventLoop由于聚合了多路复用器Selector，可以同时并发处理成百上千个客户端Channel，由于读写操作都是非阻塞的，这就可以充分提升IO线程的运行效率，避免由于频繁IO阻塞导致的线程挂起。

### 异步通讯NIO

由于Netty采用了异步通信模式，一个IO线程可以并发处理N个客户端连接和读写操作，这从根本上解决了传统同步阻塞IO一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升。

### 零拷贝（DIRECT BUFFERS使用堆外直接内存）

Netty的接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。

Netty提供了组合Buffer对象，可以聚合多个ByteBuffer对象，用户可以像操作一个Buffer那样方便的对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer。

Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。

### 内存池（基于内存池的缓冲区重用机制）

随着JVM虚拟机和JIT即时编译技术的发展，对象的分配和回收是个非常轻量级的工作。但是对于缓冲区Buffer，情况却稍有不同，特别是对于堆外直接内存的分配和回收，是一件耗时的操作。为了尽量重用缓冲区，Netty提供了基于内存池的缓冲区重用机制。

## 3.Reactor线程模型

Reactor是一种并发处理客户端请求响应的事件驱动模型。服务端在接收到客户端请求后采用多路复用策略，通过一个非阻塞的线程来异步接收所有的客户端请求，并将这些请求转发到相关的工作线程组上进行处理。

Reactor模型常常基于异步线程的方式实现，常用的Reactor线程模型有3种：Reactor单线程模型、Reactor多线程模型和Reactor主从多线程模型。

Java NIO的实现主要涉及3个核心概念：Selector（选择器）、Channel（通道）和Buffer （缓冲区）。Selector用于监听多个Channel的事件，比如连接打开或数据到达，因此，一个线程可实现对多个Channel的管理。传统的I/O基于数据流进行I/O读写操作，而Java NIO基于Channel和Buffer进行I/O读写操作，并且数据总是从Channel读取到Buffer，或者从Buffer写入Channel。

### Channel

Channel与I/O中的Stream（流）类似，只不过Stream是单向的（例如InputStream、OutputStream ），而Channel是双向的，既可以用来进行读操作，也可以用来进行写操作。

NIO中Channel的主要实现有：FileChannel、DatagramChannel、SocketChannel、ServerSocketChannel。FileChannel对应文件的l/O读写操作，DatagramChannel对应UDP的广播事件，SocketChannel对应Socket客户端的TCP的I/O读写操作，ServerSocketChannel对应Socket服务端的TCP的I/O读写操作。

### Buffer

Buffer实际上是一个容器，其内部通过一个连续的字节数组存储I/O上的数据。在NIO中，Channel在文件、网络上对数据的读取或写入都必须经过Buffer。

客户端在向服务端发送数据时，必须先将数据写入Buffer，然后将Buffer中的数据写入服务端对应的Channel。服务端在接收数据时，必须通过Channel将数据读入Buffer，然后从Buffer读取数据并处理。

在NIO中，Buffer是一个抽象类，对不同的数据类型实现不同的Buffer操作。常用的Buffer实现类有：ByteBuffer、IntBuffer、CharBuffer、LongBuffer、DoubleBuffer、FloatBuffer、ShortBuffer。

### Selector

Selector用于检测在多个注册的Channel上是否有I/O事件发生，并对检查到的I/O事件进行相应的响应和处理。因此，通过一个Selector线程可以实现对多个Channel的管理，而不必为每个连接都创建一个线程，避免线程资源的浪费和多线程之间的上下文切换导致的开销。同时，Selector只在Channel上有读写事件发生时，才会调用I/O函数进行读写操作，可极大地减少系统开销，提高系统的并发量。

Reactor单线程模型

Reactor单线程模型指所有的客户端I/O操作请求都在一个线程（Thread）上完成。

Reactor单线程模型的各模块组成及职责。

( 1 ) Client: NIO客户端，向服务端发起TCP连接，并发送数据。

( 2 ) Acceptor: NIO服务端，通过Acceptor接收客户端的TCP连接。

( 3 ) Dispatcher：接收客户端的数据并将数据以ByteBuffer的形式发送到对应的编解码器。

( 4 ) DecoderHandler：解码器，读取客户端的数据并进行数据解码、数据处理和消息应答。

( 5 ) EncoderHandler：编码器，将给客户端发送的数据（消息请求或消息应答）进行统一的编码处理，并写入通道。

由于Reactor模式使用的是异步非阻塞I/O，因此一个线程可以独立处理多个I/O相关的操作，Reactor单线程模型将所有I/O操作都集中在一个线程中处理。具体处理流程如下。

( 1) Acceptor接收客户端的TCP连接请求消息.

( 2）链路建立成功后通过Dispatcher将接收到的消息写入ByteBuffer，并派发到对应的DecoderHandler上进行消息解码和处理。

( 3 ）消息处理完成后调用对应的EncoderHandler将请求响应进行消息的编码和下发。

Reactor多线程模型

Reactor多线程模型与单线程模型最大的区别就是由一组线程（Thread Poll ）处理客户端的I/O请求操作。Reactor多线程模型将Acceptor的操作封装在一组线程池中，采用线程池的方式监听服务端端口、接收客户端的TCP连接请求、处理网络I/O读写等操线。线程池一般使用标准的JDK线程池，在该线程池中包含一个任务队列和一系列NIO线程，这些NIO线程负责具体的消息读取、解码、编码和发送。

Reactor多线程模型与单线程模型的区别在于，Reactor多线程模型用于接收客户端请求的Acceptor由一个线程来负责，用于处理客户端消息的Dispatcher由一个线程池负责，这样，基于线程池的调度和线程异步执行的能力，能够在同一时间内接收和处理更多的客户端请求。

Reactor主从多线程模型

在Reactor主从多线程模型中，服务端用于接收客户端连接的不再是一个NIO线程，而是一个独立的NIO线程池。主线程Acceptor在接收到客户端的TCP连接请求并建立完成连接后（可能要经过鉴权、登录等过程），将新创建的SocketChannel注册到子I/O线程池（Sub Reactor Pool ）的某个I/O线程上，由它负责具体的SocketChannel的读写和编解码工作。

Reactor主从多线程模型中的Acceptor线程池（Acceptor Thread Pool ）只用于客户端的鉴权、登录、握手和安全认证，一旦链路建立成功，就将链路注册到后端Sub Reactor 线程池的I/O线程上，由I/O线程负责后续的I/O操作。这样就将客户端连接的建立和消息的响应都以异步线程的方式来实现，大大提高了系统的吞吐量。

## 4.Netty的架构设计

Netty优秀的架构设计使其能够简单快速地构建网络应用程序。

Netty的整体架构分为Transport Services （传输服务层）、Protocol Support （传输协议层）和Core（核心层）3部分.

### Transport Services

Transport Services指传输服务层，主要定义了数据的传输和通信方式，包括Socket And Datagram ( Socket协议和数据包）、HTTP Tunnel ( HTTP隧道）、In-VM Pipe （本地传输管道）。

基于Socket的协议有TCP、UDP等。其中，TCP基于报文应答机制实现，主要用于可靠数据的传输，比如移动设备状态信息的传输。UDP发出的数据不需要对方应答，主要用于对数据安全性要求不是很高但是对数据传输吞吐量要求较高的场景，比如实时视频流传输。HTTP Tunnel定义了HTTP的传输方式。In-VM Pipe定义了本地数据的传输方式。

### Protocol Support

Protocol Support指传输协议层，主要定义数据传输过程中的服务类型、数据安全、数据压缩等。具体包括HTTP And WebSocket ( HTTP和WebSocket服务）、SSL And StartTLS ( SSL和StartTLS协议）、zlib/gzip Compression ( zlib/gzip压缩算法）、Large File Transfer （大文件传输）、Google ProtoBuf ( Google ProtoBuf格式）、RTSP（实时流传输协议）、Legacy Text And Binary Protocols （传统TXT和二进制数据）。

HTTP和WebSocket服务定义了客户端和服务端的数据通信方式。HTTP服务基于HTTP实现，主要用于客户端主动向服务端发起数据包请求。WebSocket主要用于服务端基于消息推送的机制实时将数据包推送到客户端。

SSL ( Secure Sockets Layer，安全套接层）主要用于传输层与应用层之间的直接网络数据的加密，是为网络通信提供安全及数据完整性的一种安全协议。SSL包含记录层( Record Layer ）和传输层（Transport Layer ），记录层协议确定传输层数据的封装格式。传输层安全协议使用X.509认证，之后利用非对称加密演算来对通信方进行身份认证，最后交换对称密钥作为此次会话的密钥。基于该会话密钥来实现通信双方数据的加密，保证两个应用程序间通信的保密性和可靠性，使客户端与服务端之间的应用程序之间的通信不被攻击者窃听。

StartTLS是一种明文通信协议的扩展，能够让明文的通信连线直接成为密连线（使用SSL或TLS加密），而不需要使用另一个特别的端口来进行加密通信，属于机会性加密。StartTLS本身是一个与应用层无关的协议，可以搭配许多应用层协议一同使用。

zlib/gzip Compression定义了消息传递过程中数据的压缩和解压缩算法，主要用于提高批量数据传输过程中网络的吞吐量。zlib是一种数据压缩格式，其使用抽象化的DEFLATE算法实现，应用十分广泛，Linux内核中使用zlib以实现网络协议的压缩、文件系统的压缩。OpenSSH、OpenSSL以zlib达到最优化的加密网络传输。gzip的基础也是DEFLATE算法，gzip也是一种数据压缩格式。

Large File Transfer定义了大文件传输过程中数据的拆包和分发策略。

Google ProtoBuf是Google发布的一款开源的数据传输格式和序列化框架。它是一种语言中立、平台无关、可扩展的序列化数据的格式，可用于通信协议、数据存储等。在序列化数据方面，它提供了灵活高效的序列化实现。ProtoBuf很适合用于数据存储或RPC数据交换格式。

RTSP ( Real Time Streaming Protocol）为实时流传输协议，是一种网络应用协议，专为娱乐和通信系统使用，以控制流媒体服务器。该协议用于创建和控制终端之间的媒体会话。媒体服务器的客户端发布VCR命令，例如，播放、录制和暂停，以便实时控制从服务器到客户端（视频点播）或从客户端到服务器（语音录音）的媒体流。

Legacy Text And Binary Protocols提供了传统文本数据格式和二进制数据格式的传输支持。

### Core

Netty的Core（核心层）封装了Netty框架的核心服服和API，具体包括Extensible Event Model（可扩展事件模型）、Universal Communication API （通用通信协议API)、Zero-Copy-Capable Rich Byte Buffer （零拷贝字节缓冲区）等。可扩展事件模型为Netty灵活的事件通信提供了基础；通用通信协议API为上层提供了统一的API访问入口，提高了Netty框架的易用性；零拷贝字节缓冲区为数据的快速读取和写入提供了保障。

## 5.Netty的核心组件

Netty的核心组件包括Bootstrap、ServerBootstrap、NioEventLoop、NioEventLoopGroup、Future、ChannelFuture、Channel、Selector、ChannelHandlerContext、ChannelHandler和ChannelPipeline。

( 1 ) Bootstrap/ServerBootstrap: Bootstrap用于客户端服务的启动引导，ServerBootstrap用于服务端服务的启动引导。

( 2 ) NioEventLoop：基于线程队列的方式执行事件操作，具体要执行的事件操作包括连接注册、端口绑定和I/O数据读写等。每个NioEventLoop线程都负责多个Channel的事件处理。

( 3) NioEventLoopGroup: NioEventLoop生命周期的管理。

( 4 ) Future/ChannelFuture:Future和ChannelFuture用于异步通信的实现，基于异步通信方式可以在I/O操作触发后注册一个监听事件，在I/O操作（数据读写完成或失败）完成后自动触发监听事件并完成后续操作。

( 5 ) Channel: Channel是Netty中的网络通信组件，用于执行具体的I/O操作。Netty中所有的数据通信都基于Channel读取或者将数据写入对应的Channel。Channel的主要功能包括网络连接的建立、连接状态的管理（网络连接的打开和关闭）、网络连接参数的配置（每次接收数据的大小）、基于异步NIO的网络数据操作（数据读取、数据写出）等。

( 6 ) Selector: Selector用于多路复用中Channel的管理。在Netty中，一个Selector可以管理多个Channel，在Channel连接建立后将连接注册到Selector，Selector在内部监听每个Channel上I/O事件的变化，当Channel有网络I/O事件发生时通知ChannelHandler执行具体的I/O操作（读取字节流或字节流写入完成）。

( 7) ChannelHandlerContext: Channel上下文信息的管理。每个ChannelHandler都对应一个ChannelHandlerContext。

( 8 ) ChannelHandler: I/O事件的拦截和处理。其中，ChannelInboundHandler用于处理数据接收的I/O操作，ChannelInboundHandler用于处理数据发送的I/O操作。

( 9 ) ChannelPipeline：基于拦截器设计模式实现的事件拦截处理和转发。Netty中的每个Channel都对应一个ChannelPipeline，在ChannelPipeline中维护了一个由ChannelHandlerContext组成的双向链表，每个ChannelHandlerContext都对应一个ChannelHandler，以完成具体Channel事件的拦截和处理。其中，数据入站由Head向Tail依次传递和处理，数据出站从Tail向Head依次传递和处理。

## 6.Netty的原理

Netty的运行核心包含两个NioEventLoopGroup工作组，一个是BossGroup，用于接收客户端连接、接收客户端数据和进行数据转发；另一个是WorkerGroup，用于具体I/O事件的触发和数据处理。

### Netty Server的初始化步骤

Netty Sever的初始化过程如下。

( 1）初始化BossGroup和WorkerGroup。

( 2 ）基于SeverBootstrap配置EventLoopGroup，包括连接参数设置、Channel类型设置、编解码Handler设置等。

( 3 ）绑定端口和服务启动。

### BossGoup的职责

BossGroup为一个事件循环组，其中包含多个事件循环（NioEventLoop ），每个NioEventLoop都包含一个Selector和一个TaskQueue（事件循环线程）。每个BossNioEventLoop循环都执行以下3个步骤。

( 1 ）轮询监听Accept事件.

( 2 ）接收和处理Accept事件，包括和客户端建立连接并生成NioSocketChannel，将NioSocketChannel注册到某个WorkerNioEventLoop的Selector上。

( 3 ）处理runAllTasks的任务。

### WorkerGroup的职责

WorkerGroup为一个事件循环组，其中包含多个事件循环（NioEventLoop ），每个Work NioEventLoop循环都执行以下3个步骤。

( 1 ）轮询监听NioSocketChannel上的I/O事件（I/O 读写事件）。

( 2 ）当NioSocketChannel有I/O事件触发时执行具体的I/O操作。

( 3 ）处理任务队列中的任务。

BossGroupWorkerGroup工作流程。

## 7.Netty的特性

Netty之所以能在高并发的分布式网络环境下实现对数据的快速处理和分发，得益于其优秀的架构设计。Netty架构设计的主要特性有多路复用模型、数据零拷贝、内存重用机制、无锁化设计和高性能的序列化框架。

### I/O多路复用模型

在高并发的I/O编程模型中，一个服务端往往要同时处理成千上万个客户端请求。传统的I/O模型会为每个Socket链路都建立一个线程专门处理该链路上数据的接收和发送。该方案的特点是，当客户端较少时，服务端的数据处理速度很快，但是当客户端的数量较多时，服务端会出现没有足够的线程资源为每个Socket链路分配一个线程的情况，服务端会出现大量的线程资源争抢和调度，性能急速下降。因此，该方案不适合高并发的场景。

当有大量客户端并发请求时，可以采用I/O多路复用模型处理该问题。I/O多路复用模型将I/O的处理分为Selector和Channel。Selector负责多个Channel的监控和管理，Channel负责真正的I/O读写操作。当Selector监控到Channel上有数据发生变化时会通知对应的Channel，Channel接收到对应的通知后处理对应的数据即可。这样一个Selector线程可以同时处理多个客户端请求。与传统的多线程I/O模型相比，I/O多路复用模型的最大优势是系统开销小，系统不需要为每个客户端连接都建立一个新的线程，也不需要维护这些线程的运行，减少了系统的维护工作量，节省了系统资源。

Netty通过在NioEventLoop（事件轮询机制）内封装Selector来实现I/O的多路复用，在一个NioEventLoop上服务端可以同时并发处理成千上万个客户端请求，同时由于Netty的I/O读写操作都是基于ByteBuffer异步非阻塞的，因此大大提高了I/O线程的运行效率，避免由于频繁I/O阻塞导致的线程挂起和系统资源的浪费。

### 数据零烤贝

Linux为了实现操作系统程序和应用程序的资源隔离，将系统分为内核态和用户态。操作系统的程序运行在内核态，具体包括进程管理、存储管理、文件系统、网络通信和模块机制；应用程序运行在用户态并且不能访问内核态的地址空间，如果应用程序需要访问系统内核态的资源，则需要通过系统调用接口或本地函数库（Libc）来完成，具体流程为用户程序调用系统应用接口或本地函数库，系统应用接口或本地函数库调用系统进程获取资源，并将其分配到对应的用户程序，用户程序资源在使用完成后通过系统调用接口或本地函数库释放资源。

传统的Socket服务基于JVM堆内存进行Socket读写，也就是说，申请内存资源时，需要通过JVM向操作系统申请堆内存，然后JVM将堆内存复制一份直接内存中，基于直接内存进行Socket读写。这样就存在频繁的JVM内存数据和Socket线程内存数据来回复制的问题，影响系统性能。

Netty的数据接收和发送均采用堆外直接内存进行Socket读写，堆外直接内存可以直接操作系统内存，不需要来回地进行字节缓冲区的二次复制，大大提高了系统的性能。

同时，Netty提供了组合Buffer对象，基于该对象可以聚合多个ByteBuffer对象，使得用户操作多个Buffer与操作一个Buffer一样方便，避免了传统的通过内存复制的方式将多个小Buffer合并为一个大Buffer带来的使用不便和性能损耗。

Netty的文件传输采用transferTo方法。transferTo方法可以直接将文件缓冲区的数据基于内存映射技术发送到目标Channel，避免了通过循环写方式导致的内存复制问题。

### 内存重用机制

JVM对于对象内存的分配和回收耗时很小，但Netty数据的接收和发送均采用堆外直接内存缓存的方式实现，而堆外直接内存缓存的分配和回收是一件耗时的操作。为了尽量重用缓冲区，Netty提供了基于内存池的缓冲区重用机制。

### 无锁化设计

对于一般程序来说，多线程会提高系统的并发度，但是线程数并不是越多越好，过多的线程会引起CPU的频繁切换而增加系统的负载。Netty内部采用串行无锁化设计的思想对I/O进行操作，避免多线程竞争CPU和资源锁定导致的性能下降。在具体的使用中，可以调整NIO线程池的线程参数，同时启动多个串行化的线程并行运行，这种局部无锁化的串行多线程设计比一个队列结合多个工作线程模型的性能更佳。

Netty的NioEventLoop的设计思路是Channel读取消息后，直接调用ChannelPipeline的fireChannelRead(Object msg）进行消息的处理，如果在运行过程中用户不主动切换线程，Netty的NioEventLoop则一直在该线程上进行消息的处理，这种线程绑定CPU持续执行的方式可以有效减少系统资源的竞争和切换，对于持续高并发的操作来说性能有很大的提升。

### 高性能的序列化框架

Netty默认基于Google ProtoBuf实现数据的序列化，通过扩展Netty的编解码接口，用户可以实现自定义的序列化框架，例如Thrift的压缩二进制编解码框架。在使用ProtoBuf序列化的时候需要注意以下几点。

( 1 ) SO_RCVBUF和SO_SNDBUF的设置：SO_RCVBUF为接收数据的Buffer大小，SO_SNDBUF为发送数据的Buffer大小，通常建议值为128KB或者256KB。

( 2) SO_TCPNODELAY的设置：将SO_TCPNODELAY 设置为true，表示开启自动粘包操作，该操作采用Nagle算法将缓冲区内字节较少的包前后相连组成一个大包一起发送，防止大量小包频繁发送造成网络的阻塞，从而提高网络的吞吐量。该算法对单条数据延迟的要求不高，但在传输数据量大的情况下能显著提高性能。

( 3 ）软中断：在开启软中断后，Netty会根据数据包的源地址、源端口、目的地址、目的端口计算一个Hash值，根据该Hash值选择对应的CPU执行软中断。即Netty将每个连接都与CPU绑定，并通过Hash值在多个CPU上均衡软中断，以提高CPU的性能。

## 8.高性能的序列化框架

Netty默认提供了对Google Protobuf的支持，通过扩展Netty的编解码接口，用户可以实现其它的高性能序列化框架，例如Thrift的压缩二进制编解码框架。

1. SO_RCVBUF和SO_SNDBUF：通常建议值为128K或者256K。小包封大包，防止网络阻塞
2. SO_TCPNODELAY：NAGLE算法通过将缓冲区内的小封包自动相连，组成较大的封包，阻止大量小封包的发送阻塞网络，从而提高网络应用效率。但是对于时延敏感的应用场景需要关闭该优化算法。
3. 软中断：开启RPS后可以实现软中断，提升网络吞吐量。RPS根据数据包的源地址，目的地址以及目的和源端口，计算出一个hash值，然后根据这个hash值来选择软中断运行的cpu，从上层来看，也就是说将每个连接和cpu绑定，并通过这个hash值，来均衡软中断在多个cpu上，提升网络并行处理性能。

## 9.Netty RPC实现

### 概念

RPC，即 Remote Procedure Call（远程过程调用），调用远程计算机上的服务，就像调用本地服务一样。RPC可以很好的解耦系统，如WebService就是一种基于Http协议的RPC。

### 关键技术

1. 服务发布与订阅：服务端使用Zookeeper注册服务地址，客户端从Zookeeper获取可用的服务地址。
2. 通信：使用Netty作为通信框架。
3. Spring：使用Spring配置服务，加载Bean，扫描注解。
4. 动态代理：客户端使用代理模式透明化服务调用。
5. 消息编解码：使用Protostuff序列化和反序列化消息。

### 核心流程

1. 服务消费方（client）调用以本地调用方式调用服务；
2. client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；
3. client stub找到服务地址，并将消息发送到服务端；
4. server stub收到消息后进行解码；
5. server stub根据解码结果调用本地的服务；
6. 本地服务执行并将结果返回给server stub；
7. server stub将返回结果打包成消息并发送至消费方；
8. client stub接收到消息，并进行解码；
9. 服务消费方得到最终结果。

### 消息编解码

息数据结构（接口名称+方法名+参数类型和参数值+超时时间+ requestID） 客户端的请求消息结构一般需要包括以下内容：

1. 接口名称：在我们的例子里接口名是“HelloWorldService”，如果不传，服务端就不知道调用哪个接口了；
2. 方法名：一个接口内可能有很多方法，如果不传方法名服务端也就不知道调用哪个方法；
3. 参数类型和参数值：参数类型有很多，比如有bool、int、long、double、string、map、list，甚至如struct（class）；以及相应的参数值；
4. 超时时间：
5. requestID，标识唯一请求id，在下面一节会详细描述requestID的用处。
6. 服务端返回的消息 ： 一般包括以下内容。返回值+状态code+requestID

### 通讯过程

核心问题(线程暂停、消息乱序)

如果使用netty的话，一般会用channel.writeAndFlush()方法来发送消息二进制串，这个方法调用后对于整个远程调用(从发出请求到接收到结果)来说是一个异步的，即对于当前线程来说，将请求发送出来后，线程就可以往后执行了，至于服务端的结果，是服务端处理完成后，再以消息的形式发送给客户端的。于是这里出现以下两个问题：

1.怎么让当前线程“暂停”，等结果回来后，再向后执行？

2.如果有多个线程同时进行远程方法调用，这时建立在client server之间的socket连接上会有很多双方发送的消息传递，前后顺序也可能是随机的，server处理完结果后，将结果消息发送给client，client收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？如下图所示，线程A和线程B同时向client socket发送请求requestA和requestB，socket先后将requestB和requestA发送至server，而server可能将responseB先返回，尽管requestB请求到达时间更晚。我们需要一种机制保证responseA丢给ThreadA，responseB丢给ThreadB。

通讯流程

requestID生成-AtomicLong

1.client线程每次通过socket调用一次远程接口前，生成一个唯一的ID，即requestID（requestID必需保证在一个Socket连接里面是唯一的），一般常常使用AtomicLong从0开始累计数字生成唯一ID；

存放回调对象callback到全局ConcurrentHashMap

2.将处理结果的回调对象callback，存放到全局ConcurrentHashMap里面put(requestID, callback)；

synchronized获取回调对象callback的锁并自旋wait

3.当线程调用channel.writeAndFlush()发送消息后，紧接着执行callback的get()方法试图获取远程返回的结果。在get()内部，则使用synchronized获取回调对象callback的锁，再先检测是否已经获取到结果，如果没有，然后调用callback的wait()方法，释放callback上的锁，让当前线程处于等待状态。

监听消息的线程收到消息，找到callback上的锁并唤醒

4.服务端接收到请求并处理后，将response结果（此结果中包含了前面的requestID）发送给客户端，客户端socket连接上专门监听消息的线程收到消息，分析结果，取到requestID，再从前面的ConcurrentHashMap里面get(requestID)，从而找到callback对象，再用synchronized获取callback上的锁，将方法调用结果设置到callback对象里，再调用callback.notifyAll()唤醒前面处于等待状态的线程。

## 10.为什么说 Netty 使用简单？

我们假设要搭建一个 Server 服务器，使用 **Java NIO 的步骤**如下：

1. 创建 ServerSocketChannel 。
   - 绑定监听端口，并配置为非阻塞模式。
2. 创建 Selector，将之前创建的 ServerSocketChannel 注册到 Selector 上，监听SelectionKey.OP_ACCEPT。
   - 循环执行 `Selector#select()` 方法，轮询就绪的 Channel。
3. 轮询就绪的 Channel 时，如果是处于OP_ACCEPT状态，说明是新的客户端接入，调用ServerSocketChannel#accept()方法，接收新的客户端。
   - 设置新接入的 SocketChannel 为非阻塞模式，并注册到 Selector 上，监听 `OP_READ` 。
4. 如果轮询的 Channel 状态是OP_READ，说明有新的就绪数据包需要读取，则构造 ByteBuffer 对象，读取数据。
   - 这里，解码数据包的过程，需要我们自己编写。

使用 **Netty 的步骤**如下：

1. 创建 NIO 线程组 EventLoopGroup 和 ServerBootstrap。
   - 设置 ServerBootstrap 的属性：线程组、SO_BACKLOG 选项，设置 NioServerSocketChannel 为 Channel
   - 设置业务处理 Handler 和 编解码器 Codec 。
   - 绑定端口，启动服务器程序。
2. 在业务处理 Handler 中，处理客户端发送的数据，并给出响应。

那么相比 Java NIO，使用 Netty 开发程序，都**简化了哪些步骤**呢？

1. 无需关心 `OP_ACCEPT`、`OP_READ`、`OP_WRITE` 等等 **IO 操作**，Netty 已经封装，对我们在使用是透明无感的。
2. 使用 boss 和 worker EventLoopGroup ，Netty 直接提供**多 Reactor 多线程模型**。
3. 在 Netty 中，我们看到有使用一个解码器 FixedLengthFrameDecoder，可以用于处理定长消息的问题，能够解决 **TCP 粘包拆包**问题，十分方便。如果使用 Java NIO ，需要我们自行实现解码器。

## 11.说说业务中 Netty 的使用场景？

- 构建高性能、低时延的各种 Java 中间件，Netty 主要作为基础通信框架提供高性能、低时延的通信服务。例如：
  - RocketMQ ，分布式消息队列。
  - Dubbo ，服务调用框架。
  - Spring WebFlux ，基于响应式的 Web 框架。
  - HDFS ，分布式文件系统。
- 公有或者私有协议栈的基础通信框架，例如可以基于 Netty 构建异步、高性能的 WebSocket、Protobuf 等协议的支持。
- 各领域应用，例如大数据、游戏等，Netty 作为高性能的通信框架用于内部各模块的数据分发、传输和汇总等，实现模块之间高性能通信。

## 12.说说 Netty 如何实现高性能？

1. **线程模型** ：更加优雅的 Reactor 模式实现、灵活的线程模型、利用 EventLoop 等创新性的机制，可以非常高效地管理成百上千的 Channel 。

2. **内存池设计** ：使用池化的 Direct Buffer 等技术，在提高 IO 性能的同时，减少了对象的创建和销毁。并且，内吃吃的内部实现是用一颗二叉查找树，更好的管理内存分配情况。

3. **内存零拷贝** ：使用 Direct Buffer ，可以使用 Zero-Copy 机制。Zero-Copy ，在操作数据时，不需要将数据 Buffer 从一个内存区域拷贝到另一个内存区域。因为少了一次内存的拷贝，因此 CPU 的效率就得到的提升。

4. **协议支持** ：提供对 Protobuf 等高性能序列化协议支持。

5. 使用更多本地代码

   。例如：

   - 直接利用 JNI 调用 Open SSL 等方式，获得比 Java 内建 SSL 引擎更好的性能。
   - 利用 JNI 提供了 Native Socket Transport ，在使用 Epoll edge-triggered 的情况下，可以有一定的性能提升。

6. 其它：

   - 利用反射等技术直接操纵 SelectionKey ，使用数组而不是 Java 容器等。
   - 实现 [FastThreadLocal](https://segmentfault.com/a/1190000012926809) 类，当请求频繁时，带来更好的性能。
   - ….

## 13.Netty 的高性能如何体现？

1. **程模型** ：采用异步非阻塞的 I/O 类库，基于 Reactor 模式实现，解决了传统同步阻塞 I/O 模式下服务端无法平滑处理客户端线性增长的问题。
2. **堆外内存** ：TCP 接收和发送缓冲区采用直接内存代替堆内存，避免了内存复制，提升了 I/O 读取和写入性能。
3. **内存池设计** ：支持通过内存池的方式循环利用 ByteBuf，避免了频繁创建和销毁 ByteBuf 带来的性能消耗。
4. **参数配置** ：可配置的 I/O 线程数目和 TCP 参数等，为不同用户提供定制化的调优参数，满足不同的性能场景。
5. **队列优化** ：采用环形数组缓冲区，实现无锁化并发编程，代替传统的线程安全容器或锁。
6. **并发能力** ：合理使用线程安全容器、原子类等，提升系统的并发能力。
7. **降低锁竞争** ：关键资源的使用采用单线程串行化的方式，避免多线程并发访问带来的锁竞争和额外的 CPU 资源消耗问题。
8. **内存泄露检测** ：通过引用计数器及时地释放不再被引用的对象，细粒度的内存管理降低了 GC 的频率，减少频繁 GC 带来的时延增大和 CPU 损耗。

## 14.Netty 的高可靠如何体现？

1. 链路有效性检测

   ：由于长连接不需要每次发送消息都创建链路，也不需要在消息完成交互时关闭链路，因此相对于短连接性能更高。为了保证长连接的链路有效性，往往需要通过心跳机制周期性地进行链路检测。使用心跳机制的原因是，避免在系统空闲时因网络闪断而断开连接，之后又遇到海量业务冲击导致消息积压无法处理。为了解决这个问题，需要周期性地对链路进行有效性检测，一旦发现问题，可以及时关闭链路，重建 TCP 连接。为了支持心跳，Netty 提供了两种链路空闲检测机制：

   - 读空闲超时机制：连续 T 周期没有消息可读时，发送心跳消息，进行链路检测。如果连续 N 个周期没有读取到心跳消息，可以主动关闭链路，重建连接。
   - 写空闲超时机制：连续 T 周期没有消息需要发送时，发送心跳消息，进行链路检测。如果连续 N 个周期没有读取对方发回的心跳消息，可以主动关闭链路，重建连接。

2. 内存保护机制

   ：Netty 提供多种机制对内存进行保护，包括以下几个方面：

   - 通过对象引用计数器对 ByteBuf 进行细粒度的内存申请和释放，对非法的对象引用进行检测和保护。
   - 可设置的内存容量上限，包括 ByteBuf、线程池线程数等，避免异常请求耗光内存。

3. **优雅停机**：优雅停机功能指的是当系统推出时，JVM 通过注册的 Shutdown Hook 拦截到退出信号量，然后执行推出操作，释放相关模块的资源占用，将缓冲区的消息处理完成或清空，将待刷新的数据持久化到磁盘和数据库中，等到资源回收和缓冲区消息处理完成之后，再退出。

## 15.Netty 的可扩展如何体现？

可定制、易扩展。

- **责任链模式** ：ChannelPipeline 基于责任链模式开发，便于业务逻辑的拦截、定制和扩展。
- **基于接口的开发** ：关键的类库都提供了接口或抽象类，便于用户自定义实现。
- **提供大量的工厂类** ：通过重载这些工厂类，可以按需创建出用户需要的对象。
- **提供大量系统参数** ：供用户按需设置，增强系统的场景定制性。

## 16.说说 Netty 的逻辑架构？

Netty 采用了典型的**三层网络架构**进行设计和开发

1. **Reactor 通信调度层**：由一系列辅助类组成，包括 Reactor 线程 NioEventLoop 及其父类，NioSocketChannel 和 NioServerSocketChannel 等等。该层的职责就是监听网络的读写和连接操作，负责将网络层的数据读到内存缓冲区，然后触发各自网络事件，例如连接创建、连接激活、读事件、写事件等。将这些事件触发到 pipeline 中，由 pipeline 管理的职责链来进行后续的处理。
2. **职责链 ChannelPipeline**：负责事件在职责链中的有序传播，以及负责动态地编排职责链。职责链可以选择监听和处理自己关心的事件，拦截处理和向后传播事件。
3. **业务逻辑编排层**：业务逻辑编排层通常有两类，一类是纯粹的业务逻辑编排，一类是应用层协议插件，用于特定协议相关的会话和链路管理。由于应用层协议栈往往是开发一次到处运行，并且变动较小，故而将应用协议到 POJO 的转变和上层业务放到不同的 ChannelHandler 中，就可以实现协议层和业务逻辑层的隔离，实现架构层面的分层隔离。

## 17.TCP 粘包 / 拆包的原因？应该这么解决？

**概念**

TCP 是以流的方式来处理数据，所以会导致粘包 / 拆包。

- 拆包：一个完整的包可能会被 TCP 拆分成多个包进行发送。
- 粘包：也可能把小的封装成一个大的数据包发送。

**原因**

- 应用程序写入的字节大小大于套接字发送缓冲区的大小，会发生**拆包**现象。而应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发生**粘包**现象。
- 待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行**拆包**。
- 以太网帧的 payload（净荷）大于 MTU（默认为 1500 字节）进行 IP 分片**拆包**。
- 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生**粘包**。

**解决**

在 Netty 中，提供了多个 Decoder 解析类，如下：

- ① FixedLengthFrameDecoder ，基于**固定长度**消息进行粘包拆包处理的。
- ② LengthFieldBasedFrameDecoder ，基于**消息头指定消息长度**进行粘包拆包处理的。
- ③ LineBasedFrameDecoder ，基于**换行**来进行消息粘包拆包处理的。
- ④ DelimiterBasedFrameDecoder ，基于**指定消息边界方式**进行粘包拆包处理的。

实际上，上述四个 FrameDecoder 实现可以进行规整：

- ① 是 ② 的特例，**固定长度**是**消息头指定消息长度**的一种形式。
- ③ 是 ④ 的特例，**换行**是于**指定消息边界方式**的一种形式。

## 18.了解哪几种序列化协议？

**概念**

- 序列化（编码），是将对象序列化为二进制形式（字节数组），主要用于网络传输、数据持久化等。
- 反序列化（解码），则是将从网络、磁盘等读取的字节数组还原成原始对象，主要用于网络传输对象的解码，以便完成远程调用。

**选型**

在选择序列化协议的选择，主要考虑以下三个因素：

- 序列化后的**字节大小**。更少的字节数，可以减少网络带宽、磁盘的占用。
- 序列化的**性能**。对 CPU、内存资源占用情况。
- 是否支持**跨语言**。例如，异构系统的对接和开发语言切换。

**方案**

1. 【重点】Java 默认提供的序列化
   - 无法跨语言；序列化后的字节大小太大；序列化的性能差。
2. 【重点】XML 。
   - 优点：人机可读性好，可指定元素或特性的名称。
   - 缺点：序列化数据只包含数据本身以及类的结构，不包括类型标识和程序集信息；只能序列化公共属性和字段；不能序列化方法；文件庞大，文件格式复杂，传输占带宽。
   - 适用场景：当做配置文件存储数据，实时数据转换。
3. 【重点】JSON ，是一种轻量级的数据交换格式。
   - 优点：兼容性高、数据格式比较简单，易于读写、序列化后数据较小，可扩展性好，兼容性好。与 XML 相比，其协议比较简单，解析速度比较快。
   - 缺点：数据的描述性比 XML 差、不适合性能要求为 ms 级别的情况、额外空间开销比较大。
   - 适用场景（可替代 XML ）：跨防火墙访问、可调式性要求高、基于Restful API 请求、传输数据量相对小，实时性要求相对低（例如秒级别）的服务。
4. 【了解】Thrift ，不仅是序列化协议，还是一个 RPC 框架。
   - 优点：序列化后的体积小, 速度快、支持多种语言和丰富的数据类型、对于数据字段的增删具有较强的兼容性、支持二进制压缩编码。
   - 缺点：使用者较少、跨防火墙访问时，不安全、不具有可读性，调试代码时相对困难、不能与其他传输层协议共同使用（例如 HTTP）、无法支持向持久层直接读写数据，即不适合做数据持久化序列化协议。
   - 适用场景：分布式系统的 RPC 解决方案。
5. 【了解】Avro ，Hadoop 的一个子项目，解决了JSON的冗长和没有IDL的问题。
   - 优点：支持丰富的数据类型、简单的动态语言结合功能、具有自我描述属性、提高了数据解析速度、快速可压缩的二进制数据形式、可以实现远程过程调用 RPC、支持跨编程语言实现。
   - 缺点：对于习惯于静态类型语言的用户不直观。
   - 适用场景：在 Hadoop 中做 Hive、Pig 和 MapReduce 的持久化数据格式。
6. 【重点】Protobuf ，将数据结构以.proto文件进行描述，通过代码生成工具可以生成对应数据结构的 POJO 对象和 Protobuf 相关的方法和属性。
   - 优点：序列化后码流小，性能高、结构化数据存储格式（XML JSON等）、通过标识字段的顺序，可以实现协议的前向兼容、结构化的文档更容易管理和维护。
   - 缺点：需要依赖于工具生成代码、支持的语言相对较少，官方只支持Java 、C++、python。
   - 适用场景：对性能要求高的 RPC 调用、具有良好的跨防火墙的访问属性、适合应用层对象的持久化。
7. 其它
   - 【重点】Protostuff ，基于 Protobuf 协议，但不需要配置proto 文件，直接导包即可。
     - 目前，微博 RPC 框架 Motan 在使用它。
   - 【了解】Jboss Marshaling ，可以直接序列化 Java 类， 无须实 `java.io.Serializable` 接口。
   - 【了解】Message Pack ，一个高效的二进制序列化格式。
   - 【重点】Hessian，采用二进制协议的轻量级 remoting on http 服务。
     - 目前，阿里 RPC 框架 Dubbo 的**默认**序列化协议。
   - 【重要】kryo ，是一个快速高效的Java对象图形序列化框架，主要特点是性能、高效和易用。该项目用来序列化对象到文件、数据库或者网络。
     - 目前，阿里 RPC 框架 Dubbo 的可选序列化协议。
   - 【重要】FST ，fast-serialization 是重新实现的 Java 快速对象序列化的开发包。序列化速度更快（2-10倍）、体积更小，而且兼容 JDK 原生的序列化。要求 JDK 1.7 支持。
     - 目前，阿里 RPC 框架 Dubbo 的可选序列化协议。

## 19.Netty 的零拷贝实现？

Netty 的零拷贝实现，是体现在多方面的，主要如下：

1. 【重点】Netty 的接收和发送 ByteBuffer 采用堆外直接内存Direct Buffer。
   - 使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝；使用堆内内存会多了一次内存拷贝，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。
   - Netty 创建的 ByteBuffer 类型，由 ChannelConfig 配置。而 ChannelConfig 配置的 ByteBufAllocator 默认创建 Direct Buffer 类型。
2. CompositeByteBuf类，可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf ，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的 Buffer 。
   - `#addComponents(...)` 方法，可将 header 与 body 合并为一个逻辑上的 ByteBuf 。这两个 ByteBuf 在CompositeByteBuf 内部都是单独存在的，即 CompositeByteBuf 只是逻辑上是一个整体。
3. 通过FileRegion包装的 FileChannel 。
   - `#tranferTo(...)` 方法，实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel ，避免了传统通过循环 write 方式，导致的内存拷贝问题。
4. 通过 **wrap** 方法, 我们可以将 `byte[]` 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作。

## 20.Netty 的零拷贝实现？

Netty 的零拷贝实现，是体现在多方面的，主要如下：

1. 【重点】Netty 的接收和发送 ByteBuffer 采用堆外直接内存Direct Buffer。
   - 使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝；使用堆内内存会多了一次内存拷贝，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。
   - Netty 创建的 ByteBuffer 类型，由 ChannelConfig 配置。而 ChannelConfig 配置的 ByteBufAllocator 默认创建 Direct Buffer 类型。
2. CompositeByteBuf类，可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf ，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的 Buffer 。
   - `#addComponents(...)` 方法，可将 header 与 body 合并为一个逻辑上的 ByteBuf 。这两个 ByteBuf 在CompositeByteBuf 内部都是单独存在的，即 CompositeByteBuf 只是逻辑上是一个整体。
3. 通过FileRegion包装的 FileChannel 。
   - `#tranferTo(...)` 方法，实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel ，避免了传统通过循环 write 方式，导致的内存拷贝问题。
4. 通过 **wrap** 方法, 我们可以将 `byte[]` 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作。

## 21.Netty 如何实现重连？

- 客户端，通过 IdleStateHandler 实现定时检测是否空闲，例如说 15 秒。
  - 如果空闲，则向服务端发起心跳。
  - 如果多次心跳失败，则关闭和服务端的连接，然后重新发起连接。
- 服务端，通过 IdleStateHandler 实现定时检测客户端是否空闲，例如说 90 秒。
  - 如果检测到空闲，则关闭客户端。
  - 注意，如果接收到客户端的心跳请求，要反馈一个心跳响应给客户端。通过这样的方式，使客户端知道自己心跳成功。

## 22Netty 为什么要实现内存管理？

在 Netty 中，IO 读写必定是非常频繁的操作，而考虑到更高效的网络传输性能，Direct ByteBuffer 必然是最合适的选择。但是 Direct ByteBuffer 的申请和释放是高成本的操作，那么进行**池化**管理，多次重用是比较有效的方式。但是，不同于一般于我们常见的对象池、连接池等**池化**的案例，ByteBuffer 是有**大小**一说。又但是，申请多大的 Direct ByteBuffer 进行池化又会是一个大问题，太大会浪费内存，太小又会出现频繁的扩容和内存复制！！！所以呢，就需要有一个合适的内存管理算法，解决**高效分配内存**的同时又解决**内存碎片化**的问题。

Netty 4.x 增加了 Pooled Buffer，实现了高性能的 buffer 池，分配策略则是结合了 buddy allocation 和 slab allocation 的 **jemalloc** 变种，代码在`io.netty.buffer.PoolArena` 中。

官方说提供了以下优势：

- 频繁分配、释放 buffer 时减少了 GC 压力。
- 在初始化新 buffer 时减少内存带宽消耗( 初始化时不可避免的要给buffer数组赋初始值 )。
- 及时的释放 direct buffer 。

## 23.**Netty的特点？**

- 一个高性能、异步事件驱动的NIO框架，它提供了对TCP、UDP和文件传输的支持
- 使用更高效的socket底层，对epoll空轮询引起的cpu占用飙升在内部进行了处理，避免了直接使用NIO的陷阱，简化了NIO的处理方式。
- 采用多种decoder/encoder 支持，对TCP粘包/分包进行自动化处理
- 可使用接受/处理线程池，提高连接效率，对重连、心跳检测的简单支持
- 可配置IO线程数、TCP参数， TCP接收和发送缓冲区使用直接内存代替堆内存，通过内存池的方式循环利用ByteBuf
- 通过引用计数器及时申请释放不再引用的对象，降低了GC频率
- 使用单线程串行化的方式，高效的Reactor线程模型
- 大量使用了volitale、使用了CAS和原子类、线程安全类的使用、读写锁的使用